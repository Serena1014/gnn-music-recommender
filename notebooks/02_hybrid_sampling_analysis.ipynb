{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hybird Core-based + Stratified Sampling",
   "id": "784adb025faa243f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:19:03.110070Z",
     "start_time": "2025-08-15T14:34:46.431236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import gc\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# SCALE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_scale_parameters(scale_name: str, target_playlists: int) -> Dict:\n",
    "    \"\"\"Get optimized parameters for different scales\"\"\"\n",
    "\n",
    "    if target_playlists <= 300:  # tiny\n",
    "        return {\n",
    "            'min_track_frequency': 3,\n",
    "            'min_user_playlists': 3,\n",
    "            'expected_total_nodes': 5000,\n",
    "            'expected_tracks': 2000,\n",
    "            'expected_artists': 500,\n",
    "            'expected_albums': 400,\n",
    "            'expected_users': 100\n",
    "        }\n",
    "    elif target_playlists <= 600:  # small\n",
    "        return {\n",
    "            'min_track_frequency': 5,\n",
    "            'min_user_playlists': 5,\n",
    "            'expected_total_nodes': 10000,\n",
    "            'expected_tracks': 4000,\n",
    "            'expected_artists': 1000,\n",
    "            'expected_albums': 800,\n",
    "            'expected_users': 200\n",
    "        }\n",
    "    elif target_playlists <= 1000:  # medium\n",
    "        return {\n",
    "            'min_track_frequency': 6,\n",
    "            'min_user_playlists': 7,\n",
    "            'expected_total_nodes': 20000,\n",
    "            'expected_tracks': 8000,\n",
    "            'expected_artists': 2000,\n",
    "            'expected_albums': 1500,\n",
    "            'expected_users': 300\n",
    "        }\n",
    "    else:  # large (1500+)\n",
    "        return {\n",
    "            'min_track_frequency': 8,\n",
    "            'min_user_playlists': 10,\n",
    "            'expected_total_nodes': 30000,\n",
    "            'expected_tracks': 12000,\n",
    "            'expected_artists': 3000,\n",
    "            'expected_albums': 2500,\n",
    "            'expected_users': 500\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-SCALE HYBRID STREAMING SAMPLER\n",
    "# =============================================================================\n",
    "\n",
    "class MultiScaleHybridStreamingSampler:\n",
    "    \"\"\"\n",
    "    Multi-Scale Hybrid Core-Based + Stratified Streaming Sampler\n",
    "\n",
    "    Maintains original methodology with scale-optimized parameters:\n",
    "    - Pass 1: Core-based filtering (length, user activity, track frequency)\n",
    "    - Pass 2: Stratified sampling with priority scoring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 scale_name: str,\n",
    "                 target_playlists: int,\n",
    "                 batch_size: int = 20,\n",
    "                 min_playlist_length: int = 10,\n",
    "                 max_playlist_length: int = 100):\n",
    "\n",
    "        # Get scale-specific parameters\n",
    "        scale_params = get_scale_parameters(scale_name, target_playlists)\n",
    "\n",
    "        # Core parameters\n",
    "        self.scale_name = scale_name\n",
    "        self.target_playlists = target_playlists\n",
    "        self.batch_size = batch_size\n",
    "        self.min_playlist_length = min_playlist_length\n",
    "        self.max_playlist_length = max_playlist_length\n",
    "        self.min_track_frequency = scale_params['min_track_frequency']\n",
    "        self.min_user_playlists = scale_params['min_user_playlists']\n",
    "\n",
    "        # Expected targets for verification\n",
    "        self.expected_total_nodes = scale_params['expected_total_nodes']\n",
    "        self.expected_tracks = scale_params['expected_tracks']\n",
    "        self.expected_artists = scale_params['expected_artists']\n",
    "        self.expected_albums = scale_params['expected_albums']\n",
    "        self.expected_users = scale_params['expected_users']\n",
    "\n",
    "        # Statistics collectors\n",
    "        self.track_counts = Counter()\n",
    "        self.user_counts = Counter()\n",
    "        self.playlist_stats = []\n",
    "\n",
    "        print(f\"🎯 MULTI-SCALE SAMPLER ({scale_name.upper()} SCALE)\")\n",
    "        print(f\"=\" * 70)\n",
    "        print(f\"📊 SCALE-OPTIMIZED PARAMETERS:\")\n",
    "        print(f\"   • Target playlists: {target_playlists:,}\")\n",
    "        print(f\"   • Min track frequency: {scale_params['min_track_frequency']}\")\n",
    "        print(f\"   • Min user playlists: {scale_params['min_user_playlists']}\")\n",
    "        print(f\"   • Expected total nodes: ~{self.expected_total_nodes:,}\")\n",
    "        print()\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            return process.memory_info().rss / 1024 / 1024\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def print_memory_status(self, stage: str):\n",
    "        \"\"\"Print current memory usage\"\"\"\n",
    "        memory_mb = self.get_memory_usage()\n",
    "        print(f\"💾 Memory usage after {stage}: {memory_mb:.1f} MB\")\n",
    "\n",
    "    def get_file_batches(self, file_pattern: str) -> List[List[str]]:\n",
    "        \"\"\"Split files into manageable batches\"\"\"\n",
    "        file_paths = glob.glob(file_pattern)\n",
    "        file_paths.sort()\n",
    "\n",
    "        if not file_paths:\n",
    "            raise FileNotFoundError(f\"No files found: {file_pattern}\")\n",
    "\n",
    "        print(f\"📁 Found {len(file_paths)} files\")\n",
    "\n",
    "        # Split into batches\n",
    "        batches = []\n",
    "        for i in range(0, len(file_paths), self.batch_size):\n",
    "            batch = file_paths[i:i + self.batch_size]\n",
    "            batches.append(batch)\n",
    "\n",
    "        print(f\"📦 Created {len(batches)} batches of ~{self.batch_size} files each\")\n",
    "        return batches\n",
    "\n",
    "    def _extract_user_id(self, playlist: Dict) -> str:\n",
    "        \"\"\"Extract user identifier with scale-appropriate consolidation\"\"\"\n",
    "        # Name-based grouping with consolidation based on scale\n",
    "        name = playlist.get('name', '').lower().strip()\n",
    "        if name:\n",
    "            words = name.split()\n",
    "            if words:\n",
    "                # Adjust consolidation level based on scale\n",
    "                char_count = 2 if self.target_playlists <= 600 else 3\n",
    "                user_base = words[0][:char_count]\n",
    "                user_id = ''.join(c for c in user_base if c.isalnum())\n",
    "                if user_id:\n",
    "                    return user_id\n",
    "\n",
    "        # PID-based consolidation\n",
    "        pid = playlist.get('pid', 0)\n",
    "        # Scale user bins based on target size\n",
    "        user_bins = max(100, self.target_playlists // 3)\n",
    "        return f\"u{pid % user_bins}\"\n",
    "\n",
    "    def pass1_core_filtering(self, file_pattern: str) -> Dict:\n",
    "        \"\"\"PASS 1: Core-based filtering + Statistics collection\"\"\"\n",
    "        print(\"🔍 PASS 1: HYBRID CORE-BASED FILTERING\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        batches = self.get_file_batches(file_pattern)\n",
    "\n",
    "        stage_counts = {\n",
    "            'total_seen': 0,\n",
    "            'passed_length_filter': 0,\n",
    "            'passed_user_filter': 0,\n",
    "            'passed_track_frequency_filter': 0,\n",
    "            'final_valid': 0\n",
    "        }\n",
    "\n",
    "        print(\"🚀 Applying Core-Based Filters:\")\n",
    "        print(f\"   ✅ Step 1: Playlist length ({self.min_playlist_length}-{self.max_playlist_length} tracks)\")\n",
    "        print(f\"   ✅ Step 2: User activity (≥{self.min_user_playlists} playlists per user)\")\n",
    "        print(f\"   ✅ Step 3: Track frequency (≥{self.min_track_frequency} appearances)\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1a: Collect user activity statistics\n",
    "        print(\"📊 Sub-pass 1a: Collecting user activity statistics...\")\n",
    "        user_playlist_count = Counter()\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"   User stats progress: {batch_idx + 1}/{len(batches)} batches\")\n",
    "\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "                    for playlist in file_playlists:\n",
    "                        user_id = self._extract_user_id(playlist)\n",
    "                        user_playlist_count[user_id] += 1\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        # Identify active users\n",
    "        active_users = {\n",
    "            user for user, count in user_playlist_count.items()\n",
    "            if count >= self.min_user_playlists\n",
    "        }\n",
    "\n",
    "        print(f\"   ✅ Identified {len(active_users):,} active users (target: ~{self.expected_users})\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1b: Apply all core filters\n",
    "        print(\"🔍 Sub-pass 1b: Applying all core filters...\")\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            print(f\"📦 Processing batch {batch_idx + 1}/{len(batches)}\")\n",
    "\n",
    "            batch_playlists = []\n",
    "\n",
    "            # Load batch\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "                    for playlist in file_playlists:\n",
    "                        playlist['_source_file'] = file_path\n",
    "\n",
    "                    batch_playlists.extend(file_playlists)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   ⚠️  Error loading {os.path.basename(file_path)}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Process batch with core filtering\n",
    "            for playlist in batch_playlists:\n",
    "                stage_counts['total_seen'] += 1\n",
    "\n",
    "                # CORE FILTER 1: Playlist length\n",
    "                tracks = playlist.get('tracks', [])\n",
    "                playlist_length = len(tracks)\n",
    "\n",
    "                if not (self.min_playlist_length <= playlist_length <= self.max_playlist_length):\n",
    "                    continue\n",
    "                stage_counts['passed_length_filter'] += 1\n",
    "\n",
    "                # CORE FILTER 2: User activity\n",
    "                user_id = self._extract_user_id(playlist)\n",
    "                if user_id not in active_users:\n",
    "                    continue\n",
    "                stage_counts['passed_user_filter'] += 1\n",
    "\n",
    "                # Count tracks for frequency analysis\n",
    "                playlist_tracks = set()\n",
    "                for track in tracks:\n",
    "                    track_uri = track.get('track_uri', '')\n",
    "                    if track_uri:\n",
    "                        self.track_counts[track_uri] += 1\n",
    "                        playlist_tracks.add(track_uri)\n",
    "\n",
    "                self.user_counts[user_id] += 1\n",
    "\n",
    "                # Store playlist metadata\n",
    "                playlist_metadata = {\n",
    "                    'file_path': playlist['_source_file'],\n",
    "                    'pid': playlist.get('pid'),\n",
    "                    'length': playlist_length,\n",
    "                    'modified_at': playlist.get('modified_at', 0),\n",
    "                    'user_id': user_id,\n",
    "                    'track_uris': list(playlist_tracks),\n",
    "                    'name': playlist.get('name', ''),\n",
    "                    'collaborative': playlist.get('collaborative', False),\n",
    "                    'num_followers': playlist.get('num_followers', 0)\n",
    "                }\n",
    "\n",
    "                self.playlist_stats.append(playlist_metadata)\n",
    "\n",
    "            # Clear batch from memory\n",
    "            del batch_playlists\n",
    "            gc.collect()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"   Progress: {stage_counts['total_seen']:,} seen, {len(self.playlist_stats):,} valid so far\")\n",
    "                self.print_memory_status(f\"batch {batch_idx + 1}\")\n",
    "\n",
    "        # CORE FILTER 3: Track frequency\n",
    "        print(f\"\\n🔍 Applying final core filter: Track frequency (≥{self.min_track_frequency})\")\n",
    "\n",
    "        core_tracks = {\n",
    "            track for track, count in self.track_counts.items()\n",
    "            if count >= self.min_track_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"   ✅ Identified {len(core_tracks):,} core tracks (target: ~{self.expected_tracks})\")\n",
    "\n",
    "        # Filter playlists that have core tracks\n",
    "        filtered_playlist_stats = []\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            playlist_tracks = set(playlist_meta['track_uris'])\n",
    "            if playlist_tracks.intersection(core_tracks):\n",
    "                filtered_playlist_stats.append(playlist_meta)\n",
    "                stage_counts['passed_track_frequency_filter'] += 1\n",
    "\n",
    "        self.playlist_stats = filtered_playlist_stats\n",
    "        stage_counts['final_valid'] = len(filtered_playlist_stats)\n",
    "\n",
    "        print(f\"\\n✅ CORE-BASED FILTERING COMPLETE:\")\n",
    "        print(f\"   📊 Filtering Funnel:\")\n",
    "        print(f\"      • Total playlists: {stage_counts['total_seen']:,}\")\n",
    "        print(f\"      • Length filter: {stage_counts['passed_length_filter']:,} ({stage_counts['passed_length_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      • User filter: {stage_counts['passed_user_filter']:,} ({stage_counts['passed_user_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      • Track frequency: {stage_counts['passed_track_frequency_filter']:,} ({stage_counts['passed_track_frequency_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      • 🎯 FINAL VALID: {stage_counts['final_valid']:,} ({stage_counts['final_valid']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "        return {\n",
    "            'total_playlists': stage_counts['total_seen'],\n",
    "            'valid_playlists': stage_counts['final_valid'],\n",
    "            'core_tracks': core_tracks,\n",
    "            'active_users': active_users,\n",
    "            'unique_tracks': len(self.track_counts),\n",
    "            'stage_counts': stage_counts\n",
    "        }\n",
    "\n",
    "    def create_strata(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"Create comprehensive strata for stratified sampling\"\"\"\n",
    "        print(\"📊 CREATING STRATIFIED SAMPLING STRATA\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Get temporal split\n",
    "        timestamps = [p['modified_at'] for p in self.playlist_stats if p['modified_at'] > 0]\n",
    "        median_time = np.median(timestamps) if timestamps else 1500000000\n",
    "\n",
    "        # Get user activity split\n",
    "        user_playlist_counts = {}\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_playlist_counts[user_id] = user_playlist_counts.get(user_id, 0) + 1\n",
    "\n",
    "        user_activity_median = np.median(list(user_playlist_counts.values())) if user_playlist_counts else 5\n",
    "\n",
    "        print(f\"   📅 Temporal split at timestamp: {median_time}\")\n",
    "        print(f\"   👥 User activity split at: {user_activity_median} playlists per user\")\n",
    "\n",
    "        # Create 12 comprehensive strata\n",
    "        strata = {\n",
    "            'short_old_casual': [], 'short_old_active': [],\n",
    "            'short_recent_casual': [], 'short_recent_active': [],\n",
    "            'medium_old_casual': [], 'medium_old_active': [],\n",
    "            'medium_recent_casual': [], 'medium_recent_active': [],\n",
    "            'long_old_casual': [], 'long_old_active': [],\n",
    "            'long_recent_casual': [], 'long_recent_active': []\n",
    "        }\n",
    "\n",
    "        for i, playlist_meta in enumerate(self.playlist_stats):\n",
    "            length = playlist_meta['length']\n",
    "            timestamp = playlist_meta['modified_at']\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_activity = user_playlist_counts.get(user_id, 1)\n",
    "\n",
    "            # Categorize\n",
    "            length_cat = 'short' if length <= 30 else 'medium' if length <= 60 else 'long'\n",
    "            time_cat = 'recent' if timestamp >= median_time else 'old'\n",
    "            activity_cat = 'active' if user_activity >= user_activity_median else 'casual'\n",
    "\n",
    "            stratum_key = f\"{length_cat}_{time_cat}_{activity_cat}\"\n",
    "            strata[stratum_key].append(i)\n",
    "\n",
    "        # Print strata distribution\n",
    "        print(\"   📋 Strata Distribution:\")\n",
    "        total_playlists = len(self.playlist_stats)\n",
    "\n",
    "        for stratum, indices in strata.items():\n",
    "            if indices:\n",
    "                percentage = len(indices) / total_playlists * 100\n",
    "                print(f\"      • {stratum:20s}: {len(indices):6,} ({percentage:4.1f}%)\")\n",
    "\n",
    "        print()\n",
    "        return strata\n",
    "\n",
    "    def _calculate_priority_score(self, playlist_meta: Dict) -> float:\n",
    "        \"\"\"Calculate priority score for playlist selection\"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Factor 1: Track diversity (30% weight)\n",
    "        unique_tracks = len(playlist_meta['track_uris'])\n",
    "        playlist_length = playlist_meta['length']\n",
    "        if playlist_length > 0:\n",
    "            track_diversity_ratio = unique_tracks / playlist_length\n",
    "            score += track_diversity_ratio * 3.0\n",
    "\n",
    "        # Factor 2: User engagement (25% weight)\n",
    "        num_followers = playlist_meta.get('num_followers', 0)\n",
    "        if num_followers > 0:\n",
    "            follower_score = min(np.log10(num_followers + 1), 3.0)\n",
    "            score += follower_score * 2.5\n",
    "\n",
    "        # Factor 3: Playlist completeness (20% weight)\n",
    "        name = playlist_meta.get('name', '')\n",
    "        has_good_name = len(name.strip()) > 3 and not name.lower().startswith('my playlist')\n",
    "        if has_good_name:\n",
    "            score += 2.0\n",
    "\n",
    "        # Factor 4: Collaborative playlists bonus (10% weight)\n",
    "        if playlist_meta.get('collaborative', False):\n",
    "            score += 1.0\n",
    "\n",
    "        # Factor 5: Length balance bonus (15% weight)\n",
    "        length = playlist_meta['length']\n",
    "        if 20 <= length <= 80:\n",
    "            score += 1.5\n",
    "\n",
    "        return score\n",
    "\n",
    "    def pass2_stratified_sampling(self, strata: Dict[str, List[int]]) -> List[Dict]:\n",
    "        \"\"\"PASS 2: Stratified sampling with priority scoring\"\"\"\n",
    "        print(\"🎲 PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        total_available = len(self.playlist_stats)\n",
    "\n",
    "        if total_available <= self.target_playlists:\n",
    "            print(f\"   📝 Available ({total_available:,}) ≤ target ({self.target_playlists:,})\")\n",
    "            selected_indices = list(range(total_available))\n",
    "        else:\n",
    "            sampling_ratio = self.target_playlists / total_available\n",
    "            selected_indices = set()\n",
    "\n",
    "            print(f\"   📊 Global sampling ratio: {sampling_ratio:.3f}\")\n",
    "            print(f\"   🏆 Using priority scoring within strata\")\n",
    "            print()\n",
    "\n",
    "            # Stratified sampling with priority scoring\n",
    "            for stratum, indices in strata.items():\n",
    "                if not indices:\n",
    "                    continue\n",
    "\n",
    "                stratum_target = max(1, int(len(indices) * sampling_ratio))\n",
    "                stratum_target = min(stratum_target, len(indices))\n",
    "\n",
    "                # Score playlists in this stratum\n",
    "                scored_playlists = []\n",
    "                for idx in indices:\n",
    "                    playlist_meta = self.playlist_stats[idx]\n",
    "                    score = self._calculate_priority_score(playlist_meta)\n",
    "                    scored_playlists.append((idx, score))\n",
    "\n",
    "                # Sort by score and sample\n",
    "                scored_playlists.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Hybrid: 70% top-scored + 30% random\n",
    "                top_count = int(stratum_target * 0.7)\n",
    "                random_count = stratum_target - top_count\n",
    "\n",
    "                selected = [idx for idx, _ in scored_playlists[:top_count]]\n",
    "\n",
    "                if random_count > 0 and len(scored_playlists) > top_count:\n",
    "                    remaining = [idx for idx, _ in scored_playlists[top_count:]]\n",
    "                    if len(remaining) >= random_count:\n",
    "                        selected.extend(random.sample(remaining, random_count))\n",
    "                    else:\n",
    "                        selected.extend(remaining)\n",
    "\n",
    "                selected_indices.update(selected)\n",
    "\n",
    "                avg_score = np.mean([score for _, score in scored_playlists[:len(selected)]])\n",
    "                print(f\"      • {stratum:20s}: {len(selected):4,} / {len(indices):5,} (avg score: {avg_score:.2f})\")\n",
    "\n",
    "            selected_indices = list(selected_indices)\n",
    "\n",
    "        print(f\"\\n   🎯 Selected {len(selected_indices):,} playlists for final loading\")\n",
    "        return self._load_selected_playlists(selected_indices)\n",
    "\n",
    "    def _load_selected_playlists(self, selected_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"Load only the selected playlists from files\"\"\"\n",
    "        print(\"   📁 Loading selected playlists...\")\n",
    "\n",
    "        # Group by file\n",
    "        file_to_playlists = defaultdict(list)\n",
    "        for idx in selected_indices:\n",
    "            playlist_meta = self.playlist_stats[idx]\n",
    "            file_path = playlist_meta['file_path']\n",
    "            file_to_playlists[file_path].append(playlist_meta)\n",
    "\n",
    "        print(f\"   📂 Loading from {len(file_to_playlists)} files\")\n",
    "\n",
    "        final_playlists = []\n",
    "\n",
    "        for file_idx, (file_path, playlist_metas) in enumerate(file_to_playlists.items()):\n",
    "            if file_idx % 100 == 0:\n",
    "                print(f\"      📖 File {file_idx + 1}/{len(file_to_playlists)}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                file_playlists = data.get('playlists', [])\n",
    "                pid_to_playlist = {p.get('pid'): p for p in file_playlists}\n",
    "\n",
    "                for meta in playlist_metas:\n",
    "                    pid = meta['pid']\n",
    "                    if pid in pid_to_playlist:\n",
    "                        playlist = pid_to_playlist[pid]\n",
    "                        playlist['_sampling_score'] = self._calculate_priority_score(meta)\n",
    "                        final_playlists.append(playlist)\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        print(f\"   ✅ Loaded {len(final_playlists):,} final playlists\")\n",
    "        return final_playlists\n",
    "\n",
    "    def _verify_final_scale(self, final_playlists: List[Dict]) -> Dict:\n",
    "        \"\"\"Verify that final scale meets experimental targets\"\"\"\n",
    "        print(\"\\n🔍 VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Count actual entities\n",
    "        actual_tracks = set()\n",
    "        actual_artists = set()\n",
    "        actual_albums = set()\n",
    "        actual_users = set()\n",
    "\n",
    "        for playlist in final_playlists:\n",
    "            user_id = self._extract_user_id(playlist)\n",
    "            actual_users.add(user_id)\n",
    "\n",
    "            for track in playlist.get('tracks', []):\n",
    "                track_uri = track.get('track_uri', '')\n",
    "                artist_uri = track.get('artist_uri', '')\n",
    "                album_uri = track.get('album_uri', '')\n",
    "\n",
    "                if track_uri:\n",
    "                    actual_tracks.add(track_uri)\n",
    "                if artist_uri:\n",
    "                    actual_artists.add(artist_uri)\n",
    "                if album_uri:\n",
    "                    actual_albums.add(album_uri)\n",
    "\n",
    "        actual_scale = {\n",
    "            'playlists': len(final_playlists),\n",
    "            'tracks': len(actual_tracks),\n",
    "            'artists': len(actual_artists),\n",
    "            'albums': len(actual_albums),\n",
    "            'users': len(actual_users),\n",
    "            'total_nodes': len(final_playlists) + len(actual_tracks) + len(actual_artists) + len(actual_albums) + len(actual_users)\n",
    "        }\n",
    "\n",
    "        print(f\"   📊 Final Entity Counts:\")\n",
    "        print(f\"      • Playlists: {actual_scale['playlists']:,} (target: {self.target_playlists:,})\")\n",
    "        print(f\"      • Tracks: {actual_scale['tracks']:,} (target: ~{self.expected_tracks:,})\")\n",
    "        print(f\"      • Artists: {actual_scale['artists']:,} (target: ~{self.expected_artists:,})\")\n",
    "        print(f\"      • Albums: {actual_scale['albums']:,} (target: ~{self.expected_albums:,})\")\n",
    "        print(f\"      • Users: {actual_scale['users']:,} (target: ~{self.expected_users:,})\")\n",
    "        print(f\"      🎯 TOTAL: {actual_scale['total_nodes']:,} (target: ~{self.expected_total_nodes:,})\")\n",
    "\n",
    "        return actual_scale\n",
    "\n",
    "    def run_hybrid_sampling(self, file_pattern: str) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"Main method: Complete hybrid sampling workflow\"\"\"\n",
    "        print(\"🚀 STARTING MULTI-SCALE HYBRID SAMPLING\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        self.print_memory_status(\"start\")\n",
    "\n",
    "        # Pass 1: Core-based filtering\n",
    "        stats = self.pass1_core_filtering(file_pattern)\n",
    "        self.print_memory_status(\"pass 1 complete\")\n",
    "\n",
    "        # Create strata\n",
    "        strata = self.create_strata()\n",
    "        self.print_memory_status(\"strata created\")\n",
    "\n",
    "        # Pass 2: Stratified sampling\n",
    "        final_playlists = self.pass2_stratified_sampling(strata)\n",
    "        self.print_memory_status(\"pass 2 complete\")\n",
    "\n",
    "        # Scale verification\n",
    "        actual_scale = self._verify_final_scale(final_playlists)\n",
    "\n",
    "        # Final statistics\n",
    "        final_stats = {\n",
    "            'methodology': 'multi_scale_hybrid_core_based_stratified_streaming',\n",
    "            'scale': self.scale_name,\n",
    "            'original_total': stats['total_playlists'],\n",
    "            'final_sampled': len(final_playlists),\n",
    "            'retention_rate': len(final_playlists) / stats['total_playlists'],\n",
    "            'core_filtering_retention': stats['valid_playlists'] / stats['total_playlists'],\n",
    "            'unique_tracks': stats['unique_tracks'],\n",
    "            'core_tracks_count': len(stats['core_tracks']),\n",
    "            'active_users_count': len(stats['active_users']),\n",
    "            'stage_counts': stats['stage_counts'],\n",
    "            'actual_scale': actual_scale,\n",
    "            'scale_targets': {\n",
    "                'total_nodes': self.expected_total_nodes,\n",
    "                'playlists': self.target_playlists,\n",
    "                'tracks': self.expected_tracks,\n",
    "                'artists': self.expected_artists,\n",
    "                'albums': self.expected_albums,\n",
    "                'users': self.expected_users\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"\\n🎉 MULTI-SCALE HYBRID SAMPLING COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"📊 Results: {stats['total_playlists']:,} → {len(final_playlists):,} playlists\")\n",
    "        print(f\"📈 Overall retention: {len(final_playlists) / stats['total_playlists']:.1%}\")\n",
    "\n",
    "        # Scale verification summary\n",
    "        total_actual = actual_scale['total_nodes']\n",
    "        scale_ratio = total_actual / self.expected_total_nodes\n",
    "        print(f\"\\n🎯 EXPERIMENTAL SCALE VERIFICATION:\")\n",
    "        print(f\"   • Actual total nodes: {total_actual:,}\")\n",
    "        print(f\"   • Target total nodes: {self.expected_total_nodes:,}\")\n",
    "        print(f\"   • Scale ratio: {scale_ratio:.3f} ({'✅ GOOD' if 0.8 <= scale_ratio <= 1.2 else '⚠️ ADJUST'})\")\n",
    "\n",
    "        return final_playlists, final_stats\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-SCALE CREATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def run_multi_scale_sampling(file_pattern: str, scales: Dict[str, int] = None):\n",
    "    \"\"\"Create multiple scale datasets in one execution\"\"\"\n",
    "\n",
    "    if scales is None:\n",
    "        scales = {\n",
    "            'tiny': 300,\n",
    "            'small': 600,\n",
    "            'medium': 1000,\n",
    "            'large': 1500\n",
    "        }\n",
    "\n",
    "    print(\"🎯 MULTI-SCALE DATASET CREATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Creating multiple scale datasets for flexible experimentation\")\n",
    "    print()\n",
    "\n",
    "    # Show scale overview\n",
    "    print(\"📊 SCALES TO CREATE:\")\n",
    "    print(\"Scale    Playlists  Expected Nodes  Expected Time\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    time_estimates = {\n",
    "        'tiny': '15-30 min',\n",
    "        'small': '45-75 min',\n",
    "        'medium': '2-3 hours',\n",
    "        'large': '4-6 hours'\n",
    "    }\n",
    "\n",
    "    for scale_name, target_playlists in scales.items():\n",
    "        scale_params = get_scale_parameters(scale_name, target_playlists)\n",
    "        expected_nodes = scale_params['expected_total_nodes']\n",
    "        time_est = time_estimates.get(scale_name, 'Unknown')\n",
    "        print(f\"{scale_name:<8} {target_playlists:<10} {expected_nodes:<15,} {time_est}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for scale_name, target_playlists in scales.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"🔧 CREATING {scale_name.upper()} SCALE DATASET\")\n",
    "        print(f\"Target: {target_playlists:,} playlists\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        try:\n",
    "            # Create scale-specific sampler\n",
    "            sampler = MultiScaleHybridStreamingSampler(\n",
    "                scale_name=scale_name,\n",
    "                target_playlists=target_playlists,\n",
    "                batch_size=20,\n",
    "                min_playlist_length=10,\n",
    "                max_playlist_length=100\n",
    "            )\n",
    "\n",
    "            # Run sampling\n",
    "            sampled_playlists, stats = sampler.run_hybrid_sampling(file_pattern)\n",
    "\n",
    "            # Create output data\n",
    "            output_data = {\n",
    "                'info': {\n",
    "                    'generated_on': datetime.now().isoformat(),\n",
    "                    'sampling_method': 'multi_scale_hybrid_core_based_stratified_streaming',\n",
    "                    'scale': scale_name,\n",
    "                    'target_playlists': target_playlists,\n",
    "                    'parameters': get_scale_parameters(scale_name, target_playlists)\n",
    "                },\n",
    "                'sampling_stats': stats,\n",
    "                'playlists': sampled_playlists\n",
    "            }\n",
    "\n",
    "            # Save scale-specific file\n",
    "            os.makedirs('../data/processed', exist_ok=True)\n",
    "            output_file = f'../data/processed/spotify_scaled_hybrid_{scale_name}.json'\n",
    "\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(output_data, f, indent=2)\n",
    "\n",
    "            file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "\n",
    "            print(f\"\\n✅ {scale_name.upper()} SCALE COMPLETED:\")\n",
    "            print(f\"   📁 File: {output_file}\")\n",
    "            print(f\"   📦 Size: {file_size_mb:.1f} MB\")\n",
    "            print(f\"   📊 Playlists: {len(sampled_playlists):,}\")\n",
    "            print(f\"   📊 Total nodes: {stats['actual_scale']['total_nodes']:,}\")\n",
    "\n",
    "            # Store results\n",
    "            results[scale_name] = {\n",
    "                'file_path': output_file,\n",
    "                'playlists': len(sampled_playlists),\n",
    "                'total_nodes': stats['actual_scale']['total_nodes'],\n",
    "                'file_size_mb': file_size_mb,\n",
    "                'stats': stats\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to create {scale_name} scale: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"🎉 MULTI-SCALE DATASET CREATION COMPLETED!\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if results:\n",
    "        print(\"📊 CREATED DATASETS:\")\n",
    "        print(\"Scale    File                                 Playlists  Nodes     Size\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for scale_name, result in results.items():\n",
    "            filename = os.path.basename(result['file_path'])\n",
    "            print(f\"{scale_name:<8} {filename:<35} {result['playlists']:<10,} {result['total_nodes']:<9,} {result['file_size_mb']:.1f}MB\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_single_scale_sampling(file_pattern: str,\n",
    "                             scale_name: str = 'small',\n",
    "                             target_playlists: int = None):\n",
    "    \"\"\"Create a single scale dataset\"\"\"\n",
    "\n",
    "    # Default targets for each scale\n",
    "    default_targets = {\n",
    "        'tiny': 300,\n",
    "        'small': 600,\n",
    "        'medium': 1000,\n",
    "        'large': 1500\n",
    "    }\n",
    "\n",
    "    if target_playlists is None:\n",
    "        target_playlists = default_targets.get(scale_name, 600)\n",
    "\n",
    "    print(f\"🎯 CREATING SINGLE {scale_name.upper()} SCALE DATASET\")\n",
    "    print(f\"Target: {target_playlists:,} playlists\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = MultiScaleHybridStreamingSampler(\n",
    "        scale_name=scale_name,\n",
    "        target_playlists=target_playlists,\n",
    "        batch_size=20,\n",
    "        min_playlist_length=10,\n",
    "        max_playlist_length=100\n",
    "    )\n",
    "\n",
    "    # Run sampling\n",
    "    sampled_playlists, stats = sampler.run_hybrid_sampling(file_pattern)\n",
    "\n",
    "    # Create output data\n",
    "    output_data = {\n",
    "        'info': {\n",
    "            'generated_on': datetime.now().isoformat(),\n",
    "            'sampling_method': 'multi_scale_hybrid_core_based_stratified_streaming',\n",
    "            'scale': scale_name,\n",
    "            'target_playlists': target_playlists,\n",
    "            'parameters': get_scale_parameters(scale_name, target_playlists)\n",
    "        },\n",
    "        'sampling_stats': stats,\n",
    "        'playlists': sampled_playlists\n",
    "    }\n",
    "\n",
    "    # Save file\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    output_file = f'../data/processed/spotify_scaled_hybrid_{scale_name}.json'\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "\n",
    "    print(f\"\\n✅ {scale_name.upper()} SCALE COMPLETED:\")\n",
    "    print(f\"   📁 File: {output_file}\")\n",
    "    print(f\"   📦 Size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"   📊 Playlists: {len(sampled_playlists):,}\")\n",
    "    print(f\"   📊 Total nodes: {stats['actual_scale']['total_nodes']:,}\")\n",
    "\n",
    "    expected_time = {\n",
    "        'tiny': '15-30 minutes',\n",
    "        'small': '45-75 minutes',\n",
    "        'medium': '2-3 hours',\n",
    "        'large': '4-6 hours'\n",
    "    }.get(scale_name, 'Unknown')\n",
    "\n",
    "    print(f\"   ⏱️ Expected experiment time: {expected_time}\")\n",
    "\n",
    "    return sampled_playlists, stats, output_file\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compare_scales_detailed():\n",
    "    \"\"\"Detailed comparison of all scales\"\"\"\n",
    "\n",
    "    scales_info = {\n",
    "        'tiny': {\n",
    "            'playlists': 300,\n",
    "            'nodes': '~5K',\n",
    "            'experiment_time': '15-30 min',\n",
    "            'min_track_freq': 3,\n",
    "            'min_user_playlists': 3\n",
    "        },\n",
    "        'small': {\n",
    "            'playlists': 600,\n",
    "            'nodes': '~10K',\n",
    "            'experiment_time': '45-75 min',\n",
    "            'min_track_freq': 5,\n",
    "            'min_user_playlists': 5\n",
    "        },\n",
    "        'medium': {\n",
    "            'playlists': 1000,\n",
    "            'nodes': '~20K',\n",
    "            'experiment_time': '2-3 hours',\n",
    "            'min_track_freq': 6,\n",
    "            'min_user_playlists': 7\n",
    "        },\n",
    "        'large': {\n",
    "            'playlists': 1500,\n",
    "            'nodes': '~30K',\n",
    "            'experiment_time': '4-6 hours',\n",
    "            'min_track_freq': 8,\n",
    "            'min_user_playlists': 10\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"📊 DETAILED SCALE COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Scale   Playlists  Nodes   Exp.Time   Track_Freq  User_Freq\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for scale, info in scales_info.items():\n",
    "        print(f\"{scale:<7} {info['playlists']:<10} {info['nodes']:<7} {info['experiment_time']:<10} \"\n",
    "              f\"{info['min_track_freq']:<11} {info['min_user_playlists']:<10}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 MULTI-SCALE SPOTIFY SAMPLER\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Show scale comparison\n",
    "    compare_scales_detailed()\n",
    "\n",
    "    print(f\"\\n🔧 USAGE OPTIONS:\")\n",
    "    print(f\"1. Create all scales at once:\")\n",
    "    print(f\"   run_multi_scale_sampling(file_pattern)\")\n",
    "    print(f\"\")\n",
    "    print(f\"2. Create single scale:\")\n",
    "    print(f\"   run_single_scale_sampling(file_pattern, 'small')\")\n",
    "    print(f\"\")\n",
    "    print(f\"3. Create custom scales:\")\n",
    "    print(f\"   custom_scales = {{'quick': 200, 'normal': 800}}\")\n",
    "    print(f\"   run_multi_scale_sampling(file_pattern, custom_scales)\")\n",
    "\n",
    "    # Example usage\n",
    "    file_pattern = \"../data/raw/data/mpd.slice.*.json\"\n",
    "    run_multi_scale_sampling(file_pattern)"
   ],
   "id": "1a1cdc82241571b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 MULTI-SCALE SPOTIFY SAMPLER\n",
      "============================================================\n",
      "📊 DETAILED SCALE COMPARISON\n",
      "====================================================================================================\n",
      "Scale   Playlists  Nodes   Exp.Time   Track_Freq  User_Freq\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tiny    300        ~5K     15-30 min  3           3         \n",
      "small   600        ~10K    45-75 min  5           5         \n",
      "medium  1000       ~20K    2-3 hours  6           7         \n",
      "large   1500       ~30K    4-6 hours  8           10        \n",
      "\n",
      "🔧 USAGE OPTIONS:\n",
      "1. Create all scales at once:\n",
      "   run_multi_scale_sampling(file_pattern)\n",
      "\n",
      "2. Create single scale:\n",
      "   run_single_scale_sampling(file_pattern, 'small')\n",
      "\n",
      "3. Create custom scales:\n",
      "   custom_scales = {'quick': 200, 'normal': 800}\n",
      "   run_multi_scale_sampling(file_pattern, custom_scales)\n",
      "🎯 MULTI-SCALE DATASET CREATION\n",
      "============================================================\n",
      "Creating multiple scale datasets for flexible experimentation\n",
      "\n",
      "📊 SCALES TO CREATE:\n",
      "Scale    Playlists  Expected Nodes  Expected Time\n",
      "--------------------------------------------------\n",
      "tiny     300        5,000           15-30 min\n",
      "small    600        10,000          45-75 min\n",
      "medium   1000       20,000          2-3 hours\n",
      "large    1500       30,000          4-6 hours\n",
      "\n",
      "\n",
      "======================================================================\n",
      "🔧 CREATING TINY SCALE DATASET\n",
      "Target: 300 playlists\n",
      "======================================================================\n",
      "🎯 MULTI-SCALE SAMPLER (TINY SCALE)\n",
      "======================================================================\n",
      "📊 SCALE-OPTIMIZED PARAMETERS:\n",
      "   • Target playlists: 300\n",
      "   • Min track frequency: 3\n",
      "   • Min user playlists: 3\n",
      "   • Expected total nodes: ~5,000\n",
      "\n",
      "🚀 STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "💾 Memory usage after start: 156.6 MB\n",
      "🔍 PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "📁 Found 1000 files\n",
      "📦 Created 50 batches of ~20 files each\n",
      "🚀 Applying Core-Based Filters:\n",
      "   ✅ Step 1: Playlist length (10-100 tracks)\n",
      "   ✅ Step 2: User activity (≥3 playlists per user)\n",
      "   ✅ Step 3: Track frequency (≥3 appearances)\n",
      "\n",
      "📊 Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ✅ Identified 820 active users (target: ~100)\n",
      "\n",
      "🔍 Sub-pass 1b: Applying all core filters...\n",
      "📦 Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,846 valid so far\n",
      "💾 Memory usage after batch 1: 1433.1 MB\n",
      "📦 Processing batch 2/50\n",
      "📦 Processing batch 3/50\n",
      "📦 Processing batch 4/50\n",
      "📦 Processing batch 5/50\n",
      "📦 Processing batch 6/50\n",
      "📦 Processing batch 7/50\n",
      "📦 Processing batch 8/50\n",
      "📦 Processing batch 9/50\n",
      "📦 Processing batch 10/50\n",
      "📦 Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,317 valid so far\n",
      "💾 Memory usage after batch 11: 2282.7 MB\n",
      "📦 Processing batch 12/50\n",
      "📦 Processing batch 13/50\n",
      "📦 Processing batch 14/50\n",
      "📦 Processing batch 15/50\n",
      "📦 Processing batch 16/50\n",
      "📦 Processing batch 17/50\n",
      "📦 Processing batch 18/50\n",
      "📦 Processing batch 19/50\n",
      "📦 Processing batch 20/50\n",
      "📦 Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,973 valid so far\n",
      "💾 Memory usage after batch 21: 2803.0 MB\n",
      "📦 Processing batch 22/50\n",
      "📦 Processing batch 23/50\n",
      "📦 Processing batch 24/50\n",
      "📦 Processing batch 25/50\n",
      "📦 Processing batch 26/50\n",
      "📦 Processing batch 27/50\n",
      "📦 Processing batch 28/50\n",
      "📦 Processing batch 29/50\n",
      "📦 Processing batch 30/50\n",
      "📦 Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,887 valid so far\n",
      "💾 Memory usage after batch 31: 3616.5 MB\n",
      "📦 Processing batch 32/50\n",
      "📦 Processing batch 33/50\n",
      "📦 Processing batch 34/50\n",
      "📦 Processing batch 35/50\n",
      "📦 Processing batch 36/50\n",
      "📦 Processing batch 37/50\n",
      "📦 Processing batch 38/50\n",
      "📦 Processing batch 39/50\n",
      "📦 Processing batch 40/50\n",
      "📦 Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,910 valid so far\n",
      "💾 Memory usage after batch 41: 4364.2 MB\n",
      "📦 Processing batch 42/50\n",
      "📦 Processing batch 43/50\n",
      "📦 Processing batch 44/50\n",
      "📦 Processing batch 45/50\n",
      "📦 Processing batch 46/50\n",
      "📦 Processing batch 47/50\n",
      "📦 Processing batch 48/50\n",
      "📦 Processing batch 49/50\n",
      "📦 Processing batch 50/50\n",
      "\n",
      "🔍 Applying final core filter: Track frequency (≥3)\n",
      "   ✅ Identified 552,365 core tracks (target: ~2000)\n",
      "\n",
      "✅ CORE-BASED FILTERING COMPLETE:\n",
      "   📊 Filtering Funnel:\n",
      "      • Total playlists: 1,000,000\n",
      "      • Length filter: 747,510 (74.8%)\n",
      "      • User filter: 747,475 (74.7%)\n",
      "      • Track frequency: 747,177 (74.7%)\n",
      "      • 🎯 FINAL VALID: 747,177 (74.7%)\n",
      "\n",
      "💾 Memory usage after pass 1 complete: 3865.2 MB\n",
      "📊 CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   📅 Temporal split at timestamp: 1487894400.0\n",
      "   👥 User activity split at: 73.0 playlists per user\n",
      "   📋 Strata Distribution:\n",
      "      • short_old_casual    :  2,250 ( 0.3%)\n",
      "      • short_old_active    : 152,867 (20.5%)\n",
      "      • short_recent_casual :  1,591 ( 0.2%)\n",
      "      • short_recent_active : 115,998 (15.5%)\n",
      "      • medium_old_casual   :  1,619 ( 0.2%)\n",
      "      • medium_old_active   : 133,714 (17.9%)\n",
      "      • medium_recent_casual:  1,724 ( 0.2%)\n",
      "      • medium_recent_active: 142,281 (19.0%)\n",
      "      • long_old_casual     :    968 ( 0.1%)\n",
      "      • long_old_active     : 81,510 (10.9%)\n",
      "      • long_recent_casual  :  1,233 ( 0.2%)\n",
      "      • long_recent_active  : 111,422 (14.9%)\n",
      "\n",
      "💾 Memory usage after strata created: 3289.1 MB\n",
      "🎲 PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   📊 Global sampling ratio: 0.000\n",
      "   🏆 Using priority scoring within strata\n",
      "\n",
      "      • short_old_casual    :    1 / 2,250 (avg score: 11.83)\n",
      "      • short_old_active    :   61 / 152,867 (avg score: 13.98)\n",
      "      • short_recent_casual :    1 / 1,591 (avg score: 13.93)\n",
      "      • short_recent_active :   46 / 115,998 (avg score: 13.68)\n",
      "      • medium_old_casual   :    1 / 1,619 (avg score: 11.52)\n",
      "      • medium_old_active   :   53 / 133,714 (avg score: 14.45)\n",
      "      • medium_recent_casual:    1 / 1,724 (avg score: 11.42)\n",
      "      • medium_recent_active:   57 / 142,281 (avg score: 14.26)\n",
      "      • long_old_casual     :    1 /   968 (avg score: 13.00)\n",
      "      • long_old_active     :   32 / 81,510 (avg score: 14.04)\n",
      "      • long_recent_casual  :    1 / 1,233 (avg score: 12.65)\n",
      "      • long_recent_active  :   44 / 111,422 (avg score: 13.83)\n",
      "\n",
      "   🎯 Selected 299 playlists for final loading\n",
      "   📁 Loading selected playlists...\n",
      "   📂 Loading from 260 files\n",
      "      📖 File 1/260\n",
      "      📖 File 101/260\n",
      "      📖 File 201/260\n",
      "   ✅ Loaded 299 final playlists\n",
      "💾 Memory usage after pass 2 complete: 1255.2 MB\n",
      "\n",
      "🔍 VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   📊 Final Entity Counts:\n",
      "      • Playlists: 299 (target: 300)\n",
      "      • Tracks: 10,631 (target: ~2,000)\n",
      "      • Artists: 4,164 (target: ~500)\n",
      "      • Albums: 6,966 (target: ~400)\n",
      "      • Users: 120 (target: ~100)\n",
      "      🎯 TOTAL: 22,180 (target: ~5,000)\n",
      "\n",
      "🎉 MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "📊 Results: 1,000,000 → 299 playlists\n",
      "📈 Overall retention: 0.0%\n",
      "\n",
      "🎯 EXPERIMENTAL SCALE VERIFICATION:\n",
      "   • Actual total nodes: 22,180\n",
      "   • Target total nodes: 5,000\n",
      "   • Scale ratio: 4.436 (⚠️ ADJUST)\n",
      "\n",
      "✅ TINY SCALE COMPLETED:\n",
      "   📁 File: ../data/processed/spotify_scaled_hybrid_tiny.json\n",
      "   📦 Size: 5.1 MB\n",
      "   📊 Playlists: 299\n",
      "   📊 Total nodes: 22,180\n",
      "\n",
      "======================================================================\n",
      "🔧 CREATING SMALL SCALE DATASET\n",
      "Target: 600 playlists\n",
      "======================================================================\n",
      "🎯 MULTI-SCALE SAMPLER (SMALL SCALE)\n",
      "======================================================================\n",
      "📊 SCALE-OPTIMIZED PARAMETERS:\n",
      "   • Target playlists: 600\n",
      "   • Min track frequency: 5\n",
      "   • Min user playlists: 5\n",
      "   • Expected total nodes: ~10,000\n",
      "\n",
      "🚀 STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "💾 Memory usage after start: 1023.1 MB\n",
      "🔍 PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "📁 Found 1000 files\n",
      "📦 Created 50 batches of ~20 files each\n",
      "🚀 Applying Core-Based Filters:\n",
      "   ✅ Step 1: Playlist length (10-100 tracks)\n",
      "   ✅ Step 2: User activity (≥5 playlists per user)\n",
      "   ✅ Step 3: Track frequency (≥5 appearances)\n",
      "\n",
      "📊 Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ✅ Identified 901 active users (target: ~200)\n",
      "\n",
      "🔍 Sub-pass 1b: Applying all core filters...\n",
      "📦 Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,845 valid so far\n",
      "💾 Memory usage after batch 1: 1561.9 MB\n",
      "📦 Processing batch 2/50\n",
      "📦 Processing batch 3/50\n",
      "📦 Processing batch 4/50\n",
      "📦 Processing batch 5/50\n",
      "📦 Processing batch 6/50\n",
      "📦 Processing batch 7/50\n",
      "📦 Processing batch 8/50\n",
      "📦 Processing batch 9/50\n",
      "📦 Processing batch 10/50\n",
      "📦 Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,309 valid so far\n",
      "💾 Memory usage after batch 11: 2298.8 MB\n",
      "📦 Processing batch 12/50\n",
      "📦 Processing batch 13/50\n",
      "📦 Processing batch 14/50\n",
      "📦 Processing batch 15/50\n",
      "📦 Processing batch 16/50\n",
      "📦 Processing batch 17/50\n",
      "📦 Processing batch 18/50\n",
      "📦 Processing batch 19/50\n",
      "📦 Processing batch 20/50\n",
      "📦 Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,951 valid so far\n",
      "💾 Memory usage after batch 21: 2873.7 MB\n",
      "📦 Processing batch 22/50\n",
      "📦 Processing batch 23/50\n",
      "📦 Processing batch 24/50\n",
      "📦 Processing batch 25/50\n",
      "📦 Processing batch 26/50\n",
      "📦 Processing batch 27/50\n",
      "📦 Processing batch 28/50\n",
      "📦 Processing batch 29/50\n",
      "📦 Processing batch 30/50\n",
      "📦 Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,853 valid so far\n",
      "💾 Memory usage after batch 31: 3762.2 MB\n",
      "📦 Processing batch 32/50\n",
      "📦 Processing batch 33/50\n",
      "📦 Processing batch 34/50\n",
      "📦 Processing batch 35/50\n",
      "📦 Processing batch 36/50\n",
      "📦 Processing batch 37/50\n",
      "📦 Processing batch 38/50\n",
      "📦 Processing batch 39/50\n",
      "📦 Processing batch 40/50\n",
      "📦 Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,867 valid so far\n",
      "💾 Memory usage after batch 41: 4553.5 MB\n",
      "📦 Processing batch 42/50\n",
      "📦 Processing batch 43/50\n",
      "📦 Processing batch 44/50\n",
      "📦 Processing batch 45/50\n",
      "📦 Processing batch 46/50\n",
      "📦 Processing batch 47/50\n",
      "📦 Processing batch 48/50\n",
      "📦 Processing batch 49/50\n",
      "📦 Processing batch 50/50\n",
      "\n",
      "🔍 Applying final core filter: Track frequency (≥5)\n",
      "   ✅ Identified 375,364 core tracks (target: ~4000)\n",
      "\n",
      "✅ CORE-BASED FILTERING COMPLETE:\n",
      "   📊 Filtering Funnel:\n",
      "      • Total playlists: 1,000,000\n",
      "      • Length filter: 747,510 (74.8%)\n",
      "      • User filter: 747,424 (74.7%)\n",
      "      • Track frequency: 746,735 (74.7%)\n",
      "      • 🎯 FINAL VALID: 746,735 (74.7%)\n",
      "\n",
      "💾 Memory usage after pass 1 complete: 5098.2 MB\n",
      "📊 CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   📅 Temporal split at timestamp: 1487894400.0\n",
      "   👥 User activity split at: 63.0 playlists per user\n",
      "   📋 Strata Distribution:\n",
      "      • short_old_casual    :  2,650 ( 0.4%)\n",
      "      • short_old_active    : 152,326 (20.4%)\n",
      "      • short_recent_casual :  2,180 ( 0.3%)\n",
      "      • short_recent_active : 115,313 (15.4%)\n",
      "      • medium_old_casual   :  1,903 ( 0.3%)\n",
      "      • medium_old_active   : 133,339 (17.9%)\n",
      "      • medium_recent_casual:  2,309 ( 0.3%)\n",
      "      • medium_recent_active: 141,651 (19.0%)\n",
      "      • long_old_casual     :  1,114 ( 0.1%)\n",
      "      • long_old_active     : 81,317 (10.9%)\n",
      "      • long_recent_casual  :  1,657 ( 0.2%)\n",
      "      • long_recent_active  : 110,976 (14.9%)\n",
      "\n",
      "💾 Memory usage after strata created: 5124.6 MB\n",
      "🎲 PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   📊 Global sampling ratio: 0.001\n",
      "   🏆 Using priority scoring within strata\n",
      "\n",
      "      • short_old_casual    :    2 / 2,650 (avg score: 11.91)\n",
      "      • short_old_active    :  122 / 152,326 (avg score: 13.12)\n",
      "      • short_recent_casual :    1 / 2,180 (avg score: 13.93)\n",
      "      • short_recent_active :   92 / 115,313 (avg score: 12.81)\n",
      "      • medium_old_casual   :    1 / 1,903 (avg score: 11.52)\n",
      "      • medium_old_active   :  107 / 133,339 (avg score: 13.55)\n",
      "      • medium_recent_casual:    1 / 2,309 (avg score: 11.42)\n",
      "      • medium_recent_active:  113 / 141,651 (avg score: 13.46)\n",
      "      • long_old_casual     :    1 / 1,114 (avg score: 13.00)\n",
      "      • long_old_active     :   65 / 81,317 (avg score: 13.25)\n",
      "      • long_recent_casual  :    1 / 1,657 (avg score: 12.65)\n",
      "      • long_recent_active  :   89 / 110,976 (avg score: 13.01)\n",
      "\n",
      "   🎯 Selected 595 playlists for final loading\n",
      "   📁 Loading selected playlists...\n",
      "   📂 Loading from 449 files\n",
      "      📖 File 1/449\n",
      "      📖 File 101/449\n",
      "      📖 File 201/449\n",
      "      📖 File 301/449\n",
      "      📖 File 401/449\n",
      "   ✅ Loaded 595 final playlists\n",
      "💾 Memory usage after pass 2 complete: 4620.0 MB\n",
      "\n",
      "🔍 VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   📊 Final Entity Counts:\n",
      "      • Playlists: 595 (target: 600)\n",
      "      • Tracks: 18,749 (target: ~4,000)\n",
      "      • Artists: 6,585 (target: ~1,000)\n",
      "      • Albums: 11,914 (target: ~800)\n",
      "      • Users: 162 (target: ~200)\n",
      "      🎯 TOTAL: 38,005 (target: ~10,000)\n",
      "\n",
      "🎉 MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "📊 Results: 1,000,000 → 595 playlists\n",
      "📈 Overall retention: 0.1%\n",
      "\n",
      "🎯 EXPERIMENTAL SCALE VERIFICATION:\n",
      "   • Actual total nodes: 38,005\n",
      "   • Target total nodes: 10,000\n",
      "   • Scale ratio: 3.800 (⚠️ ADJUST)\n",
      "\n",
      "✅ SMALL SCALE COMPLETED:\n",
      "   📁 File: ../data/processed/spotify_scaled_hybrid_small.json\n",
      "   📦 Size: 10.1 MB\n",
      "   📊 Playlists: 595\n",
      "   📊 Total nodes: 38,005\n",
      "\n",
      "======================================================================\n",
      "🔧 CREATING MEDIUM SCALE DATASET\n",
      "Target: 1,000 playlists\n",
      "======================================================================\n",
      "🎯 MULTI-SCALE SAMPLER (MEDIUM SCALE)\n",
      "======================================================================\n",
      "📊 SCALE-OPTIMIZED PARAMETERS:\n",
      "   • Target playlists: 1,000\n",
      "   • Min track frequency: 6\n",
      "   • Min user playlists: 7\n",
      "   • Expected total nodes: ~20,000\n",
      "\n",
      "🚀 STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "💾 Memory usage after start: 2332.5 MB\n",
      "🔍 PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "📁 Found 1000 files\n",
      "📦 Created 50 batches of ~20 files each\n",
      "🚀 Applying Core-Based Filters:\n",
      "   ✅ Step 1: Playlist length (10-100 tracks)\n",
      "   ✅ Step 2: User activity (≥7 playlists per user)\n",
      "   ✅ Step 3: Track frequency (≥6 appearances)\n",
      "\n",
      "📊 Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ✅ Identified 2,827 active users (target: ~300)\n",
      "\n",
      "🔍 Sub-pass 1b: Applying all core filters...\n",
      "📦 Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,829 valid so far\n",
      "💾 Memory usage after batch 1: 1732.1 MB\n",
      "📦 Processing batch 2/50\n",
      "📦 Processing batch 3/50\n",
      "📦 Processing batch 4/50\n",
      "📦 Processing batch 5/50\n",
      "📦 Processing batch 6/50\n",
      "📦 Processing batch 7/50\n",
      "📦 Processing batch 8/50\n",
      "📦 Processing batch 9/50\n",
      "📦 Processing batch 10/50\n",
      "📦 Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,122 valid so far\n",
      "💾 Memory usage after batch 11: 2392.5 MB\n",
      "📦 Processing batch 12/50\n",
      "📦 Processing batch 13/50\n",
      "📦 Processing batch 14/50\n",
      "📦 Processing batch 15/50\n",
      "📦 Processing batch 16/50\n",
      "📦 Processing batch 17/50\n",
      "📦 Processing batch 18/50\n",
      "📦 Processing batch 19/50\n",
      "📦 Processing batch 20/50\n",
      "📦 Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,570 valid so far\n",
      "💾 Memory usage after batch 21: 3110.8 MB\n",
      "📦 Processing batch 22/50\n",
      "📦 Processing batch 23/50\n",
      "📦 Processing batch 24/50\n",
      "📦 Processing batch 25/50\n",
      "📦 Processing batch 26/50\n",
      "📦 Processing batch 27/50\n",
      "📦 Processing batch 28/50\n",
      "📦 Processing batch 29/50\n",
      "📦 Processing batch 30/50\n",
      "📦 Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,283 valid so far\n",
      "💾 Memory usage after batch 31: 3845.2 MB\n",
      "📦 Processing batch 32/50\n",
      "📦 Processing batch 33/50\n",
      "📦 Processing batch 34/50\n",
      "📦 Processing batch 35/50\n",
      "📦 Processing batch 36/50\n",
      "📦 Processing batch 37/50\n",
      "📦 Processing batch 38/50\n",
      "📦 Processing batch 39/50\n",
      "📦 Processing batch 40/50\n",
      "📦 Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,140 valid so far\n",
      "💾 Memory usage after batch 41: 4502.1 MB\n",
      "📦 Processing batch 42/50\n",
      "📦 Processing batch 43/50\n",
      "📦 Processing batch 44/50\n",
      "📦 Processing batch 45/50\n",
      "📦 Processing batch 46/50\n",
      "📦 Processing batch 47/50\n",
      "📦 Processing batch 48/50\n",
      "📦 Processing batch 49/50\n",
      "📦 Processing batch 50/50\n",
      "\n",
      "🔍 Applying final core filter: Track frequency (≥6)\n",
      "   ✅ Identified 328,152 core tracks (target: ~8000)\n",
      "\n",
      "✅ CORE-BASED FILTERING COMPLETE:\n",
      "   📊 Filtering Funnel:\n",
      "      • Total playlists: 1,000,000\n",
      "      • Length filter: 747,510 (74.8%)\n",
      "      • User filter: 746,560 (74.7%)\n",
      "      • Track frequency: 745,704 (74.6%)\n",
      "      • 🎯 FINAL VALID: 745,704 (74.6%)\n",
      "\n",
      "💾 Memory usage after pass 1 complete: 4556.1 MB\n",
      "📊 CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   📅 Temporal split at timestamp: 1487980800.0\n",
      "   👥 User activity split at: 40.0 playlists per user\n",
      "   📋 Strata Distribution:\n",
      "      • short_old_casual    :  6,209 ( 0.8%)\n",
      "      • short_old_active    : 148,760 (19.9%)\n",
      "      • short_recent_casual :  4,941 ( 0.7%)\n",
      "      • short_recent_active : 112,086 (15.0%)\n",
      "      • medium_old_casual   :  4,647 ( 0.6%)\n",
      "      • medium_old_active   : 130,675 (17.5%)\n",
      "      • medium_recent_casual:  5,262 ( 0.7%)\n",
      "      • medium_recent_active: 138,273 (18.5%)\n",
      "      • long_old_casual     :  2,646 ( 0.4%)\n",
      "      • long_old_active     : 79,853 (10.7%)\n",
      "      • long_recent_casual  :  3,715 ( 0.5%)\n",
      "      • long_recent_active  : 108,637 (14.6%)\n",
      "\n",
      "💾 Memory usage after strata created: 2653.2 MB\n",
      "🎲 PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   📊 Global sampling ratio: 0.001\n",
      "   🏆 Using priority scoring within strata\n",
      "\n",
      "      • short_old_casual    :    8 / 6,209 (avg score: 12.44)\n",
      "      • short_old_active    :  199 / 148,760 (avg score: 12.40)\n",
      "      • short_recent_casual :    6 / 4,941 (avg score: 11.73)\n",
      "      • short_recent_active :  150 / 112,086 (avg score: 12.21)\n",
      "      • medium_old_casual   :    6 / 4,647 (avg score: 13.28)\n",
      "      • medium_old_active   :  175 / 130,675 (avg score: 12.72)\n",
      "      • medium_recent_casual:    7 / 5,262 (avg score: 11.85)\n",
      "      • medium_recent_active:  185 / 138,273 (avg score: 12.71)\n",
      "      • long_old_casual     :    3 / 2,646 (avg score: 12.75)\n",
      "      • long_old_active     :  107 / 79,853 (avg score: 12.62)\n",
      "      • long_recent_casual  :    4 / 3,715 (avg score: 12.49)\n",
      "      • long_recent_active  :  145 / 108,637 (avg score: 12.34)\n",
      "\n",
      "   🎯 Selected 995 playlists for final loading\n",
      "   📁 Loading selected playlists...\n",
      "   📂 Loading from 635 files\n",
      "      📖 File 1/635\n",
      "      📖 File 101/635\n",
      "      📖 File 201/635\n",
      "      📖 File 301/635\n",
      "      📖 File 401/635\n",
      "      📖 File 501/635\n",
      "      📖 File 601/635\n",
      "   ✅ Loaded 995 final playlists\n",
      "💾 Memory usage after pass 2 complete: 5270.6 MB\n",
      "\n",
      "🔍 VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   📊 Final Entity Counts:\n",
      "      • Playlists: 995 (target: 1,000)\n",
      "      • Tracks: 28,672 (target: ~8,000)\n",
      "      • Artists: 9,504 (target: ~2,000)\n",
      "      • Albums: 17,710 (target: ~1,500)\n",
      "      • Users: 413 (target: ~300)\n",
      "      🎯 TOTAL: 57,294 (target: ~20,000)\n",
      "\n",
      "🎉 MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "📊 Results: 1,000,000 → 995 playlists\n",
      "📈 Overall retention: 0.1%\n",
      "\n",
      "🎯 EXPERIMENTAL SCALE VERIFICATION:\n",
      "   • Actual total nodes: 57,294\n",
      "   • Target total nodes: 20,000\n",
      "   • Scale ratio: 2.865 (⚠️ ADJUST)\n",
      "\n",
      "✅ MEDIUM SCALE COMPLETED:\n",
      "   📁 File: ../data/processed/spotify_scaled_hybrid_medium.json\n",
      "   📦 Size: 16.9 MB\n",
      "   📊 Playlists: 995\n",
      "   📊 Total nodes: 57,294\n",
      "\n",
      "======================================================================\n",
      "🔧 CREATING LARGE SCALE DATASET\n",
      "Target: 1,500 playlists\n",
      "======================================================================\n",
      "🎯 MULTI-SCALE SAMPLER (LARGE SCALE)\n",
      "======================================================================\n",
      "📊 SCALE-OPTIMIZED PARAMETERS:\n",
      "   • Target playlists: 1,500\n",
      "   • Min track frequency: 8\n",
      "   • Min user playlists: 10\n",
      "   • Expected total nodes: ~30,000\n",
      "\n",
      "🚀 STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "💾 Memory usage after start: 3500.0 MB\n",
      "🔍 PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "📁 Found 1000 files\n",
      "📦 Created 50 batches of ~20 files each\n",
      "🚀 Applying Core-Based Filters:\n",
      "   ✅ Step 1: Playlist length (10-100 tracks)\n",
      "   ✅ Step 2: User activity (≥10 playlists per user)\n",
      "   ✅ Step 3: Track frequency (≥8 appearances)\n",
      "\n",
      "📊 Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ✅ Identified 2,852 active users (target: ~500)\n",
      "\n",
      "🔍 Sub-pass 1b: Applying all core filters...\n",
      "📦 Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,810 valid so far\n",
      "💾 Memory usage after batch 1: 1676.4 MB\n",
      "📦 Processing batch 2/50\n",
      "📦 Processing batch 3/50\n",
      "📦 Processing batch 4/50\n",
      "📦 Processing batch 5/50\n",
      "📦 Processing batch 6/50\n",
      "📦 Processing batch 7/50\n",
      "📦 Processing batch 8/50\n",
      "📦 Processing batch 9/50\n",
      "📦 Processing batch 10/50\n",
      "📦 Processing batch 11/50\n",
      "   Progress: 220,000 seen, 163,923 valid so far\n",
      "💾 Memory usage after batch 11: 2370.4 MB\n",
      "📦 Processing batch 12/50\n",
      "📦 Processing batch 13/50\n",
      "📦 Processing batch 14/50\n",
      "📦 Processing batch 15/50\n",
      "📦 Processing batch 16/50\n",
      "📦 Processing batch 17/50\n",
      "📦 Processing batch 18/50\n",
      "📦 Processing batch 19/50\n",
      "📦 Processing batch 20/50\n",
      "📦 Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,189 valid so far\n",
      "💾 Memory usage after batch 21: 3188.5 MB\n",
      "📦 Processing batch 22/50\n",
      "📦 Processing batch 23/50\n",
      "📦 Processing batch 24/50\n",
      "📦 Processing batch 25/50\n",
      "📦 Processing batch 26/50\n",
      "📦 Processing batch 27/50\n",
      "📦 Processing batch 28/50\n",
      "📦 Processing batch 29/50\n",
      "📦 Processing batch 30/50\n",
      "📦 Processing batch 31/50\n",
      "   Progress: 620,000 seen, 462,710 valid so far\n",
      "💾 Memory usage after batch 31: 3828.1 MB\n",
      "📦 Processing batch 32/50\n",
      "📦 Processing batch 33/50\n",
      "📦 Processing batch 34/50\n",
      "📦 Processing batch 35/50\n",
      "📦 Processing batch 36/50\n",
      "📦 Processing batch 37/50\n",
      "📦 Processing batch 38/50\n",
      "📦 Processing batch 39/50\n",
      "📦 Processing batch 40/50\n",
      "📦 Processing batch 41/50\n",
      "   Progress: 820,000 seen, 611,422 valid so far\n",
      "💾 Memory usage after batch 41: 4559.0 MB\n",
      "📦 Processing batch 42/50\n",
      "📦 Processing batch 43/50\n",
      "📦 Processing batch 44/50\n",
      "📦 Processing batch 45/50\n",
      "📦 Processing batch 46/50\n",
      "📦 Processing batch 47/50\n",
      "📦 Processing batch 48/50\n",
      "📦 Processing batch 49/50\n",
      "📦 Processing batch 50/50\n",
      "\n",
      "🔍 Applying final core filter: Track frequency (≥8)\n",
      "   ✅ Identified 265,883 core tracks (target: ~12000)\n",
      "\n",
      "✅ CORE-BASED FILTERING COMPLETE:\n",
      "   📊 Filtering Funnel:\n",
      "      • Total playlists: 1,000,000\n",
      "      • Length filter: 747,510 (74.8%)\n",
      "      • User filter: 745,687 (74.6%)\n",
      "      • Track frequency: 744,508 (74.5%)\n",
      "      • 🎯 FINAL VALID: 744,508 (74.5%)\n",
      "\n",
      "💾 Memory usage after pass 1 complete: 5094.4 MB\n",
      "📊 CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   📅 Temporal split at timestamp: 1487980800.0\n",
      "   👥 User activity split at: 33.0 playlists per user\n",
      "   📋 Strata Distribution:\n",
      "      • short_old_casual    :  5,786 ( 0.8%)\n",
      "      • short_old_active    : 148,848 (20.0%)\n",
      "      • short_recent_casual :  5,193 ( 0.7%)\n",
      "      • short_recent_active : 111,624 (15.0%)\n",
      "      • medium_old_casual   :  4,457 ( 0.6%)\n",
      "      • medium_old_active   : 130,646 (17.5%)\n",
      "      • medium_recent_casual:  5,525 ( 0.7%)\n",
      "      • medium_recent_active: 137,843 (18.5%)\n",
      "      • long_old_casual     :  2,533 ( 0.3%)\n",
      "      • long_old_active     : 79,818 (10.7%)\n",
      "      • long_recent_casual  :  3,977 ( 0.5%)\n",
      "      • long_recent_active  : 108,258 (14.5%)\n",
      "\n",
      "💾 Memory usage after strata created: 5115.8 MB\n",
      "🎲 PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   📊 Global sampling ratio: 0.002\n",
      "   🏆 Using priority scoring within strata\n",
      "\n",
      "      • short_old_casual    :   11 / 5,786 (avg score: 12.02)\n",
      "      • short_old_active    :  299 / 148,848 (avg score: 11.83)\n",
      "      • short_recent_casual :   10 / 5,193 (avg score: 11.17)\n",
      "      • short_recent_active :  224 / 111,624 (avg score: 11.70)\n",
      "      • medium_old_casual   :    8 / 4,457 (avg score: 12.64)\n",
      "      • medium_old_active   :  263 / 130,646 (avg score: 12.15)\n",
      "      • medium_recent_casual:   11 / 5,525 (avg score: 11.31)\n",
      "      • medium_recent_active:  277 / 137,843 (avg score: 12.12)\n",
      "      • long_old_casual     :    5 / 2,533 (avg score: 12.31)\n",
      "      • long_old_active     :  160 / 79,818 (avg score: 12.05)\n",
      "      • long_recent_casual  :    8 / 3,977 (avg score: 11.67)\n",
      "      • long_recent_active  :  218 / 108,258 (avg score: 11.82)\n",
      "\n",
      "   🎯 Selected 1,494 playlists for final loading\n",
      "   📁 Loading selected playlists...\n",
      "   📂 Loading from 792 files\n",
      "      📖 File 1/792\n",
      "      📖 File 101/792\n",
      "      📖 File 201/792\n",
      "      📖 File 301/792\n",
      "      📖 File 401/792\n",
      "      📖 File 501/792\n",
      "      📖 File 601/792\n",
      "      📖 File 701/792\n",
      "   ✅ Loaded 1,494 final playlists\n",
      "💾 Memory usage after pass 2 complete: 2268.6 MB\n",
      "\n",
      "🔍 VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   📊 Final Entity Counts:\n",
      "      • Playlists: 1,494 (target: 1,500)\n",
      "      • Tracks: 39,213 (target: ~12,000)\n",
      "      • Artists: 12,268 (target: ~3,000)\n",
      "      • Albums: 23,843 (target: ~2,500)\n",
      "      • Users: 531 (target: ~500)\n",
      "      🎯 TOTAL: 77,349 (target: ~30,000)\n",
      "\n",
      "🎉 MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "📊 Results: 1,000,000 → 1,494 playlists\n",
      "📈 Overall retention: 0.1%\n",
      "\n",
      "🎯 EXPERIMENTAL SCALE VERIFICATION:\n",
      "   • Actual total nodes: 77,349\n",
      "   • Target total nodes: 30,000\n",
      "   • Scale ratio: 2.578 (⚠️ ADJUST)\n",
      "\n",
      "✅ LARGE SCALE COMPLETED:\n",
      "   📁 File: ../data/processed/spotify_scaled_hybrid_large.json\n",
      "   📦 Size: 25.4 MB\n",
      "   📊 Playlists: 1,494\n",
      "   📊 Total nodes: 77,349\n",
      "\n",
      "======================================================================\n",
      "🎉 MULTI-SCALE DATASET CREATION COMPLETED!\n",
      "======================================================================\n",
      "📊 CREATED DATASETS:\n",
      "Scale    File                                 Playlists  Nodes     Size\n",
      "--------------------------------------------------------------------------------\n",
      "tiny     spotify_scaled_hybrid_tiny.json     299        22,180    5.1MB\n",
      "small    spotify_scaled_hybrid_small.json    595        38,005    10.1MB\n",
      "medium   spotify_scaled_hybrid_medium.json   995        57,294    16.9MB\n",
      "large    spotify_scaled_hybrid_large.json    1,494      77,349    25.4MB\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
