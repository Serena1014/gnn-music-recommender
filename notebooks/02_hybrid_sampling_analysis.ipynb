{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hybird Core-based + Stratified Sampling",
   "id": "784adb025faa243f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T15:19:03.110070Z",
     "start_time": "2025-08-15T14:34:46.431236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import gc\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# SCALE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_scale_parameters(scale_name: str, target_playlists: int) -> Dict:\n",
    "    \"\"\"Get optimized parameters for different scales\"\"\"\n",
    "\n",
    "    if target_playlists <= 300:  # tiny\n",
    "        return {\n",
    "            'min_track_frequency': 3,\n",
    "            'min_user_playlists': 3,\n",
    "            'expected_total_nodes': 5000,\n",
    "            'expected_tracks': 2000,\n",
    "            'expected_artists': 500,\n",
    "            'expected_albums': 400,\n",
    "            'expected_users': 100\n",
    "        }\n",
    "    elif target_playlists <= 600:  # small\n",
    "        return {\n",
    "            'min_track_frequency': 5,\n",
    "            'min_user_playlists': 5,\n",
    "            'expected_total_nodes': 10000,\n",
    "            'expected_tracks': 4000,\n",
    "            'expected_artists': 1000,\n",
    "            'expected_albums': 800,\n",
    "            'expected_users': 200\n",
    "        }\n",
    "    elif target_playlists <= 1000:  # medium\n",
    "        return {\n",
    "            'min_track_frequency': 6,\n",
    "            'min_user_playlists': 7,\n",
    "            'expected_total_nodes': 20000,\n",
    "            'expected_tracks': 8000,\n",
    "            'expected_artists': 2000,\n",
    "            'expected_albums': 1500,\n",
    "            'expected_users': 300\n",
    "        }\n",
    "    else:  # large (1500+)\n",
    "        return {\n",
    "            'min_track_frequency': 8,\n",
    "            'min_user_playlists': 10,\n",
    "            'expected_total_nodes': 30000,\n",
    "            'expected_tracks': 12000,\n",
    "            'expected_artists': 3000,\n",
    "            'expected_albums': 2500,\n",
    "            'expected_users': 500\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-SCALE HYBRID STREAMING SAMPLER\n",
    "# =============================================================================\n",
    "\n",
    "class MultiScaleHybridStreamingSampler:\n",
    "    \"\"\"\n",
    "    Multi-Scale Hybrid Core-Based + Stratified Streaming Sampler\n",
    "\n",
    "    Maintains original methodology with scale-optimized parameters:\n",
    "    - Pass 1: Core-based filtering (length, user activity, track frequency)\n",
    "    - Pass 2: Stratified sampling with priority scoring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 scale_name: str,\n",
    "                 target_playlists: int,\n",
    "                 batch_size: int = 20,\n",
    "                 min_playlist_length: int = 10,\n",
    "                 max_playlist_length: int = 100):\n",
    "\n",
    "        # Get scale-specific parameters\n",
    "        scale_params = get_scale_parameters(scale_name, target_playlists)\n",
    "\n",
    "        # Core parameters\n",
    "        self.scale_name = scale_name\n",
    "        self.target_playlists = target_playlists\n",
    "        self.batch_size = batch_size\n",
    "        self.min_playlist_length = min_playlist_length\n",
    "        self.max_playlist_length = max_playlist_length\n",
    "        self.min_track_frequency = scale_params['min_track_frequency']\n",
    "        self.min_user_playlists = scale_params['min_user_playlists']\n",
    "\n",
    "        # Expected targets for verification\n",
    "        self.expected_total_nodes = scale_params['expected_total_nodes']\n",
    "        self.expected_tracks = scale_params['expected_tracks']\n",
    "        self.expected_artists = scale_params['expected_artists']\n",
    "        self.expected_albums = scale_params['expected_albums']\n",
    "        self.expected_users = scale_params['expected_users']\n",
    "\n",
    "        # Statistics collectors\n",
    "        self.track_counts = Counter()\n",
    "        self.user_counts = Counter()\n",
    "        self.playlist_stats = []\n",
    "\n",
    "        print(f\"üéØ MULTI-SCALE SAMPLER ({scale_name.upper()} SCALE)\")\n",
    "        print(f\"=\" * 70)\n",
    "        print(f\"üìä SCALE-OPTIMIZED PARAMETERS:\")\n",
    "        print(f\"   ‚Ä¢ Target playlists: {target_playlists:,}\")\n",
    "        print(f\"   ‚Ä¢ Min track frequency: {scale_params['min_track_frequency']}\")\n",
    "        print(f\"   ‚Ä¢ Min user playlists: {scale_params['min_user_playlists']}\")\n",
    "        print(f\"   ‚Ä¢ Expected total nodes: ~{self.expected_total_nodes:,}\")\n",
    "        print()\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            return process.memory_info().rss / 1024 / 1024\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def print_memory_status(self, stage: str):\n",
    "        \"\"\"Print current memory usage\"\"\"\n",
    "        memory_mb = self.get_memory_usage()\n",
    "        print(f\"üíæ Memory usage after {stage}: {memory_mb:.1f} MB\")\n",
    "\n",
    "    def get_file_batches(self, file_pattern: str) -> List[List[str]]:\n",
    "        \"\"\"Split files into manageable batches\"\"\"\n",
    "        file_paths = glob.glob(file_pattern)\n",
    "        file_paths.sort()\n",
    "\n",
    "        if not file_paths:\n",
    "            raise FileNotFoundError(f\"No files found: {file_pattern}\")\n",
    "\n",
    "        print(f\"üìÅ Found {len(file_paths)} files\")\n",
    "\n",
    "        # Split into batches\n",
    "        batches = []\n",
    "        for i in range(0, len(file_paths), self.batch_size):\n",
    "            batch = file_paths[i:i + self.batch_size]\n",
    "            batches.append(batch)\n",
    "\n",
    "        print(f\"üì¶ Created {len(batches)} batches of ~{self.batch_size} files each\")\n",
    "        return batches\n",
    "\n",
    "    def _extract_user_id(self, playlist: Dict) -> str:\n",
    "        \"\"\"Extract user identifier with scale-appropriate consolidation\"\"\"\n",
    "        # Name-based grouping with consolidation based on scale\n",
    "        name = playlist.get('name', '').lower().strip()\n",
    "        if name:\n",
    "            words = name.split()\n",
    "            if words:\n",
    "                # Adjust consolidation level based on scale\n",
    "                char_count = 2 if self.target_playlists <= 600 else 3\n",
    "                user_base = words[0][:char_count]\n",
    "                user_id = ''.join(c for c in user_base if c.isalnum())\n",
    "                if user_id:\n",
    "                    return user_id\n",
    "\n",
    "        # PID-based consolidation\n",
    "        pid = playlist.get('pid', 0)\n",
    "        # Scale user bins based on target size\n",
    "        user_bins = max(100, self.target_playlists // 3)\n",
    "        return f\"u{pid % user_bins}\"\n",
    "\n",
    "    def pass1_core_filtering(self, file_pattern: str) -> Dict:\n",
    "        \"\"\"PASS 1: Core-based filtering + Statistics collection\"\"\"\n",
    "        print(\"üîç PASS 1: HYBRID CORE-BASED FILTERING\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        batches = self.get_file_batches(file_pattern)\n",
    "\n",
    "        stage_counts = {\n",
    "            'total_seen': 0,\n",
    "            'passed_length_filter': 0,\n",
    "            'passed_user_filter': 0,\n",
    "            'passed_track_frequency_filter': 0,\n",
    "            'final_valid': 0\n",
    "        }\n",
    "\n",
    "        print(\"üöÄ Applying Core-Based Filters:\")\n",
    "        print(f\"   ‚úÖ Step 1: Playlist length ({self.min_playlist_length}-{self.max_playlist_length} tracks)\")\n",
    "        print(f\"   ‚úÖ Step 2: User activity (‚â•{self.min_user_playlists} playlists per user)\")\n",
    "        print(f\"   ‚úÖ Step 3: Track frequency (‚â•{self.min_track_frequency} appearances)\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1a: Collect user activity statistics\n",
    "        print(\"üìä Sub-pass 1a: Collecting user activity statistics...\")\n",
    "        user_playlist_count = Counter()\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"   User stats progress: {batch_idx + 1}/{len(batches)} batches\")\n",
    "\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "                    for playlist in file_playlists:\n",
    "                        user_id = self._extract_user_id(playlist)\n",
    "                        user_playlist_count[user_id] += 1\n",
    "\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        # Identify active users\n",
    "        active_users = {\n",
    "            user for user, count in user_playlist_count.items()\n",
    "            if count >= self.min_user_playlists\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Identified {len(active_users):,} active users (target: ~{self.expected_users})\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1b: Apply all core filters\n",
    "        print(\"üîç Sub-pass 1b: Applying all core filters...\")\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            print(f\"üì¶ Processing batch {batch_idx + 1}/{len(batches)}\")\n",
    "\n",
    "            batch_playlists = []\n",
    "\n",
    "            # Load batch\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "                    for playlist in file_playlists:\n",
    "                        playlist['_source_file'] = file_path\n",
    "\n",
    "                    batch_playlists.extend(file_playlists)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error loading {os.path.basename(file_path)}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Process batch with core filtering\n",
    "            for playlist in batch_playlists:\n",
    "                stage_counts['total_seen'] += 1\n",
    "\n",
    "                # CORE FILTER 1: Playlist length\n",
    "                tracks = playlist.get('tracks', [])\n",
    "                playlist_length = len(tracks)\n",
    "\n",
    "                if not (self.min_playlist_length <= playlist_length <= self.max_playlist_length):\n",
    "                    continue\n",
    "                stage_counts['passed_length_filter'] += 1\n",
    "\n",
    "                # CORE FILTER 2: User activity\n",
    "                user_id = self._extract_user_id(playlist)\n",
    "                if user_id not in active_users:\n",
    "                    continue\n",
    "                stage_counts['passed_user_filter'] += 1\n",
    "\n",
    "                # Count tracks for frequency analysis\n",
    "                playlist_tracks = set()\n",
    "                for track in tracks:\n",
    "                    track_uri = track.get('track_uri', '')\n",
    "                    if track_uri:\n",
    "                        self.track_counts[track_uri] += 1\n",
    "                        playlist_tracks.add(track_uri)\n",
    "\n",
    "                self.user_counts[user_id] += 1\n",
    "\n",
    "                # Store playlist metadata\n",
    "                playlist_metadata = {\n",
    "                    'file_path': playlist['_source_file'],\n",
    "                    'pid': playlist.get('pid'),\n",
    "                    'length': playlist_length,\n",
    "                    'modified_at': playlist.get('modified_at', 0),\n",
    "                    'user_id': user_id,\n",
    "                    'track_uris': list(playlist_tracks),\n",
    "                    'name': playlist.get('name', ''),\n",
    "                    'collaborative': playlist.get('collaborative', False),\n",
    "                    'num_followers': playlist.get('num_followers', 0)\n",
    "                }\n",
    "\n",
    "                self.playlist_stats.append(playlist_metadata)\n",
    "\n",
    "            # Clear batch from memory\n",
    "            del batch_playlists\n",
    "            gc.collect()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"   Progress: {stage_counts['total_seen']:,} seen, {len(self.playlist_stats):,} valid so far\")\n",
    "                self.print_memory_status(f\"batch {batch_idx + 1}\")\n",
    "\n",
    "        # CORE FILTER 3: Track frequency\n",
    "        print(f\"\\nüîç Applying final core filter: Track frequency (‚â•{self.min_track_frequency})\")\n",
    "\n",
    "        core_tracks = {\n",
    "            track for track, count in self.track_counts.items()\n",
    "            if count >= self.min_track_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Identified {len(core_tracks):,} core tracks (target: ~{self.expected_tracks})\")\n",
    "\n",
    "        # Filter playlists that have core tracks\n",
    "        filtered_playlist_stats = []\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            playlist_tracks = set(playlist_meta['track_uris'])\n",
    "            if playlist_tracks.intersection(core_tracks):\n",
    "                filtered_playlist_stats.append(playlist_meta)\n",
    "                stage_counts['passed_track_frequency_filter'] += 1\n",
    "\n",
    "        self.playlist_stats = filtered_playlist_stats\n",
    "        stage_counts['final_valid'] = len(filtered_playlist_stats)\n",
    "\n",
    "        print(f\"\\n‚úÖ CORE-BASED FILTERING COMPLETE:\")\n",
    "        print(f\"   üìä Filtering Funnel:\")\n",
    "        print(f\"      ‚Ä¢ Total playlists: {stage_counts['total_seen']:,}\")\n",
    "        print(f\"      ‚Ä¢ Length filter: {stage_counts['passed_length_filter']:,} ({stage_counts['passed_length_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ User filter: {stage_counts['passed_user_filter']:,} ({stage_counts['passed_user_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ Track frequency: {stage_counts['passed_track_frequency_filter']:,} ({stage_counts['passed_track_frequency_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ üéØ FINAL VALID: {stage_counts['final_valid']:,} ({stage_counts['final_valid']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "        return {\n",
    "            'total_playlists': stage_counts['total_seen'],\n",
    "            'valid_playlists': stage_counts['final_valid'],\n",
    "            'core_tracks': core_tracks,\n",
    "            'active_users': active_users,\n",
    "            'unique_tracks': len(self.track_counts),\n",
    "            'stage_counts': stage_counts\n",
    "        }\n",
    "\n",
    "    def create_strata(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"Create comprehensive strata for stratified sampling\"\"\"\n",
    "        print(\"üìä CREATING STRATIFIED SAMPLING STRATA\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Get temporal split\n",
    "        timestamps = [p['modified_at'] for p in self.playlist_stats if p['modified_at'] > 0]\n",
    "        median_time = np.median(timestamps) if timestamps else 1500000000\n",
    "\n",
    "        # Get user activity split\n",
    "        user_playlist_counts = {}\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_playlist_counts[user_id] = user_playlist_counts.get(user_id, 0) + 1\n",
    "\n",
    "        user_activity_median = np.median(list(user_playlist_counts.values())) if user_playlist_counts else 5\n",
    "\n",
    "        print(f\"   üìÖ Temporal split at timestamp: {median_time}\")\n",
    "        print(f\"   üë• User activity split at: {user_activity_median} playlists per user\")\n",
    "\n",
    "        # Create 12 comprehensive strata\n",
    "        strata = {\n",
    "            'short_old_casual': [], 'short_old_active': [],\n",
    "            'short_recent_casual': [], 'short_recent_active': [],\n",
    "            'medium_old_casual': [], 'medium_old_active': [],\n",
    "            'medium_recent_casual': [], 'medium_recent_active': [],\n",
    "            'long_old_casual': [], 'long_old_active': [],\n",
    "            'long_recent_casual': [], 'long_recent_active': []\n",
    "        }\n",
    "\n",
    "        for i, playlist_meta in enumerate(self.playlist_stats):\n",
    "            length = playlist_meta['length']\n",
    "            timestamp = playlist_meta['modified_at']\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_activity = user_playlist_counts.get(user_id, 1)\n",
    "\n",
    "            # Categorize\n",
    "            length_cat = 'short' if length <= 30 else 'medium' if length <= 60 else 'long'\n",
    "            time_cat = 'recent' if timestamp >= median_time else 'old'\n",
    "            activity_cat = 'active' if user_activity >= user_activity_median else 'casual'\n",
    "\n",
    "            stratum_key = f\"{length_cat}_{time_cat}_{activity_cat}\"\n",
    "            strata[stratum_key].append(i)\n",
    "\n",
    "        # Print strata distribution\n",
    "        print(\"   üìã Strata Distribution:\")\n",
    "        total_playlists = len(self.playlist_stats)\n",
    "\n",
    "        for stratum, indices in strata.items():\n",
    "            if indices:\n",
    "                percentage = len(indices) / total_playlists * 100\n",
    "                print(f\"      ‚Ä¢ {stratum:20s}: {len(indices):6,} ({percentage:4.1f}%)\")\n",
    "\n",
    "        print()\n",
    "        return strata\n",
    "\n",
    "    def _calculate_priority_score(self, playlist_meta: Dict) -> float:\n",
    "        \"\"\"Calculate priority score for playlist selection\"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Factor 1: Track diversity (30% weight)\n",
    "        unique_tracks = len(playlist_meta['track_uris'])\n",
    "        playlist_length = playlist_meta['length']\n",
    "        if playlist_length > 0:\n",
    "            track_diversity_ratio = unique_tracks / playlist_length\n",
    "            score += track_diversity_ratio * 3.0\n",
    "\n",
    "        # Factor 2: User engagement (25% weight)\n",
    "        num_followers = playlist_meta.get('num_followers', 0)\n",
    "        if num_followers > 0:\n",
    "            follower_score = min(np.log10(num_followers + 1), 3.0)\n",
    "            score += follower_score * 2.5\n",
    "\n",
    "        # Factor 3: Playlist completeness (20% weight)\n",
    "        name = playlist_meta.get('name', '')\n",
    "        has_good_name = len(name.strip()) > 3 and not name.lower().startswith('my playlist')\n",
    "        if has_good_name:\n",
    "            score += 2.0\n",
    "\n",
    "        # Factor 4: Collaborative playlists bonus (10% weight)\n",
    "        if playlist_meta.get('collaborative', False):\n",
    "            score += 1.0\n",
    "\n",
    "        # Factor 5: Length balance bonus (15% weight)\n",
    "        length = playlist_meta['length']\n",
    "        if 20 <= length <= 80:\n",
    "            score += 1.5\n",
    "\n",
    "        return score\n",
    "\n",
    "    def pass2_stratified_sampling(self, strata: Dict[str, List[int]]) -> List[Dict]:\n",
    "        \"\"\"PASS 2: Stratified sampling with priority scoring\"\"\"\n",
    "        print(\"üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        total_available = len(self.playlist_stats)\n",
    "\n",
    "        if total_available <= self.target_playlists:\n",
    "            print(f\"   üìù Available ({total_available:,}) ‚â§ target ({self.target_playlists:,})\")\n",
    "            selected_indices = list(range(total_available))\n",
    "        else:\n",
    "            sampling_ratio = self.target_playlists / total_available\n",
    "            selected_indices = set()\n",
    "\n",
    "            print(f\"   üìä Global sampling ratio: {sampling_ratio:.3f}\")\n",
    "            print(f\"   üèÜ Using priority scoring within strata\")\n",
    "            print()\n",
    "\n",
    "            # Stratified sampling with priority scoring\n",
    "            for stratum, indices in strata.items():\n",
    "                if not indices:\n",
    "                    continue\n",
    "\n",
    "                stratum_target = max(1, int(len(indices) * sampling_ratio))\n",
    "                stratum_target = min(stratum_target, len(indices))\n",
    "\n",
    "                # Score playlists in this stratum\n",
    "                scored_playlists = []\n",
    "                for idx in indices:\n",
    "                    playlist_meta = self.playlist_stats[idx]\n",
    "                    score = self._calculate_priority_score(playlist_meta)\n",
    "                    scored_playlists.append((idx, score))\n",
    "\n",
    "                # Sort by score and sample\n",
    "                scored_playlists.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Hybrid: 70% top-scored + 30% random\n",
    "                top_count = int(stratum_target * 0.7)\n",
    "                random_count = stratum_target - top_count\n",
    "\n",
    "                selected = [idx for idx, _ in scored_playlists[:top_count]]\n",
    "\n",
    "                if random_count > 0 and len(scored_playlists) > top_count:\n",
    "                    remaining = [idx for idx, _ in scored_playlists[top_count:]]\n",
    "                    if len(remaining) >= random_count:\n",
    "                        selected.extend(random.sample(remaining, random_count))\n",
    "                    else:\n",
    "                        selected.extend(remaining)\n",
    "\n",
    "                selected_indices.update(selected)\n",
    "\n",
    "                avg_score = np.mean([score for _, score in scored_playlists[:len(selected)]])\n",
    "                print(f\"      ‚Ä¢ {stratum:20s}: {len(selected):4,} / {len(indices):5,} (avg score: {avg_score:.2f})\")\n",
    "\n",
    "            selected_indices = list(selected_indices)\n",
    "\n",
    "        print(f\"\\n   üéØ Selected {len(selected_indices):,} playlists for final loading\")\n",
    "        return self._load_selected_playlists(selected_indices)\n",
    "\n",
    "    def _load_selected_playlists(self, selected_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"Load only the selected playlists from files\"\"\"\n",
    "        print(\"   üìÅ Loading selected playlists...\")\n",
    "\n",
    "        # Group by file\n",
    "        file_to_playlists = defaultdict(list)\n",
    "        for idx in selected_indices:\n",
    "            playlist_meta = self.playlist_stats[idx]\n",
    "            file_path = playlist_meta['file_path']\n",
    "            file_to_playlists[file_path].append(playlist_meta)\n",
    "\n",
    "        print(f\"   üìÇ Loading from {len(file_to_playlists)} files\")\n",
    "\n",
    "        final_playlists = []\n",
    "\n",
    "        for file_idx, (file_path, playlist_metas) in enumerate(file_to_playlists.items()):\n",
    "            if file_idx % 100 == 0:\n",
    "                print(f\"      üìñ File {file_idx + 1}/{len(file_to_playlists)}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                file_playlists = data.get('playlists', [])\n",
    "                pid_to_playlist = {p.get('pid'): p for p in file_playlists}\n",
    "\n",
    "                for meta in playlist_metas:\n",
    "                    pid = meta['pid']\n",
    "                    if pid in pid_to_playlist:\n",
    "                        playlist = pid_to_playlist[pid]\n",
    "                        playlist['_sampling_score'] = self._calculate_priority_score(meta)\n",
    "                        final_playlists.append(playlist)\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        print(f\"   ‚úÖ Loaded {len(final_playlists):,} final playlists\")\n",
    "        return final_playlists\n",
    "\n",
    "    def _verify_final_scale(self, final_playlists: List[Dict]) -> Dict:\n",
    "        \"\"\"Verify that final scale meets experimental targets\"\"\"\n",
    "        print(\"\\nüîç VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Count actual entities\n",
    "        actual_tracks = set()\n",
    "        actual_artists = set()\n",
    "        actual_albums = set()\n",
    "        actual_users = set()\n",
    "\n",
    "        for playlist in final_playlists:\n",
    "            user_id = self._extract_user_id(playlist)\n",
    "            actual_users.add(user_id)\n",
    "\n",
    "            for track in playlist.get('tracks', []):\n",
    "                track_uri = track.get('track_uri', '')\n",
    "                artist_uri = track.get('artist_uri', '')\n",
    "                album_uri = track.get('album_uri', '')\n",
    "\n",
    "                if track_uri:\n",
    "                    actual_tracks.add(track_uri)\n",
    "                if artist_uri:\n",
    "                    actual_artists.add(artist_uri)\n",
    "                if album_uri:\n",
    "                    actual_albums.add(album_uri)\n",
    "\n",
    "        actual_scale = {\n",
    "            'playlists': len(final_playlists),\n",
    "            'tracks': len(actual_tracks),\n",
    "            'artists': len(actual_artists),\n",
    "            'albums': len(actual_albums),\n",
    "            'users': len(actual_users),\n",
    "            'total_nodes': len(final_playlists) + len(actual_tracks) + len(actual_artists) + len(actual_albums) + len(actual_users)\n",
    "        }\n",
    "\n",
    "        print(f\"   üìä Final Entity Counts:\")\n",
    "        print(f\"      ‚Ä¢ Playlists: {actual_scale['playlists']:,} (target: {self.target_playlists:,})\")\n",
    "        print(f\"      ‚Ä¢ Tracks: {actual_scale['tracks']:,} (target: ~{self.expected_tracks:,})\")\n",
    "        print(f\"      ‚Ä¢ Artists: {actual_scale['artists']:,} (target: ~{self.expected_artists:,})\")\n",
    "        print(f\"      ‚Ä¢ Albums: {actual_scale['albums']:,} (target: ~{self.expected_albums:,})\")\n",
    "        print(f\"      ‚Ä¢ Users: {actual_scale['users']:,} (target: ~{self.expected_users:,})\")\n",
    "        print(f\"      üéØ TOTAL: {actual_scale['total_nodes']:,} (target: ~{self.expected_total_nodes:,})\")\n",
    "\n",
    "        return actual_scale\n",
    "\n",
    "    def run_hybrid_sampling(self, file_pattern: str) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"Main method: Complete hybrid sampling workflow\"\"\"\n",
    "        print(\"üöÄ STARTING MULTI-SCALE HYBRID SAMPLING\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        self.print_memory_status(\"start\")\n",
    "\n",
    "        # Pass 1: Core-based filtering\n",
    "        stats = self.pass1_core_filtering(file_pattern)\n",
    "        self.print_memory_status(\"pass 1 complete\")\n",
    "\n",
    "        # Create strata\n",
    "        strata = self.create_strata()\n",
    "        self.print_memory_status(\"strata created\")\n",
    "\n",
    "        # Pass 2: Stratified sampling\n",
    "        final_playlists = self.pass2_stratified_sampling(strata)\n",
    "        self.print_memory_status(\"pass 2 complete\")\n",
    "\n",
    "        # Scale verification\n",
    "        actual_scale = self._verify_final_scale(final_playlists)\n",
    "\n",
    "        # Final statistics\n",
    "        final_stats = {\n",
    "            'methodology': 'multi_scale_hybrid_core_based_stratified_streaming',\n",
    "            'scale': self.scale_name,\n",
    "            'original_total': stats['total_playlists'],\n",
    "            'final_sampled': len(final_playlists),\n",
    "            'retention_rate': len(final_playlists) / stats['total_playlists'],\n",
    "            'core_filtering_retention': stats['valid_playlists'] / stats['total_playlists'],\n",
    "            'unique_tracks': stats['unique_tracks'],\n",
    "            'core_tracks_count': len(stats['core_tracks']),\n",
    "            'active_users_count': len(stats['active_users']),\n",
    "            'stage_counts': stats['stage_counts'],\n",
    "            'actual_scale': actual_scale,\n",
    "            'scale_targets': {\n",
    "                'total_nodes': self.expected_total_nodes,\n",
    "                'playlists': self.target_playlists,\n",
    "                'tracks': self.expected_tracks,\n",
    "                'artists': self.expected_artists,\n",
    "                'albums': self.expected_albums,\n",
    "                'users': self.expected_users\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"\\nüéâ MULTI-SCALE HYBRID SAMPLING COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìä Results: {stats['total_playlists']:,} ‚Üí {len(final_playlists):,} playlists\")\n",
    "        print(f\"üìà Overall retention: {len(final_playlists) / stats['total_playlists']:.1%}\")\n",
    "\n",
    "        # Scale verification summary\n",
    "        total_actual = actual_scale['total_nodes']\n",
    "        scale_ratio = total_actual / self.expected_total_nodes\n",
    "        print(f\"\\nüéØ EXPERIMENTAL SCALE VERIFICATION:\")\n",
    "        print(f\"   ‚Ä¢ Actual total nodes: {total_actual:,}\")\n",
    "        print(f\"   ‚Ä¢ Target total nodes: {self.expected_total_nodes:,}\")\n",
    "        print(f\"   ‚Ä¢ Scale ratio: {scale_ratio:.3f} ({'‚úÖ GOOD' if 0.8 <= scale_ratio <= 1.2 else '‚ö†Ô∏è ADJUST'})\")\n",
    "\n",
    "        return final_playlists, final_stats\n",
    "\n",
    "# =============================================================================\n",
    "# MULTI-SCALE CREATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def run_multi_scale_sampling(file_pattern: str, scales: Dict[str, int] = None):\n",
    "    \"\"\"Create multiple scale datasets in one execution\"\"\"\n",
    "\n",
    "    if scales is None:\n",
    "        scales = {\n",
    "            'tiny': 300,\n",
    "            'small': 600,\n",
    "            'medium': 1000,\n",
    "            'large': 1500\n",
    "        }\n",
    "\n",
    "    print(\"üéØ MULTI-SCALE DATASET CREATION\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Creating multiple scale datasets for flexible experimentation\")\n",
    "    print()\n",
    "\n",
    "    # Show scale overview\n",
    "    print(\"üìä SCALES TO CREATE:\")\n",
    "    print(\"Scale    Playlists  Expected Nodes  Expected Time\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    time_estimates = {\n",
    "        'tiny': '15-30 min',\n",
    "        'small': '45-75 min',\n",
    "        'medium': '2-3 hours',\n",
    "        'large': '4-6 hours'\n",
    "    }\n",
    "\n",
    "    for scale_name, target_playlists in scales.items():\n",
    "        scale_params = get_scale_parameters(scale_name, target_playlists)\n",
    "        expected_nodes = scale_params['expected_total_nodes']\n",
    "        time_est = time_estimates.get(scale_name, 'Unknown')\n",
    "        print(f\"{scale_name:<8} {target_playlists:<10} {expected_nodes:<15,} {time_est}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for scale_name, target_playlists in scales.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üîß CREATING {scale_name.upper()} SCALE DATASET\")\n",
    "        print(f\"Target: {target_playlists:,} playlists\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        try:\n",
    "            # Create scale-specific sampler\n",
    "            sampler = MultiScaleHybridStreamingSampler(\n",
    "                scale_name=scale_name,\n",
    "                target_playlists=target_playlists,\n",
    "                batch_size=20,\n",
    "                min_playlist_length=10,\n",
    "                max_playlist_length=100\n",
    "            )\n",
    "\n",
    "            # Run sampling\n",
    "            sampled_playlists, stats = sampler.run_hybrid_sampling(file_pattern)\n",
    "\n",
    "            # Create output data\n",
    "            output_data = {\n",
    "                'info': {\n",
    "                    'generated_on': datetime.now().isoformat(),\n",
    "                    'sampling_method': 'multi_scale_hybrid_core_based_stratified_streaming',\n",
    "                    'scale': scale_name,\n",
    "                    'target_playlists': target_playlists,\n",
    "                    'parameters': get_scale_parameters(scale_name, target_playlists)\n",
    "                },\n",
    "                'sampling_stats': stats,\n",
    "                'playlists': sampled_playlists\n",
    "            }\n",
    "\n",
    "            # Save scale-specific file\n",
    "            os.makedirs('../data/processed', exist_ok=True)\n",
    "            output_file = f'../data/processed/spotify_scaled_hybrid_{scale_name}.json'\n",
    "\n",
    "            with open(output_file, 'w') as f:\n",
    "                json.dump(output_data, f, indent=2)\n",
    "\n",
    "            file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "\n",
    "            print(f\"\\n‚úÖ {scale_name.upper()} SCALE COMPLETED:\")\n",
    "            print(f\"   üìÅ File: {output_file}\")\n",
    "            print(f\"   üì¶ Size: {file_size_mb:.1f} MB\")\n",
    "            print(f\"   üìä Playlists: {len(sampled_playlists):,}\")\n",
    "            print(f\"   üìä Total nodes: {stats['actual_scale']['total_nodes']:,}\")\n",
    "\n",
    "            # Store results\n",
    "            results[scale_name] = {\n",
    "                'file_path': output_file,\n",
    "                'playlists': len(sampled_playlists),\n",
    "                'total_nodes': stats['actual_scale']['total_nodes'],\n",
    "                'file_size_mb': file_size_mb,\n",
    "                'stats': stats\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to create {scale_name} scale: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"üéâ MULTI-SCALE DATASET CREATION COMPLETED!\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    if results:\n",
    "        print(\"üìä CREATED DATASETS:\")\n",
    "        print(\"Scale    File                                 Playlists  Nodes     Size\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        for scale_name, result in results.items():\n",
    "            filename = os.path.basename(result['file_path'])\n",
    "            print(f\"{scale_name:<8} {filename:<35} {result['playlists']:<10,} {result['total_nodes']:<9,} {result['file_size_mb']:.1f}MB\")\n",
    "\n",
    "    return results\n",
    "\n",
    "def run_single_scale_sampling(file_pattern: str,\n",
    "                             scale_name: str = 'small',\n",
    "                             target_playlists: int = None):\n",
    "    \"\"\"Create a single scale dataset\"\"\"\n",
    "\n",
    "    # Default targets for each scale\n",
    "    default_targets = {\n",
    "        'tiny': 300,\n",
    "        'small': 600,\n",
    "        'medium': 1000,\n",
    "        'large': 1500\n",
    "    }\n",
    "\n",
    "    if target_playlists is None:\n",
    "        target_playlists = default_targets.get(scale_name, 600)\n",
    "\n",
    "    print(f\"üéØ CREATING SINGLE {scale_name.upper()} SCALE DATASET\")\n",
    "    print(f\"Target: {target_playlists:,} playlists\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Set seeds for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Create sampler\n",
    "    sampler = MultiScaleHybridStreamingSampler(\n",
    "        scale_name=scale_name,\n",
    "        target_playlists=target_playlists,\n",
    "        batch_size=20,\n",
    "        min_playlist_length=10,\n",
    "        max_playlist_length=100\n",
    "    )\n",
    "\n",
    "    # Run sampling\n",
    "    sampled_playlists, stats = sampler.run_hybrid_sampling(file_pattern)\n",
    "\n",
    "    # Create output data\n",
    "    output_data = {\n",
    "        'info': {\n",
    "            'generated_on': datetime.now().isoformat(),\n",
    "            'sampling_method': 'multi_scale_hybrid_core_based_stratified_streaming',\n",
    "            'scale': scale_name,\n",
    "            'target_playlists': target_playlists,\n",
    "            'parameters': get_scale_parameters(scale_name, target_playlists)\n",
    "        },\n",
    "        'sampling_stats': stats,\n",
    "        'playlists': sampled_playlists\n",
    "    }\n",
    "\n",
    "    # Save file\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    output_file = f'../data/processed/spotify_scaled_hybrid_{scale_name}.json'\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "\n",
    "    print(f\"\\n‚úÖ {scale_name.upper()} SCALE COMPLETED:\")\n",
    "    print(f\"   üìÅ File: {output_file}\")\n",
    "    print(f\"   üì¶ Size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"   üìä Playlists: {len(sampled_playlists):,}\")\n",
    "    print(f\"   üìä Total nodes: {stats['actual_scale']['total_nodes']:,}\")\n",
    "\n",
    "    expected_time = {\n",
    "        'tiny': '15-30 minutes',\n",
    "        'small': '45-75 minutes',\n",
    "        'medium': '2-3 hours',\n",
    "        'large': '4-6 hours'\n",
    "    }.get(scale_name, 'Unknown')\n",
    "\n",
    "    print(f\"   ‚è±Ô∏è Expected experiment time: {expected_time}\")\n",
    "\n",
    "    return sampled_playlists, stats, output_file\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def compare_scales_detailed():\n",
    "    \"\"\"Detailed comparison of all scales\"\"\"\n",
    "\n",
    "    scales_info = {\n",
    "        'tiny': {\n",
    "            'playlists': 300,\n",
    "            'nodes': '~5K',\n",
    "            'experiment_time': '15-30 min',\n",
    "            'min_track_freq': 3,\n",
    "            'min_user_playlists': 3\n",
    "        },\n",
    "        'small': {\n",
    "            'playlists': 600,\n",
    "            'nodes': '~10K',\n",
    "            'experiment_time': '45-75 min',\n",
    "            'min_track_freq': 5,\n",
    "            'min_user_playlists': 5\n",
    "        },\n",
    "        'medium': {\n",
    "            'playlists': 1000,\n",
    "            'nodes': '~20K',\n",
    "            'experiment_time': '2-3 hours',\n",
    "            'min_track_freq': 6,\n",
    "            'min_user_playlists': 7\n",
    "        },\n",
    "        'large': {\n",
    "            'playlists': 1500,\n",
    "            'nodes': '~30K',\n",
    "            'experiment_time': '4-6 hours',\n",
    "            'min_track_freq': 8,\n",
    "            'min_user_playlists': 10\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"üìä DETAILED SCALE COMPARISON\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Scale   Playlists  Nodes   Exp.Time   Track_Freq  User_Freq\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    for scale, info in scales_info.items():\n",
    "        print(f\"{scale:<7} {info['playlists']:<10} {info['nodes']:<7} {info['experiment_time']:<10} \"\n",
    "              f\"{info['min_track_freq']:<11} {info['min_user_playlists']:<10}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üéØ MULTI-SCALE SPOTIFY SAMPLER\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Show scale comparison\n",
    "    compare_scales_detailed()\n",
    "\n",
    "    print(f\"\\nüîß USAGE OPTIONS:\")\n",
    "    print(f\"1. Create all scales at once:\")\n",
    "    print(f\"   run_multi_scale_sampling(file_pattern)\")\n",
    "    print(f\"\")\n",
    "    print(f\"2. Create single scale:\")\n",
    "    print(f\"   run_single_scale_sampling(file_pattern, 'small')\")\n",
    "    print(f\"\")\n",
    "    print(f\"3. Create custom scales:\")\n",
    "    print(f\"   custom_scales = {{'quick': 200, 'normal': 800}}\")\n",
    "    print(f\"   run_multi_scale_sampling(file_pattern, custom_scales)\")\n",
    "\n",
    "    # Example usage\n",
    "    file_pattern = \"../data/raw/data/mpd.slice.*.json\"\n",
    "    run_multi_scale_sampling(file_pattern)"
   ],
   "id": "1a1cdc82241571b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MULTI-SCALE SPOTIFY SAMPLER\n",
      "============================================================\n",
      "üìä DETAILED SCALE COMPARISON\n",
      "====================================================================================================\n",
      "Scale   Playlists  Nodes   Exp.Time   Track_Freq  User_Freq\n",
      "----------------------------------------------------------------------------------------------------\n",
      "tiny    300        ~5K     15-30 min  3           3         \n",
      "small   600        ~10K    45-75 min  5           5         \n",
      "medium  1000       ~20K    2-3 hours  6           7         \n",
      "large   1500       ~30K    4-6 hours  8           10        \n",
      "\n",
      "üîß USAGE OPTIONS:\n",
      "1. Create all scales at once:\n",
      "   run_multi_scale_sampling(file_pattern)\n",
      "\n",
      "2. Create single scale:\n",
      "   run_single_scale_sampling(file_pattern, 'small')\n",
      "\n",
      "3. Create custom scales:\n",
      "   custom_scales = {'quick': 200, 'normal': 800}\n",
      "   run_multi_scale_sampling(file_pattern, custom_scales)\n",
      "üéØ MULTI-SCALE DATASET CREATION\n",
      "============================================================\n",
      "Creating multiple scale datasets for flexible experimentation\n",
      "\n",
      "üìä SCALES TO CREATE:\n",
      "Scale    Playlists  Expected Nodes  Expected Time\n",
      "--------------------------------------------------\n",
      "tiny     300        5,000           15-30 min\n",
      "small    600        10,000          45-75 min\n",
      "medium   1000       20,000          2-3 hours\n",
      "large    1500       30,000          4-6 hours\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üîß CREATING TINY SCALE DATASET\n",
      "Target: 300 playlists\n",
      "======================================================================\n",
      "üéØ MULTI-SCALE SAMPLER (TINY SCALE)\n",
      "======================================================================\n",
      "üìä SCALE-OPTIMIZED PARAMETERS:\n",
      "   ‚Ä¢ Target playlists: 300\n",
      "   ‚Ä¢ Min track frequency: 3\n",
      "   ‚Ä¢ Min user playlists: 3\n",
      "   ‚Ä¢ Expected total nodes: ~5,000\n",
      "\n",
      "üöÄ STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "üíæ Memory usage after start: 156.6 MB\n",
      "üîç PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "üìÅ Found 1000 files\n",
      "üì¶ Created 50 batches of ~20 files each\n",
      "üöÄ Applying Core-Based Filters:\n",
      "   ‚úÖ Step 1: Playlist length (10-100 tracks)\n",
      "   ‚úÖ Step 2: User activity (‚â•3 playlists per user)\n",
      "   ‚úÖ Step 3: Track frequency (‚â•3 appearances)\n",
      "\n",
      "üìä Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ‚úÖ Identified 820 active users (target: ~100)\n",
      "\n",
      "üîç Sub-pass 1b: Applying all core filters...\n",
      "üì¶ Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,846 valid so far\n",
      "üíæ Memory usage after batch 1: 1433.1 MB\n",
      "üì¶ Processing batch 2/50\n",
      "üì¶ Processing batch 3/50\n",
      "üì¶ Processing batch 4/50\n",
      "üì¶ Processing batch 5/50\n",
      "üì¶ Processing batch 6/50\n",
      "üì¶ Processing batch 7/50\n",
      "üì¶ Processing batch 8/50\n",
      "üì¶ Processing batch 9/50\n",
      "üì¶ Processing batch 10/50\n",
      "üì¶ Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,317 valid so far\n",
      "üíæ Memory usage after batch 11: 2282.7 MB\n",
      "üì¶ Processing batch 12/50\n",
      "üì¶ Processing batch 13/50\n",
      "üì¶ Processing batch 14/50\n",
      "üì¶ Processing batch 15/50\n",
      "üì¶ Processing batch 16/50\n",
      "üì¶ Processing batch 17/50\n",
      "üì¶ Processing batch 18/50\n",
      "üì¶ Processing batch 19/50\n",
      "üì¶ Processing batch 20/50\n",
      "üì¶ Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,973 valid so far\n",
      "üíæ Memory usage after batch 21: 2803.0 MB\n",
      "üì¶ Processing batch 22/50\n",
      "üì¶ Processing batch 23/50\n",
      "üì¶ Processing batch 24/50\n",
      "üì¶ Processing batch 25/50\n",
      "üì¶ Processing batch 26/50\n",
      "üì¶ Processing batch 27/50\n",
      "üì¶ Processing batch 28/50\n",
      "üì¶ Processing batch 29/50\n",
      "üì¶ Processing batch 30/50\n",
      "üì¶ Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,887 valid so far\n",
      "üíæ Memory usage after batch 31: 3616.5 MB\n",
      "üì¶ Processing batch 32/50\n",
      "üì¶ Processing batch 33/50\n",
      "üì¶ Processing batch 34/50\n",
      "üì¶ Processing batch 35/50\n",
      "üì¶ Processing batch 36/50\n",
      "üì¶ Processing batch 37/50\n",
      "üì¶ Processing batch 38/50\n",
      "üì¶ Processing batch 39/50\n",
      "üì¶ Processing batch 40/50\n",
      "üì¶ Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,910 valid so far\n",
      "üíæ Memory usage after batch 41: 4364.2 MB\n",
      "üì¶ Processing batch 42/50\n",
      "üì¶ Processing batch 43/50\n",
      "üì¶ Processing batch 44/50\n",
      "üì¶ Processing batch 45/50\n",
      "üì¶ Processing batch 46/50\n",
      "üì¶ Processing batch 47/50\n",
      "üì¶ Processing batch 48/50\n",
      "üì¶ Processing batch 49/50\n",
      "üì¶ Processing batch 50/50\n",
      "\n",
      "üîç Applying final core filter: Track frequency (‚â•3)\n",
      "   ‚úÖ Identified 552,365 core tracks (target: ~2000)\n",
      "\n",
      "‚úÖ CORE-BASED FILTERING COMPLETE:\n",
      "   üìä Filtering Funnel:\n",
      "      ‚Ä¢ Total playlists: 1,000,000\n",
      "      ‚Ä¢ Length filter: 747,510 (74.8%)\n",
      "      ‚Ä¢ User filter: 747,475 (74.7%)\n",
      "      ‚Ä¢ Track frequency: 747,177 (74.7%)\n",
      "      ‚Ä¢ üéØ FINAL VALID: 747,177 (74.7%)\n",
      "\n",
      "üíæ Memory usage after pass 1 complete: 3865.2 MB\n",
      "üìä CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   üìÖ Temporal split at timestamp: 1487894400.0\n",
      "   üë• User activity split at: 73.0 playlists per user\n",
      "   üìã Strata Distribution:\n",
      "      ‚Ä¢ short_old_casual    :  2,250 ( 0.3%)\n",
      "      ‚Ä¢ short_old_active    : 152,867 (20.5%)\n",
      "      ‚Ä¢ short_recent_casual :  1,591 ( 0.2%)\n",
      "      ‚Ä¢ short_recent_active : 115,998 (15.5%)\n",
      "      ‚Ä¢ medium_old_casual   :  1,619 ( 0.2%)\n",
      "      ‚Ä¢ medium_old_active   : 133,714 (17.9%)\n",
      "      ‚Ä¢ medium_recent_casual:  1,724 ( 0.2%)\n",
      "      ‚Ä¢ medium_recent_active: 142,281 (19.0%)\n",
      "      ‚Ä¢ long_old_casual     :    968 ( 0.1%)\n",
      "      ‚Ä¢ long_old_active     : 81,510 (10.9%)\n",
      "      ‚Ä¢ long_recent_casual  :  1,233 ( 0.2%)\n",
      "      ‚Ä¢ long_recent_active  : 111,422 (14.9%)\n",
      "\n",
      "üíæ Memory usage after strata created: 3289.1 MB\n",
      "üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   üìä Global sampling ratio: 0.000\n",
      "   üèÜ Using priority scoring within strata\n",
      "\n",
      "      ‚Ä¢ short_old_casual    :    1 / 2,250 (avg score: 11.83)\n",
      "      ‚Ä¢ short_old_active    :   61 / 152,867 (avg score: 13.98)\n",
      "      ‚Ä¢ short_recent_casual :    1 / 1,591 (avg score: 13.93)\n",
      "      ‚Ä¢ short_recent_active :   46 / 115,998 (avg score: 13.68)\n",
      "      ‚Ä¢ medium_old_casual   :    1 / 1,619 (avg score: 11.52)\n",
      "      ‚Ä¢ medium_old_active   :   53 / 133,714 (avg score: 14.45)\n",
      "      ‚Ä¢ medium_recent_casual:    1 / 1,724 (avg score: 11.42)\n",
      "      ‚Ä¢ medium_recent_active:   57 / 142,281 (avg score: 14.26)\n",
      "      ‚Ä¢ long_old_casual     :    1 /   968 (avg score: 13.00)\n",
      "      ‚Ä¢ long_old_active     :   32 / 81,510 (avg score: 14.04)\n",
      "      ‚Ä¢ long_recent_casual  :    1 / 1,233 (avg score: 12.65)\n",
      "      ‚Ä¢ long_recent_active  :   44 / 111,422 (avg score: 13.83)\n",
      "\n",
      "   üéØ Selected 299 playlists for final loading\n",
      "   üìÅ Loading selected playlists...\n",
      "   üìÇ Loading from 260 files\n",
      "      üìñ File 1/260\n",
      "      üìñ File 101/260\n",
      "      üìñ File 201/260\n",
      "   ‚úÖ Loaded 299 final playlists\n",
      "üíæ Memory usage after pass 2 complete: 1255.2 MB\n",
      "\n",
      "üîç VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   üìä Final Entity Counts:\n",
      "      ‚Ä¢ Playlists: 299 (target: 300)\n",
      "      ‚Ä¢ Tracks: 10,631 (target: ~2,000)\n",
      "      ‚Ä¢ Artists: 4,164 (target: ~500)\n",
      "      ‚Ä¢ Albums: 6,966 (target: ~400)\n",
      "      ‚Ä¢ Users: 120 (target: ~100)\n",
      "      üéØ TOTAL: 22,180 (target: ~5,000)\n",
      "\n",
      "üéâ MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "üìä Results: 1,000,000 ‚Üí 299 playlists\n",
      "üìà Overall retention: 0.0%\n",
      "\n",
      "üéØ EXPERIMENTAL SCALE VERIFICATION:\n",
      "   ‚Ä¢ Actual total nodes: 22,180\n",
      "   ‚Ä¢ Target total nodes: 5,000\n",
      "   ‚Ä¢ Scale ratio: 4.436 (‚ö†Ô∏è ADJUST)\n",
      "\n",
      "‚úÖ TINY SCALE COMPLETED:\n",
      "   üìÅ File: ../data/processed/spotify_scaled_hybrid_tiny.json\n",
      "   üì¶ Size: 5.1 MB\n",
      "   üìä Playlists: 299\n",
      "   üìä Total nodes: 22,180\n",
      "\n",
      "======================================================================\n",
      "üîß CREATING SMALL SCALE DATASET\n",
      "Target: 600 playlists\n",
      "======================================================================\n",
      "üéØ MULTI-SCALE SAMPLER (SMALL SCALE)\n",
      "======================================================================\n",
      "üìä SCALE-OPTIMIZED PARAMETERS:\n",
      "   ‚Ä¢ Target playlists: 600\n",
      "   ‚Ä¢ Min track frequency: 5\n",
      "   ‚Ä¢ Min user playlists: 5\n",
      "   ‚Ä¢ Expected total nodes: ~10,000\n",
      "\n",
      "üöÄ STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "üíæ Memory usage after start: 1023.1 MB\n",
      "üîç PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "üìÅ Found 1000 files\n",
      "üì¶ Created 50 batches of ~20 files each\n",
      "üöÄ Applying Core-Based Filters:\n",
      "   ‚úÖ Step 1: Playlist length (10-100 tracks)\n",
      "   ‚úÖ Step 2: User activity (‚â•5 playlists per user)\n",
      "   ‚úÖ Step 3: Track frequency (‚â•5 appearances)\n",
      "\n",
      "üìä Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ‚úÖ Identified 901 active users (target: ~200)\n",
      "\n",
      "üîç Sub-pass 1b: Applying all core filters...\n",
      "üì¶ Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,845 valid so far\n",
      "üíæ Memory usage after batch 1: 1561.9 MB\n",
      "üì¶ Processing batch 2/50\n",
      "üì¶ Processing batch 3/50\n",
      "üì¶ Processing batch 4/50\n",
      "üì¶ Processing batch 5/50\n",
      "üì¶ Processing batch 6/50\n",
      "üì¶ Processing batch 7/50\n",
      "üì¶ Processing batch 8/50\n",
      "üì¶ Processing batch 9/50\n",
      "üì¶ Processing batch 10/50\n",
      "üì¶ Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,309 valid so far\n",
      "üíæ Memory usage after batch 11: 2298.8 MB\n",
      "üì¶ Processing batch 12/50\n",
      "üì¶ Processing batch 13/50\n",
      "üì¶ Processing batch 14/50\n",
      "üì¶ Processing batch 15/50\n",
      "üì¶ Processing batch 16/50\n",
      "üì¶ Processing batch 17/50\n",
      "üì¶ Processing batch 18/50\n",
      "üì¶ Processing batch 19/50\n",
      "üì¶ Processing batch 20/50\n",
      "üì¶ Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,951 valid so far\n",
      "üíæ Memory usage after batch 21: 2873.7 MB\n",
      "üì¶ Processing batch 22/50\n",
      "üì¶ Processing batch 23/50\n",
      "üì¶ Processing batch 24/50\n",
      "üì¶ Processing batch 25/50\n",
      "üì¶ Processing batch 26/50\n",
      "üì¶ Processing batch 27/50\n",
      "üì¶ Processing batch 28/50\n",
      "üì¶ Processing batch 29/50\n",
      "üì¶ Processing batch 30/50\n",
      "üì¶ Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,853 valid so far\n",
      "üíæ Memory usage after batch 31: 3762.2 MB\n",
      "üì¶ Processing batch 32/50\n",
      "üì¶ Processing batch 33/50\n",
      "üì¶ Processing batch 34/50\n",
      "üì¶ Processing batch 35/50\n",
      "üì¶ Processing batch 36/50\n",
      "üì¶ Processing batch 37/50\n",
      "üì¶ Processing batch 38/50\n",
      "üì¶ Processing batch 39/50\n",
      "üì¶ Processing batch 40/50\n",
      "üì¶ Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,867 valid so far\n",
      "üíæ Memory usage after batch 41: 4553.5 MB\n",
      "üì¶ Processing batch 42/50\n",
      "üì¶ Processing batch 43/50\n",
      "üì¶ Processing batch 44/50\n",
      "üì¶ Processing batch 45/50\n",
      "üì¶ Processing batch 46/50\n",
      "üì¶ Processing batch 47/50\n",
      "üì¶ Processing batch 48/50\n",
      "üì¶ Processing batch 49/50\n",
      "üì¶ Processing batch 50/50\n",
      "\n",
      "üîç Applying final core filter: Track frequency (‚â•5)\n",
      "   ‚úÖ Identified 375,364 core tracks (target: ~4000)\n",
      "\n",
      "‚úÖ CORE-BASED FILTERING COMPLETE:\n",
      "   üìä Filtering Funnel:\n",
      "      ‚Ä¢ Total playlists: 1,000,000\n",
      "      ‚Ä¢ Length filter: 747,510 (74.8%)\n",
      "      ‚Ä¢ User filter: 747,424 (74.7%)\n",
      "      ‚Ä¢ Track frequency: 746,735 (74.7%)\n",
      "      ‚Ä¢ üéØ FINAL VALID: 746,735 (74.7%)\n",
      "\n",
      "üíæ Memory usage after pass 1 complete: 5098.2 MB\n",
      "üìä CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   üìÖ Temporal split at timestamp: 1487894400.0\n",
      "   üë• User activity split at: 63.0 playlists per user\n",
      "   üìã Strata Distribution:\n",
      "      ‚Ä¢ short_old_casual    :  2,650 ( 0.4%)\n",
      "      ‚Ä¢ short_old_active    : 152,326 (20.4%)\n",
      "      ‚Ä¢ short_recent_casual :  2,180 ( 0.3%)\n",
      "      ‚Ä¢ short_recent_active : 115,313 (15.4%)\n",
      "      ‚Ä¢ medium_old_casual   :  1,903 ( 0.3%)\n",
      "      ‚Ä¢ medium_old_active   : 133,339 (17.9%)\n",
      "      ‚Ä¢ medium_recent_casual:  2,309 ( 0.3%)\n",
      "      ‚Ä¢ medium_recent_active: 141,651 (19.0%)\n",
      "      ‚Ä¢ long_old_casual     :  1,114 ( 0.1%)\n",
      "      ‚Ä¢ long_old_active     : 81,317 (10.9%)\n",
      "      ‚Ä¢ long_recent_casual  :  1,657 ( 0.2%)\n",
      "      ‚Ä¢ long_recent_active  : 110,976 (14.9%)\n",
      "\n",
      "üíæ Memory usage after strata created: 5124.6 MB\n",
      "üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   üìä Global sampling ratio: 0.001\n",
      "   üèÜ Using priority scoring within strata\n",
      "\n",
      "      ‚Ä¢ short_old_casual    :    2 / 2,650 (avg score: 11.91)\n",
      "      ‚Ä¢ short_old_active    :  122 / 152,326 (avg score: 13.12)\n",
      "      ‚Ä¢ short_recent_casual :    1 / 2,180 (avg score: 13.93)\n",
      "      ‚Ä¢ short_recent_active :   92 / 115,313 (avg score: 12.81)\n",
      "      ‚Ä¢ medium_old_casual   :    1 / 1,903 (avg score: 11.52)\n",
      "      ‚Ä¢ medium_old_active   :  107 / 133,339 (avg score: 13.55)\n",
      "      ‚Ä¢ medium_recent_casual:    1 / 2,309 (avg score: 11.42)\n",
      "      ‚Ä¢ medium_recent_active:  113 / 141,651 (avg score: 13.46)\n",
      "      ‚Ä¢ long_old_casual     :    1 / 1,114 (avg score: 13.00)\n",
      "      ‚Ä¢ long_old_active     :   65 / 81,317 (avg score: 13.25)\n",
      "      ‚Ä¢ long_recent_casual  :    1 / 1,657 (avg score: 12.65)\n",
      "      ‚Ä¢ long_recent_active  :   89 / 110,976 (avg score: 13.01)\n",
      "\n",
      "   üéØ Selected 595 playlists for final loading\n",
      "   üìÅ Loading selected playlists...\n",
      "   üìÇ Loading from 449 files\n",
      "      üìñ File 1/449\n",
      "      üìñ File 101/449\n",
      "      üìñ File 201/449\n",
      "      üìñ File 301/449\n",
      "      üìñ File 401/449\n",
      "   ‚úÖ Loaded 595 final playlists\n",
      "üíæ Memory usage after pass 2 complete: 4620.0 MB\n",
      "\n",
      "üîç VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   üìä Final Entity Counts:\n",
      "      ‚Ä¢ Playlists: 595 (target: 600)\n",
      "      ‚Ä¢ Tracks: 18,749 (target: ~4,000)\n",
      "      ‚Ä¢ Artists: 6,585 (target: ~1,000)\n",
      "      ‚Ä¢ Albums: 11,914 (target: ~800)\n",
      "      ‚Ä¢ Users: 162 (target: ~200)\n",
      "      üéØ TOTAL: 38,005 (target: ~10,000)\n",
      "\n",
      "üéâ MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "üìä Results: 1,000,000 ‚Üí 595 playlists\n",
      "üìà Overall retention: 0.1%\n",
      "\n",
      "üéØ EXPERIMENTAL SCALE VERIFICATION:\n",
      "   ‚Ä¢ Actual total nodes: 38,005\n",
      "   ‚Ä¢ Target total nodes: 10,000\n",
      "   ‚Ä¢ Scale ratio: 3.800 (‚ö†Ô∏è ADJUST)\n",
      "\n",
      "‚úÖ SMALL SCALE COMPLETED:\n",
      "   üìÅ File: ../data/processed/spotify_scaled_hybrid_small.json\n",
      "   üì¶ Size: 10.1 MB\n",
      "   üìä Playlists: 595\n",
      "   üìä Total nodes: 38,005\n",
      "\n",
      "======================================================================\n",
      "üîß CREATING MEDIUM SCALE DATASET\n",
      "Target: 1,000 playlists\n",
      "======================================================================\n",
      "üéØ MULTI-SCALE SAMPLER (MEDIUM SCALE)\n",
      "======================================================================\n",
      "üìä SCALE-OPTIMIZED PARAMETERS:\n",
      "   ‚Ä¢ Target playlists: 1,000\n",
      "   ‚Ä¢ Min track frequency: 6\n",
      "   ‚Ä¢ Min user playlists: 7\n",
      "   ‚Ä¢ Expected total nodes: ~20,000\n",
      "\n",
      "üöÄ STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "üíæ Memory usage after start: 2332.5 MB\n",
      "üîç PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "üìÅ Found 1000 files\n",
      "üì¶ Created 50 batches of ~20 files each\n",
      "üöÄ Applying Core-Based Filters:\n",
      "   ‚úÖ Step 1: Playlist length (10-100 tracks)\n",
      "   ‚úÖ Step 2: User activity (‚â•7 playlists per user)\n",
      "   ‚úÖ Step 3: Track frequency (‚â•6 appearances)\n",
      "\n",
      "üìä Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ‚úÖ Identified 2,827 active users (target: ~300)\n",
      "\n",
      "üîç Sub-pass 1b: Applying all core filters...\n",
      "üì¶ Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,829 valid so far\n",
      "üíæ Memory usage after batch 1: 1732.1 MB\n",
      "üì¶ Processing batch 2/50\n",
      "üì¶ Processing batch 3/50\n",
      "üì¶ Processing batch 4/50\n",
      "üì¶ Processing batch 5/50\n",
      "üì¶ Processing batch 6/50\n",
      "üì¶ Processing batch 7/50\n",
      "üì¶ Processing batch 8/50\n",
      "üì¶ Processing batch 9/50\n",
      "üì¶ Processing batch 10/50\n",
      "üì¶ Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,122 valid so far\n",
      "üíæ Memory usage after batch 11: 2392.5 MB\n",
      "üì¶ Processing batch 12/50\n",
      "üì¶ Processing batch 13/50\n",
      "üì¶ Processing batch 14/50\n",
      "üì¶ Processing batch 15/50\n",
      "üì¶ Processing batch 16/50\n",
      "üì¶ Processing batch 17/50\n",
      "üì¶ Processing batch 18/50\n",
      "üì¶ Processing batch 19/50\n",
      "üì¶ Processing batch 20/50\n",
      "üì¶ Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,570 valid so far\n",
      "üíæ Memory usage after batch 21: 3110.8 MB\n",
      "üì¶ Processing batch 22/50\n",
      "üì¶ Processing batch 23/50\n",
      "üì¶ Processing batch 24/50\n",
      "üì¶ Processing batch 25/50\n",
      "üì¶ Processing batch 26/50\n",
      "üì¶ Processing batch 27/50\n",
      "üì¶ Processing batch 28/50\n",
      "üì¶ Processing batch 29/50\n",
      "üì¶ Processing batch 30/50\n",
      "üì¶ Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,283 valid so far\n",
      "üíæ Memory usage after batch 31: 3845.2 MB\n",
      "üì¶ Processing batch 32/50\n",
      "üì¶ Processing batch 33/50\n",
      "üì¶ Processing batch 34/50\n",
      "üì¶ Processing batch 35/50\n",
      "üì¶ Processing batch 36/50\n",
      "üì¶ Processing batch 37/50\n",
      "üì¶ Processing batch 38/50\n",
      "üì¶ Processing batch 39/50\n",
      "üì¶ Processing batch 40/50\n",
      "üì¶ Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,140 valid so far\n",
      "üíæ Memory usage after batch 41: 4502.1 MB\n",
      "üì¶ Processing batch 42/50\n",
      "üì¶ Processing batch 43/50\n",
      "üì¶ Processing batch 44/50\n",
      "üì¶ Processing batch 45/50\n",
      "üì¶ Processing batch 46/50\n",
      "üì¶ Processing batch 47/50\n",
      "üì¶ Processing batch 48/50\n",
      "üì¶ Processing batch 49/50\n",
      "üì¶ Processing batch 50/50\n",
      "\n",
      "üîç Applying final core filter: Track frequency (‚â•6)\n",
      "   ‚úÖ Identified 328,152 core tracks (target: ~8000)\n",
      "\n",
      "‚úÖ CORE-BASED FILTERING COMPLETE:\n",
      "   üìä Filtering Funnel:\n",
      "      ‚Ä¢ Total playlists: 1,000,000\n",
      "      ‚Ä¢ Length filter: 747,510 (74.8%)\n",
      "      ‚Ä¢ User filter: 746,560 (74.7%)\n",
      "      ‚Ä¢ Track frequency: 745,704 (74.6%)\n",
      "      ‚Ä¢ üéØ FINAL VALID: 745,704 (74.6%)\n",
      "\n",
      "üíæ Memory usage after pass 1 complete: 4556.1 MB\n",
      "üìä CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   üìÖ Temporal split at timestamp: 1487980800.0\n",
      "   üë• User activity split at: 40.0 playlists per user\n",
      "   üìã Strata Distribution:\n",
      "      ‚Ä¢ short_old_casual    :  6,209 ( 0.8%)\n",
      "      ‚Ä¢ short_old_active    : 148,760 (19.9%)\n",
      "      ‚Ä¢ short_recent_casual :  4,941 ( 0.7%)\n",
      "      ‚Ä¢ short_recent_active : 112,086 (15.0%)\n",
      "      ‚Ä¢ medium_old_casual   :  4,647 ( 0.6%)\n",
      "      ‚Ä¢ medium_old_active   : 130,675 (17.5%)\n",
      "      ‚Ä¢ medium_recent_casual:  5,262 ( 0.7%)\n",
      "      ‚Ä¢ medium_recent_active: 138,273 (18.5%)\n",
      "      ‚Ä¢ long_old_casual     :  2,646 ( 0.4%)\n",
      "      ‚Ä¢ long_old_active     : 79,853 (10.7%)\n",
      "      ‚Ä¢ long_recent_casual  :  3,715 ( 0.5%)\n",
      "      ‚Ä¢ long_recent_active  : 108,637 (14.6%)\n",
      "\n",
      "üíæ Memory usage after strata created: 2653.2 MB\n",
      "üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   üìä Global sampling ratio: 0.001\n",
      "   üèÜ Using priority scoring within strata\n",
      "\n",
      "      ‚Ä¢ short_old_casual    :    8 / 6,209 (avg score: 12.44)\n",
      "      ‚Ä¢ short_old_active    :  199 / 148,760 (avg score: 12.40)\n",
      "      ‚Ä¢ short_recent_casual :    6 / 4,941 (avg score: 11.73)\n",
      "      ‚Ä¢ short_recent_active :  150 / 112,086 (avg score: 12.21)\n",
      "      ‚Ä¢ medium_old_casual   :    6 / 4,647 (avg score: 13.28)\n",
      "      ‚Ä¢ medium_old_active   :  175 / 130,675 (avg score: 12.72)\n",
      "      ‚Ä¢ medium_recent_casual:    7 / 5,262 (avg score: 11.85)\n",
      "      ‚Ä¢ medium_recent_active:  185 / 138,273 (avg score: 12.71)\n",
      "      ‚Ä¢ long_old_casual     :    3 / 2,646 (avg score: 12.75)\n",
      "      ‚Ä¢ long_old_active     :  107 / 79,853 (avg score: 12.62)\n",
      "      ‚Ä¢ long_recent_casual  :    4 / 3,715 (avg score: 12.49)\n",
      "      ‚Ä¢ long_recent_active  :  145 / 108,637 (avg score: 12.34)\n",
      "\n",
      "   üéØ Selected 995 playlists for final loading\n",
      "   üìÅ Loading selected playlists...\n",
      "   üìÇ Loading from 635 files\n",
      "      üìñ File 1/635\n",
      "      üìñ File 101/635\n",
      "      üìñ File 201/635\n",
      "      üìñ File 301/635\n",
      "      üìñ File 401/635\n",
      "      üìñ File 501/635\n",
      "      üìñ File 601/635\n",
      "   ‚úÖ Loaded 995 final playlists\n",
      "üíæ Memory usage after pass 2 complete: 5270.6 MB\n",
      "\n",
      "üîç VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   üìä Final Entity Counts:\n",
      "      ‚Ä¢ Playlists: 995 (target: 1,000)\n",
      "      ‚Ä¢ Tracks: 28,672 (target: ~8,000)\n",
      "      ‚Ä¢ Artists: 9,504 (target: ~2,000)\n",
      "      ‚Ä¢ Albums: 17,710 (target: ~1,500)\n",
      "      ‚Ä¢ Users: 413 (target: ~300)\n",
      "      üéØ TOTAL: 57,294 (target: ~20,000)\n",
      "\n",
      "üéâ MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "üìä Results: 1,000,000 ‚Üí 995 playlists\n",
      "üìà Overall retention: 0.1%\n",
      "\n",
      "üéØ EXPERIMENTAL SCALE VERIFICATION:\n",
      "   ‚Ä¢ Actual total nodes: 57,294\n",
      "   ‚Ä¢ Target total nodes: 20,000\n",
      "   ‚Ä¢ Scale ratio: 2.865 (‚ö†Ô∏è ADJUST)\n",
      "\n",
      "‚úÖ MEDIUM SCALE COMPLETED:\n",
      "   üìÅ File: ../data/processed/spotify_scaled_hybrid_medium.json\n",
      "   üì¶ Size: 16.9 MB\n",
      "   üìä Playlists: 995\n",
      "   üìä Total nodes: 57,294\n",
      "\n",
      "======================================================================\n",
      "üîß CREATING LARGE SCALE DATASET\n",
      "Target: 1,500 playlists\n",
      "======================================================================\n",
      "üéØ MULTI-SCALE SAMPLER (LARGE SCALE)\n",
      "======================================================================\n",
      "üìä SCALE-OPTIMIZED PARAMETERS:\n",
      "   ‚Ä¢ Target playlists: 1,500\n",
      "   ‚Ä¢ Min track frequency: 8\n",
      "   ‚Ä¢ Min user playlists: 10\n",
      "   ‚Ä¢ Expected total nodes: ~30,000\n",
      "\n",
      "üöÄ STARTING MULTI-SCALE HYBRID SAMPLING\n",
      "======================================================================\n",
      "üíæ Memory usage after start: 3500.0 MB\n",
      "üîç PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "üìÅ Found 1000 files\n",
      "üì¶ Created 50 batches of ~20 files each\n",
      "üöÄ Applying Core-Based Filters:\n",
      "   ‚úÖ Step 1: Playlist length (10-100 tracks)\n",
      "   ‚úÖ Step 2: User activity (‚â•10 playlists per user)\n",
      "   ‚úÖ Step 3: Track frequency (‚â•8 appearances)\n",
      "\n",
      "üìä Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ‚úÖ Identified 2,852 active users (target: ~500)\n",
      "\n",
      "üîç Sub-pass 1b: Applying all core filters...\n",
      "üì¶ Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,810 valid so far\n",
      "üíæ Memory usage after batch 1: 1676.4 MB\n",
      "üì¶ Processing batch 2/50\n",
      "üì¶ Processing batch 3/50\n",
      "üì¶ Processing batch 4/50\n",
      "üì¶ Processing batch 5/50\n",
      "üì¶ Processing batch 6/50\n",
      "üì¶ Processing batch 7/50\n",
      "üì¶ Processing batch 8/50\n",
      "üì¶ Processing batch 9/50\n",
      "üì¶ Processing batch 10/50\n",
      "üì¶ Processing batch 11/50\n",
      "   Progress: 220,000 seen, 163,923 valid so far\n",
      "üíæ Memory usage after batch 11: 2370.4 MB\n",
      "üì¶ Processing batch 12/50\n",
      "üì¶ Processing batch 13/50\n",
      "üì¶ Processing batch 14/50\n",
      "üì¶ Processing batch 15/50\n",
      "üì¶ Processing batch 16/50\n",
      "üì¶ Processing batch 17/50\n",
      "üì¶ Processing batch 18/50\n",
      "üì¶ Processing batch 19/50\n",
      "üì¶ Processing batch 20/50\n",
      "üì¶ Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,189 valid so far\n",
      "üíæ Memory usage after batch 21: 3188.5 MB\n",
      "üì¶ Processing batch 22/50\n",
      "üì¶ Processing batch 23/50\n",
      "üì¶ Processing batch 24/50\n",
      "üì¶ Processing batch 25/50\n",
      "üì¶ Processing batch 26/50\n",
      "üì¶ Processing batch 27/50\n",
      "üì¶ Processing batch 28/50\n",
      "üì¶ Processing batch 29/50\n",
      "üì¶ Processing batch 30/50\n",
      "üì¶ Processing batch 31/50\n",
      "   Progress: 620,000 seen, 462,710 valid so far\n",
      "üíæ Memory usage after batch 31: 3828.1 MB\n",
      "üì¶ Processing batch 32/50\n",
      "üì¶ Processing batch 33/50\n",
      "üì¶ Processing batch 34/50\n",
      "üì¶ Processing batch 35/50\n",
      "üì¶ Processing batch 36/50\n",
      "üì¶ Processing batch 37/50\n",
      "üì¶ Processing batch 38/50\n",
      "üì¶ Processing batch 39/50\n",
      "üì¶ Processing batch 40/50\n",
      "üì¶ Processing batch 41/50\n",
      "   Progress: 820,000 seen, 611,422 valid so far\n",
      "üíæ Memory usage after batch 41: 4559.0 MB\n",
      "üì¶ Processing batch 42/50\n",
      "üì¶ Processing batch 43/50\n",
      "üì¶ Processing batch 44/50\n",
      "üì¶ Processing batch 45/50\n",
      "üì¶ Processing batch 46/50\n",
      "üì¶ Processing batch 47/50\n",
      "üì¶ Processing batch 48/50\n",
      "üì¶ Processing batch 49/50\n",
      "üì¶ Processing batch 50/50\n",
      "\n",
      "üîç Applying final core filter: Track frequency (‚â•8)\n",
      "   ‚úÖ Identified 265,883 core tracks (target: ~12000)\n",
      "\n",
      "‚úÖ CORE-BASED FILTERING COMPLETE:\n",
      "   üìä Filtering Funnel:\n",
      "      ‚Ä¢ Total playlists: 1,000,000\n",
      "      ‚Ä¢ Length filter: 747,510 (74.8%)\n",
      "      ‚Ä¢ User filter: 745,687 (74.6%)\n",
      "      ‚Ä¢ Track frequency: 744,508 (74.5%)\n",
      "      ‚Ä¢ üéØ FINAL VALID: 744,508 (74.5%)\n",
      "\n",
      "üíæ Memory usage after pass 1 complete: 5094.4 MB\n",
      "üìä CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   üìÖ Temporal split at timestamp: 1487980800.0\n",
      "   üë• User activity split at: 33.0 playlists per user\n",
      "   üìã Strata Distribution:\n",
      "      ‚Ä¢ short_old_casual    :  5,786 ( 0.8%)\n",
      "      ‚Ä¢ short_old_active    : 148,848 (20.0%)\n",
      "      ‚Ä¢ short_recent_casual :  5,193 ( 0.7%)\n",
      "      ‚Ä¢ short_recent_active : 111,624 (15.0%)\n",
      "      ‚Ä¢ medium_old_casual   :  4,457 ( 0.6%)\n",
      "      ‚Ä¢ medium_old_active   : 130,646 (17.5%)\n",
      "      ‚Ä¢ medium_recent_casual:  5,525 ( 0.7%)\n",
      "      ‚Ä¢ medium_recent_active: 137,843 (18.5%)\n",
      "      ‚Ä¢ long_old_casual     :  2,533 ( 0.3%)\n",
      "      ‚Ä¢ long_old_active     : 79,818 (10.7%)\n",
      "      ‚Ä¢ long_recent_casual  :  3,977 ( 0.5%)\n",
      "      ‚Ä¢ long_recent_active  : 108,258 (14.5%)\n",
      "\n",
      "üíæ Memory usage after strata created: 5115.8 MB\n",
      "üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   üìä Global sampling ratio: 0.002\n",
      "   üèÜ Using priority scoring within strata\n",
      "\n",
      "      ‚Ä¢ short_old_casual    :   11 / 5,786 (avg score: 12.02)\n",
      "      ‚Ä¢ short_old_active    :  299 / 148,848 (avg score: 11.83)\n",
      "      ‚Ä¢ short_recent_casual :   10 / 5,193 (avg score: 11.17)\n",
      "      ‚Ä¢ short_recent_active :  224 / 111,624 (avg score: 11.70)\n",
      "      ‚Ä¢ medium_old_casual   :    8 / 4,457 (avg score: 12.64)\n",
      "      ‚Ä¢ medium_old_active   :  263 / 130,646 (avg score: 12.15)\n",
      "      ‚Ä¢ medium_recent_casual:   11 / 5,525 (avg score: 11.31)\n",
      "      ‚Ä¢ medium_recent_active:  277 / 137,843 (avg score: 12.12)\n",
      "      ‚Ä¢ long_old_casual     :    5 / 2,533 (avg score: 12.31)\n",
      "      ‚Ä¢ long_old_active     :  160 / 79,818 (avg score: 12.05)\n",
      "      ‚Ä¢ long_recent_casual  :    8 / 3,977 (avg score: 11.67)\n",
      "      ‚Ä¢ long_recent_active  :  218 / 108,258 (avg score: 11.82)\n",
      "\n",
      "   üéØ Selected 1,494 playlists for final loading\n",
      "   üìÅ Loading selected playlists...\n",
      "   üìÇ Loading from 792 files\n",
      "      üìñ File 1/792\n",
      "      üìñ File 101/792\n",
      "      üìñ File 201/792\n",
      "      üìñ File 301/792\n",
      "      üìñ File 401/792\n",
      "      üìñ File 501/792\n",
      "      üìñ File 601/792\n",
      "      üìñ File 701/792\n",
      "   ‚úÖ Loaded 1,494 final playlists\n",
      "üíæ Memory usage after pass 2 complete: 2268.6 MB\n",
      "\n",
      "üîç VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   üìä Final Entity Counts:\n",
      "      ‚Ä¢ Playlists: 1,494 (target: 1,500)\n",
      "      ‚Ä¢ Tracks: 39,213 (target: ~12,000)\n",
      "      ‚Ä¢ Artists: 12,268 (target: ~3,000)\n",
      "      ‚Ä¢ Albums: 23,843 (target: ~2,500)\n",
      "      ‚Ä¢ Users: 531 (target: ~500)\n",
      "      üéØ TOTAL: 77,349 (target: ~30,000)\n",
      "\n",
      "üéâ MULTI-SCALE HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "üìä Results: 1,000,000 ‚Üí 1,494 playlists\n",
      "üìà Overall retention: 0.1%\n",
      "\n",
      "üéØ EXPERIMENTAL SCALE VERIFICATION:\n",
      "   ‚Ä¢ Actual total nodes: 77,349\n",
      "   ‚Ä¢ Target total nodes: 30,000\n",
      "   ‚Ä¢ Scale ratio: 2.578 (‚ö†Ô∏è ADJUST)\n",
      "\n",
      "‚úÖ LARGE SCALE COMPLETED:\n",
      "   üìÅ File: ../data/processed/spotify_scaled_hybrid_large.json\n",
      "   üì¶ Size: 25.4 MB\n",
      "   üìä Playlists: 1,494\n",
      "   üìä Total nodes: 77,349\n",
      "\n",
      "======================================================================\n",
      "üéâ MULTI-SCALE DATASET CREATION COMPLETED!\n",
      "======================================================================\n",
      "üìä CREATED DATASETS:\n",
      "Scale    File                                 Playlists  Nodes     Size\n",
      "--------------------------------------------------------------------------------\n",
      "tiny     spotify_scaled_hybrid_tiny.json     299        22,180    5.1MB\n",
      "small    spotify_scaled_hybrid_small.json    595        38,005    10.1MB\n",
      "medium   spotify_scaled_hybrid_medium.json   995        57,294    16.9MB\n",
      "large    spotify_scaled_hybrid_large.json    1,494      77,349    25.4MB\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
