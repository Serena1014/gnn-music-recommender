{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hybird Core-based + Stratified Sampling",
   "id": "784adb025faa243f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-23T14:17:13.308394Z",
     "start_time": "2025-06-23T14:17:13.304617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set, Iterator\n",
    "import gc\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:17:26.305944Z",
     "start_time": "2025-06-23T14:17:26.300810Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"üéµ HYBRID CORE-BASED + STRATIFIED SAMPLING\")\n",
    "print(\"=\" * 70)\n",
    "print(\"üíæ Memory-efficient streaming approach for large datasets\")\n",
    "print(\"üéØ Combines: Core filtering + Stratified sampling + Priority scoring\")\n",
    "print()"
   ],
   "id": "31b06b8340da4572",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéµ HYBRID CORE-BASED + STRATIFIED SAMPLING\n",
      "======================================================================\n",
      "üíæ Memory-efficient streaming approach for large datasets\n",
      "üéØ Combines: Core filtering + Stratified sampling + Priority scoring\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Memory Monitoring Utilities",
   "id": "32ade12d48317531"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T14:18:26.778805Z",
     "start_time": "2025-06-23T14:18:26.773245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    try:\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / 1024 / 1024  # MB\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def print_memory_status(stage: str):\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    memory_mb = get_memory_usage()\n",
    "    print(f\"üíæ Memory usage after {stage}: {memory_mb:.1f} MB\")\n",
    "\n",
    "print(\"‚úÖ Memory monitoring utilities loaded\")"
   ],
   "id": "918c792c48ade06c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Memory monitoring utilities loaded\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Streaming Sampler Class",
   "id": "64cd9936bbee5e60"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:00:13.587017Z",
     "start_time": "2025-06-23T16:00:13.423407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HybridStreamingSampler:\n",
    "    \"\"\"\n",
    "    Hybrid Core-Based + Stratified Streaming Sampler\n",
    "    - Pass 1: Core-based filtering (length, user activity, track frequency)\n",
    "    - Pass 2: Stratified sampling with priority scoring\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 target_playlists: int = 50000,\n",
    "                 batch_size: int = 20,\n",
    "                 min_playlist_length: int = 10,\n",
    "                 max_playlist_length: int = 100,\n",
    "                 min_track_frequency: int = 5,\n",
    "                 min_user_playlists: int = 3):\n",
    "\n",
    "        self.target_playlists = target_playlists\n",
    "        self.batch_size = batch_size\n",
    "        self.min_playlist_length = min_playlist_length\n",
    "        self.max_playlist_length = max_playlist_length\n",
    "        self.min_track_frequency = min_track_frequency\n",
    "        self.min_user_playlists = min_user_playlists\n",
    "\n",
    "        # Statistics collectors\n",
    "        self.track_counts = Counter()\n",
    "        self.user_counts = Counter()\n",
    "        self.playlist_stats = []\n",
    "\n",
    "        print(f\"üéØ Hybrid Streaming Sampler Configuration:\")\n",
    "        print(f\"   ‚Ä¢ Target playlists: {target_playlists:,}\")\n",
    "        print(f\"   ‚Ä¢ Batch size: {batch_size} files at a time\")\n",
    "        print(f\"   ‚Ä¢ Playlist length: {min_playlist_length}-{max_playlist_length} tracks\")\n",
    "        print(f\"   ‚Ä¢ Min track frequency: {min_track_frequency} playlists\")\n",
    "        print(f\"   ‚Ä¢ Min user playlists: {min_user_playlists} playlists\")\n",
    "        print()\n",
    "\n",
    "    def get_file_batches(self, file_pattern: str) -> List[List[str]]:\n",
    "        \"\"\"Split files into manageable batches\"\"\"\n",
    "        file_paths = glob.glob(file_pattern)\n",
    "        file_paths.sort()\n",
    "\n",
    "        if not file_paths:\n",
    "            raise FileNotFoundError(f\"No files found: {file_pattern}\")\n",
    "\n",
    "        print(f\"üìÅ Found {len(file_paths)} files\")\n",
    "\n",
    "        # Split into batches\n",
    "        batches = []\n",
    "        for i in range(0, len(file_paths), self.batch_size):\n",
    "            batch = file_paths[i:i + self.batch_size]\n",
    "            batches.append(batch)\n",
    "\n",
    "        print(f\"üì¶ Created {len(batches)} batches of ~{self.batch_size} files each\")\n",
    "        return batches\n",
    "\n",
    "    def _extract_user_id(self, playlist: Dict) -> str:\n",
    "        \"\"\"Extract user identifier from playlist\"\"\"\n",
    "        # Method 1: Use name-based grouping\n",
    "        name = playlist.get('name', '').lower().strip()\n",
    "        if name:\n",
    "            # Use first word of playlist name as user identifier\n",
    "            words = name.split()\n",
    "            if words:\n",
    "                user_id = words[0]\n",
    "                # Clean user_id (keep only alphanumeric)\n",
    "                user_id = ''.join(c for c in user_id if c.isalnum())[:20]\n",
    "                return user_id if user_id else 'anonymous'\n",
    "\n",
    "        # Method 2: Use PID-based grouping (if no name)\n",
    "        pid = playlist.get('pid', 0)\n",
    "        return f\"user_{pid % 10000}\"  # Group into ~10k users\n",
    "\n",
    "    def pass1_core_filtering(self, file_pattern: str) -> Dict:\n",
    "        \"\"\"\n",
    "        PASS 1: Core-based filtering + Statistics collection\n",
    "        \"\"\"\n",
    "        print(\"üîç PASS 1: HYBRID CORE-BASED FILTERING\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        batches = self.get_file_batches(file_pattern)\n",
    "\n",
    "        # Stage counts\n",
    "        stage_counts = {\n",
    "            'total_seen': 0,\n",
    "            'passed_length_filter': 0,\n",
    "            'passed_user_filter': 0,\n",
    "            'passed_track_frequency_filter': 0,\n",
    "            'final_valid': 0\n",
    "        }\n",
    "\n",
    "        print(\"üöÄ Applying Core-Based Filters:\")\n",
    "        print(f\"   ‚úÖ Step 1: Playlist length ({self.min_playlist_length}-{self.max_playlist_length} tracks)\")\n",
    "        print(f\"   ‚úÖ Step 2: User activity (‚â•{self.min_user_playlists} playlists per user)\")\n",
    "        print(f\"   ‚úÖ Step 3: Track frequency (‚â•{self.min_track_frequency} appearances)\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1a: Collect user activity statistics\n",
    "        print(\"üìä Sub-pass 1a: Collecting user activity statistics...\")\n",
    "        user_playlist_count = Counter()\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"   User stats progress: {batch_idx + 1}/{len(batches)} batches\")\n",
    "\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "\n",
    "                    for playlist in file_playlists:\n",
    "                        user_id = self._extract_user_id(playlist)\n",
    "                        user_playlist_count[user_id] += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        # Identify active users\n",
    "        active_users = {\n",
    "            user for user, count in user_playlist_count.items()\n",
    "            if count >= self.min_user_playlists\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Identified {len(active_users):,} active users\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1b: Apply all core filters\n",
    "        print(\"üîç Sub-pass 1b: Applying all core filters...\")\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            print(f\"üì¶ Processing batch {batch_idx + 1}/{len(batches)}\")\n",
    "\n",
    "            batch_playlists = []\n",
    "\n",
    "            # Load batch\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "\n",
    "                    # Add source file info\n",
    "                    for playlist in file_playlists:\n",
    "                        playlist['_source_file'] = file_path\n",
    "\n",
    "                    batch_playlists.extend(file_playlists)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error loading {os.path.basename(file_path)}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Process batch with core filtering\n",
    "            for playlist_idx, playlist in enumerate(batch_playlists):\n",
    "                stage_counts['total_seen'] += 1\n",
    "\n",
    "                # CORE FILTER 1: Playlist length\n",
    "                tracks = playlist.get('tracks', [])\n",
    "                playlist_length = len(tracks)\n",
    "\n",
    "                if not (self.min_playlist_length <= playlist_length <= self.max_playlist_length):\n",
    "                    continue\n",
    "                stage_counts['passed_length_filter'] += 1\n",
    "\n",
    "                # CORE FILTER 2: User activity\n",
    "                user_id = self._extract_user_id(playlist)\n",
    "                if user_id not in active_users:\n",
    "                    continue\n",
    "                stage_counts['passed_user_filter'] += 1\n",
    "\n",
    "                # Count tracks for frequency analysis\n",
    "                playlist_tracks = set()\n",
    "                for track in tracks:\n",
    "                    track_uri = track.get('track_uri', '')\n",
    "                    if track_uri:\n",
    "                        self.track_counts[track_uri] += 1\n",
    "                        playlist_tracks.add(track_uri)\n",
    "\n",
    "                self.user_counts[user_id] += 1\n",
    "\n",
    "                # Store playlist metadata\n",
    "                playlist_metadata = {\n",
    "                    'file_path': playlist['_source_file'],\n",
    "                    'pid': playlist.get('pid'),\n",
    "                    'length': playlist_length,\n",
    "                    'modified_at': playlist.get('modified_at', 0),\n",
    "                    'user_id': user_id,\n",
    "                    'track_uris': list(playlist_tracks),\n",
    "                    'name': playlist.get('name', ''),\n",
    "                    'collaborative': playlist.get('collaborative', False),\n",
    "                    'num_followers': playlist.get('num_followers', 0)\n",
    "                }\n",
    "\n",
    "                self.playlist_stats.append(playlist_metadata)\n",
    "\n",
    "            # Clear batch from memory\n",
    "            del batch_playlists\n",
    "            gc.collect()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"   Progress: {stage_counts['total_seen']:,} seen, {len(self.playlist_stats):,} valid so far\")\n",
    "                print_memory_status(f\"batch {batch_idx + 1}\")\n",
    "\n",
    "        # CORE FILTER 3: Track frequency\n",
    "        print(f\"\\nüîç Applying final core filter: Track frequency\")\n",
    "\n",
    "        core_tracks = {\n",
    "            track for track, count in self.track_counts.items()\n",
    "            if count >= self.min_track_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Identified {len(core_tracks):,} core tracks\")\n",
    "\n",
    "        # Filter playlists that have core tracks\n",
    "        filtered_playlist_stats = []\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            playlist_tracks = set(playlist_meta['track_uris'])\n",
    "            if playlist_tracks.intersection(core_tracks):\n",
    "                filtered_playlist_stats.append(playlist_meta)\n",
    "                stage_counts['passed_track_frequency_filter'] += 1\n",
    "\n",
    "        self.playlist_stats = filtered_playlist_stats\n",
    "        stage_counts['final_valid'] = len(filtered_playlist_stats)\n",
    "\n",
    "        print(f\"\\n‚úÖ CORE-BASED FILTERING COMPLETE:\")\n",
    "        print(f\"   üìä Filtering Funnel:\")\n",
    "        print(f\"      ‚Ä¢ Total playlists: {stage_counts['total_seen']:,}\")\n",
    "        print(f\"      ‚Ä¢ Length filter: {stage_counts['passed_length_filter']:,} ({stage_counts['passed_length_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ User filter: {stage_counts['passed_user_filter']:,} ({stage_counts['passed_user_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ Track frequency: {stage_counts['passed_track_frequency_filter']:,} ({stage_counts['passed_track_frequency_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ üéØ FINAL VALID: {stage_counts['final_valid']:,} ({stage_counts['final_valid']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "        return {\n",
    "            'total_playlists': stage_counts['total_seen'],\n",
    "            'valid_playlists': stage_counts['final_valid'],\n",
    "            'core_tracks': core_tracks,\n",
    "            'active_users': active_users,\n",
    "            'unique_tracks': len(self.track_counts),\n",
    "            'stage_counts': stage_counts\n",
    "        }\n",
    "\n",
    "    def create_strata(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"Create comprehensive strata for stratified sampling\"\"\"\n",
    "        print(\"üìä CREATING STRATIFIED SAMPLING STRATA\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Get temporal split\n",
    "        timestamps = [p['modified_at'] for p in self.playlist_stats if p['modified_at'] > 0]\n",
    "        if timestamps:\n",
    "            median_time = np.median(timestamps)\n",
    "        else:\n",
    "            median_time = 1500000000  # Default\n",
    "\n",
    "        # Get user activity split\n",
    "        user_playlist_counts = {}\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_playlist_counts[user_id] = user_playlist_counts.get(user_id, 0) + 1\n",
    "\n",
    "        user_activity_median = np.median(list(user_playlist_counts.values())) if user_playlist_counts else 5\n",
    "\n",
    "        print(f\"   üìÖ Temporal split at timestamp: {median_time}\")\n",
    "        print(f\"   üë• User activity split at: {user_activity_median} playlists per user\")\n",
    "\n",
    "        # Create 12 comprehensive strata\n",
    "        strata = {\n",
    "            'short_old_casual': [], 'short_old_active': [],\n",
    "            'short_recent_casual': [], 'short_recent_active': [],\n",
    "            'medium_old_casual': [], 'medium_old_active': [],\n",
    "            'medium_recent_casual': [], 'medium_recent_active': [],\n",
    "            'long_old_casual': [], 'long_old_active': [],\n",
    "            'long_recent_casual': [], 'long_recent_active': []\n",
    "        }\n",
    "\n",
    "        for i, playlist_meta in enumerate(self.playlist_stats):\n",
    "            length = playlist_meta['length']\n",
    "            timestamp = playlist_meta['modified_at']\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_activity = user_playlist_counts.get(user_id, 1)\n",
    "\n",
    "            # Length category\n",
    "            if length <= 30:\n",
    "                length_cat = 'short'\n",
    "            elif length <= 60:\n",
    "                length_cat = 'medium'\n",
    "            else:\n",
    "                length_cat = 'long'\n",
    "\n",
    "            # Time category\n",
    "            time_cat = 'recent' if timestamp >= median_time else 'old'\n",
    "\n",
    "            # User activity category\n",
    "            activity_cat = 'active' if user_activity >= user_activity_median else 'casual'\n",
    "\n",
    "            # Combine into stratum\n",
    "            stratum_key = f\"{length_cat}_{time_cat}_{activity_cat}\"\n",
    "            strata[stratum_key].append(i)\n",
    "\n",
    "        # Print strata distribution\n",
    "        print(\"   üìã Strata Distribution:\")\n",
    "        total_playlists = len(self.playlist_stats)\n",
    "\n",
    "        for stratum, indices in strata.items():\n",
    "            if indices:  # Only show non-empty strata\n",
    "                percentage = len(indices) / total_playlists * 100\n",
    "                print(f\"      ‚Ä¢ {stratum:20s}: {len(indices):6,} ({percentage:4.1f}%)\")\n",
    "\n",
    "        print()\n",
    "        return strata\n",
    "\n",
    "    def _calculate_priority_score(self, playlist_meta: Dict) -> float:\n",
    "        \"\"\"Calculate priority score for playlist selection\"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Factor 1: Track diversity (30% weight)\n",
    "        unique_tracks = len(playlist_meta['track_uris'])\n",
    "        playlist_length = playlist_meta['length']\n",
    "        if playlist_length > 0:\n",
    "            track_diversity_ratio = unique_tracks / playlist_length\n",
    "            score += track_diversity_ratio * 3.0\n",
    "\n",
    "        # Factor 2: User engagement (25% weight)\n",
    "        num_followers = playlist_meta.get('num_followers', 0)\n",
    "        if num_followers > 0:\n",
    "            follower_score = min(np.log10(num_followers + 1), 3.0)\n",
    "            score += follower_score * 2.5\n",
    "\n",
    "        # Factor 3: Playlist completeness (20% weight)\n",
    "        name = playlist_meta.get('name', '')\n",
    "        has_good_name = len(name.strip()) > 3 and not name.lower().startswith('my playlist')\n",
    "        if has_good_name:\n",
    "            score += 2.0\n",
    "\n",
    "        # Factor 4: Collaborative playlists bonus (10% weight)\n",
    "        if playlist_meta.get('collaborative', False):\n",
    "            score += 1.0\n",
    "\n",
    "        # Factor 5: Length balance bonus (15% weight)\n",
    "        length = playlist_meta['length']\n",
    "        if 20 <= length <= 80:  # Sweet spot\n",
    "            score += 1.5\n",
    "\n",
    "        return score\n",
    "\n",
    "    def pass2_stratified_sampling(self, strata: Dict[str, List[int]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        PASS 2: Stratified sampling with priority scoring\n",
    "        \"\"\"\n",
    "        print(\"üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        total_available = len(self.playlist_stats)\n",
    "\n",
    "        if total_available <= self.target_playlists:\n",
    "            print(f\"   üìù Available ({total_available:,}) ‚â§ target ({self.target_playlists:,})\")\n",
    "            selected_indices = list(range(total_available))\n",
    "        else:\n",
    "            sampling_ratio = self.target_playlists / total_available\n",
    "            selected_indices = set()\n",
    "\n",
    "            print(f\"   üìä Global sampling ratio: {sampling_ratio:.3f}\")\n",
    "            print(f\"   üèÜ Using priority scoring within strata\")\n",
    "            print()\n",
    "\n",
    "            for stratum, indices in strata.items():\n",
    "                if not indices:\n",
    "                    continue\n",
    "\n",
    "                stratum_target = max(1, int(len(indices) * sampling_ratio))\n",
    "                stratum_target = min(stratum_target, len(indices))\n",
    "\n",
    "                # Score playlists in this stratum\n",
    "                scored_playlists = []\n",
    "                for idx in indices:\n",
    "                    playlist_meta = self.playlist_stats[idx]\n",
    "                    score = self._calculate_priority_score(playlist_meta)\n",
    "                    scored_playlists.append((idx, score))\n",
    "\n",
    "                # Sort by score and sample\n",
    "                scored_playlists.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Hybrid: 70% top-scored + 30% random\n",
    "                top_count = int(stratum_target * 0.7)\n",
    "                random_count = stratum_target - top_count\n",
    "\n",
    "                selected = [idx for idx, _ in scored_playlists[:top_count]]\n",
    "\n",
    "                if random_count > 0 and len(scored_playlists) > top_count:\n",
    "                    remaining = [idx for idx, _ in scored_playlists[top_count:]]\n",
    "                    if len(remaining) >= random_count:\n",
    "                        selected.extend(random.sample(remaining, random_count))\n",
    "                    else:\n",
    "                        selected.extend(remaining)\n",
    "\n",
    "                selected_indices.update(selected)\n",
    "\n",
    "                avg_score = np.mean([score for _, score in scored_playlists[:len(selected)]])\n",
    "                print(f\"      ‚Ä¢ {stratum:20s}: {len(selected):4,} / {len(indices):5,} (avg score: {avg_score:.2f})\")\n",
    "\n",
    "            selected_indices = list(selected_indices)\n",
    "\n",
    "        print(f\"\\n   üéØ Selected {len(selected_indices):,} playlists for final loading\")\n",
    "\n",
    "        # Load selected playlists\n",
    "        return self._load_selected_playlists(selected_indices)\n",
    "\n",
    "    def _load_selected_playlists(self, selected_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"Load only the selected playlists from files\"\"\"\n",
    "        print(\"   üìÅ Loading selected playlists...\")\n",
    "\n",
    "        # Group by file\n",
    "        file_to_playlists = defaultdict(list)\n",
    "        for idx in selected_indices:\n",
    "            playlist_meta = self.playlist_stats[idx]\n",
    "            file_path = playlist_meta['file_path']\n",
    "            file_to_playlists[file_path].append(playlist_meta)\n",
    "\n",
    "        print(f\"   üìÇ Loading from {len(file_to_playlists)} files\")\n",
    "\n",
    "        # Load playlists\n",
    "        final_playlists = []\n",
    "\n",
    "        for file_idx, (file_path, playlist_metas) in enumerate(file_to_playlists.items()):\n",
    "            if file_idx % 100 == 0:\n",
    "                print(f\"      üìñ File {file_idx + 1}/{len(file_to_playlists)}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                file_playlists = data.get('playlists', [])\n",
    "                pid_to_playlist = {p.get('pid'): p for p in file_playlists}\n",
    "\n",
    "                for meta in playlist_metas:\n",
    "                    pid = meta['pid']\n",
    "                    if pid in pid_to_playlist:\n",
    "                        playlist = pid_to_playlist[pid]\n",
    "                        playlist['_sampling_score'] = self._calculate_priority_score(meta)\n",
    "                        final_playlists.append(playlist)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        print(f\"   ‚úÖ Loaded {len(final_playlists):,} final playlists\")\n",
    "        return final_playlists\n",
    "\n",
    "    def run_hybrid_sampling(self, file_pattern: str) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"Main method: Complete hybrid sampling workflow\"\"\"\n",
    "        print(\"üöÄ STARTING HYBRID CORE-BASED + STRATIFIED SAMPLING\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        print_memory_status(\"start\")\n",
    "\n",
    "        # Pass 1: Core-based filtering\n",
    "        stats = self.pass1_core_filtering(file_pattern)\n",
    "        print_memory_status(\"pass 1 complete\")\n",
    "\n",
    "        # Create strata\n",
    "        strata = self.create_strata()\n",
    "        print_memory_status(\"strata created\")\n",
    "\n",
    "        # Pass 2: Stratified sampling\n",
    "        final_playlists = self.pass2_stratified_sampling(strata)\n",
    "        print_memory_status(\"pass 2 complete\")\n",
    "\n",
    "        # Final statistics\n",
    "        final_stats = {\n",
    "            'methodology': 'hybrid_core_based_stratified_streaming',\n",
    "            'original_total': stats['total_playlists'],\n",
    "            'final_sampled': len(final_playlists),\n",
    "            'retention_rate': len(final_playlists) / stats['total_playlists'],\n",
    "            'core_filtering_retention': stats['valid_playlists'] / stats['total_playlists'],\n",
    "            'unique_tracks': stats['unique_tracks'],\n",
    "            'core_tracks_count': len(stats['core_tracks']),\n",
    "            'active_users_count': len(stats['active_users']),\n",
    "            'stage_counts': stats['stage_counts']\n",
    "        }\n",
    "\n",
    "        print(\"\\nüéâ HYBRID SAMPLING COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìä Results: {stats['total_playlists']:,} ‚Üí {len(final_playlists):,} playlists\")\n",
    "        print(f\"üìà Overall retention: {len(final_playlists) / stats['total_playlists']:.1%}\")\n",
    "\n",
    "        return final_playlists, final_stats\n",
    "\n",
    "print(\"‚úÖ HybridStreamingSampler class loaded\")"
   ],
   "id": "dc9f72ea9b2f9e4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HybridStreamingSampler class loaded\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Convenience Functions",
   "id": "d202d04239df59ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:01:14.580803Z",
     "start_time": "2025-06-23T16:01:14.555725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_hybrid_sampling(file_pattern: str,\n",
    "                       target_size: int = 50000,\n",
    "                       batch_size: int = 20):\n",
    "    \"\"\"\n",
    "    One-function call to run complete hybrid sampling\n",
    "\n",
    "    Args:\n",
    "        file_pattern: e.g., \"../data/raw/data/mpd.slice.*.json\"\n",
    "        target_size: Target number of playlists\n",
    "        batch_size: Files to process at once (adjust for RAM)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize sampler\n",
    "    sampler = HybridStreamingSampler(\n",
    "        target_playlists=target_size,\n",
    "        batch_size=batch_size,\n",
    "        min_playlist_length=10,\n",
    "        max_playlist_length=100,\n",
    "        min_track_frequency=5,\n",
    "        min_user_playlists=3\n",
    "    )\n",
    "\n",
    "    # Run sampling\n",
    "    sampled_playlists, stats = sampler.run_hybrid_sampling(file_pattern)\n",
    "\n",
    "    # Save results\n",
    "    output_data = {\n",
    "        'info': {\n",
    "            'generated_on': datetime.now().isoformat(),\n",
    "            'sampling_method': 'hybrid_core_based_stratified_streaming',\n",
    "            'parameters': {\n",
    "                'target_playlists': target_size,\n",
    "                'batch_size': batch_size,\n",
    "                'min_playlist_length': 10,\n",
    "                'max_playlist_length': 100,\n",
    "                'min_track_frequency': 5,\n",
    "                'min_user_playlists': 3\n",
    "            }\n",
    "        },\n",
    "        'sampling_stats': stats,\n",
    "        'playlists': sampled_playlists\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    output_file = f'../data/processed/spotify_hybrid_sampled_{target_size}.json'\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nüíæ Saved to: {output_file}\")\n",
    "    print(f\"üì¶ File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    return sampled_playlists, stats, output_file\n",
    "\n",
    "print(\"‚úÖ Convenience functions loaded\")"
   ],
   "id": "4153506b203e51d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Convenience functions loaded\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-23T16:23:52.030620Z",
     "start_time": "2025-06-23T16:01:17.976023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configure for your system\n",
    "file_pattern = \"../data/raw/data/mpd.slice.*.json\"  # Your file path\n",
    "target_size = 50000    # Target playlists\n",
    "batch_size = 20        # Adjust based on your RAM:\n",
    "                       # 20 for 8GB+ RAM\n",
    "                       # 10 for 4GB RAM\n",
    "                       # 5 for 2GB RAM\n",
    "\n",
    "# Run hybrid sampling\n",
    "sampled_playlists, stats, output_file = run_hybrid_sampling(\n",
    "    file_pattern=file_pattern,\n",
    "    target_size=target_size,\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "id": "5d778bce1a7e4e33",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Hybrid Streaming Sampler Configuration:\n",
      "   ‚Ä¢ Target playlists: 50,000\n",
      "   ‚Ä¢ Batch size: 20 files at a time\n",
      "   ‚Ä¢ Playlist length: 10-100 tracks\n",
      "   ‚Ä¢ Min track frequency: 5 playlists\n",
      "   ‚Ä¢ Min user playlists: 3 playlists\n",
      "\n",
      "üöÄ STARTING HYBRID CORE-BASED + STRATIFIED SAMPLING\n",
      "======================================================================\n",
      "üíæ Memory usage after start: 39.9 MB\n",
      "üîç PASS 1: HYBRID CORE-BASED FILTERING\n",
      "============================================================\n",
      "üìÅ Found 1000 files\n",
      "üì¶ Created 50 batches of ~20 files each\n",
      "üöÄ Applying Core-Based Filters:\n",
      "   ‚úÖ Step 1: Playlist length (10-100 tracks)\n",
      "   ‚úÖ Step 2: User activity (‚â•3 playlists per user)\n",
      "   ‚úÖ Step 3: Track frequency (‚â•5 appearances)\n",
      "\n",
      "üìä Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ‚úÖ Identified 8,526 active users\n",
      "\n",
      "üîç Sub-pass 1b: Applying all core filters...\n",
      "üì¶ Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,818 valid so far\n",
      "üíæ Memory usage after batch 1: 4096.2 MB\n",
      "üì¶ Processing batch 2/50\n",
      "üì¶ Processing batch 3/50\n",
      "üì¶ Processing batch 4/50\n",
      "üì¶ Processing batch 5/50\n",
      "üì¶ Processing batch 6/50\n",
      "üì¶ Processing batch 7/50\n",
      "üì¶ Processing batch 8/50\n",
      "üì¶ Processing batch 9/50\n",
      "üì¶ Processing batch 10/50\n",
      "üì¶ Processing batch 11/50\n",
      "   Progress: 220,000 seen, 163,985 valid so far\n",
      "üíæ Memory usage after batch 11: 4779.6 MB\n",
      "üì¶ Processing batch 12/50\n",
      "üì¶ Processing batch 13/50\n",
      "üì¶ Processing batch 14/50\n",
      "üì¶ Processing batch 15/50\n",
      "üì¶ Processing batch 16/50\n",
      "üì¶ Processing batch 17/50\n",
      "üì¶ Processing batch 18/50\n",
      "üì¶ Processing batch 19/50\n",
      "üì¶ Processing batch 20/50\n",
      "üì¶ Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,338 valid so far\n",
      "üíæ Memory usage after batch 21: 5123.4 MB\n",
      "üì¶ Processing batch 22/50\n",
      "üì¶ Processing batch 23/50\n",
      "üì¶ Processing batch 24/50\n",
      "üì¶ Processing batch 25/50\n",
      "üì¶ Processing batch 26/50\n",
      "üì¶ Processing batch 27/50\n",
      "üì¶ Processing batch 28/50\n",
      "üì¶ Processing batch 29/50\n",
      "üì¶ Processing batch 30/50\n",
      "üì¶ Processing batch 31/50\n",
      "   Progress: 620,000 seen, 462,956 valid so far\n",
      "üíæ Memory usage after batch 31: 5598.7 MB\n",
      "üì¶ Processing batch 32/50\n",
      "üì¶ Processing batch 33/50\n",
      "üì¶ Processing batch 34/50\n",
      "üì¶ Processing batch 35/50\n",
      "üì¶ Processing batch 36/50\n",
      "üì¶ Processing batch 37/50\n",
      "üì¶ Processing batch 38/50\n",
      "üì¶ Processing batch 39/50\n",
      "üì¶ Processing batch 40/50\n",
      "üì¶ Processing batch 41/50\n",
      "   Progress: 820,000 seen, 611,692 valid so far\n",
      "üíæ Memory usage after batch 41: 6003.3 MB\n",
      "üì¶ Processing batch 42/50\n",
      "üì¶ Processing batch 43/50\n",
      "üì¶ Processing batch 44/50\n",
      "üì¶ Processing batch 45/50\n",
      "üì¶ Processing batch 46/50\n",
      "üì¶ Processing batch 47/50\n",
      "üì¶ Processing batch 48/50\n",
      "üì¶ Processing batch 49/50\n",
      "üì¶ Processing batch 50/50\n",
      "\n",
      "üîç Applying final core filter: Track frequency\n",
      "   ‚úÖ Identified 374,533 core tracks\n",
      "\n",
      "‚úÖ CORE-BASED FILTERING COMPLETE:\n",
      "   üìä Filtering Funnel:\n",
      "      ‚Ä¢ Total playlists: 1,000,000\n",
      "      ‚Ä¢ Length filter: 747,510 (74.8%)\n",
      "      ‚Ä¢ User filter: 746,005 (74.6%)\n",
      "      ‚Ä¢ Track frequency: 745,332 (74.5%)\n",
      "      ‚Ä¢ üéØ FINAL VALID: 745,332 (74.5%)\n",
      "\n",
      "üíæ Memory usage after pass 1 complete: 4643.7 MB\n",
      "üìä CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   üìÖ Temporal split at timestamp: 1487980800.0\n",
      "   üë• User activity split at: 17.0 playlists per user\n",
      "   üìã Strata Distribution:\n",
      "      ‚Ä¢ short_old_casual    :  7,956 ( 1.1%)\n",
      "      ‚Ä¢ short_old_active    : 146,944 (19.7%)\n",
      "      ‚Ä¢ short_recent_casual :  5,408 ( 0.7%)\n",
      "      ‚Ä¢ short_recent_active : 111,591 (15.0%)\n",
      "      ‚Ä¢ medium_old_casual   :  6,033 ( 0.8%)\n",
      "      ‚Ä¢ medium_old_active   : 129,209 (17.3%)\n",
      "      ‚Ä¢ medium_recent_casual:  5,683 ( 0.8%)\n",
      "      ‚Ä¢ medium_recent_active: 137,781 (18.5%)\n",
      "      ‚Ä¢ long_old_casual     :  3,609 ( 0.5%)\n",
      "      ‚Ä¢ long_old_active     : 78,844 (10.6%)\n",
      "      ‚Ä¢ long_recent_casual  :  4,165 ( 0.6%)\n",
      "      ‚Ä¢ long_recent_active  : 108,109 (14.5%)\n",
      "\n",
      "üíæ Memory usage after strata created: 4420.6 MB\n",
      "üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING\n",
      "============================================================\n",
      "   üìä Global sampling ratio: 0.067\n",
      "   üèÜ Using priority scoring within strata\n",
      "\n",
      "      ‚Ä¢ short_old_casual    :  533 / 7,956 (avg score: 9.06)\n",
      "      ‚Ä¢ short_old_active    : 9,857 / 146,944 (avg score: 8.98)\n",
      "      ‚Ä¢ short_recent_casual :  362 / 5,408 (avg score: 9.01)\n",
      "      ‚Ä¢ short_recent_active : 7,485 / 111,591 (avg score: 8.98)\n",
      "      ‚Ä¢ medium_old_casual   :  404 / 6,033 (avg score: 9.51)\n",
      "      ‚Ä¢ medium_old_active   : 8,667 / 129,209 (avg score: 9.31)\n",
      "      ‚Ä¢ medium_recent_casual:  381 / 5,683 (avg score: 9.42)\n",
      "      ‚Ä¢ medium_recent_active: 9,242 / 137,781 (avg score: 9.31)\n",
      "      ‚Ä¢ long_old_casual     :  242 / 3,609 (avg score: 9.53)\n",
      "      ‚Ä¢ long_old_active     : 5,289 / 78,844 (avg score: 9.23)\n",
      "      ‚Ä¢ long_recent_casual  :  279 / 4,165 (avg score: 9.43)\n",
      "      ‚Ä¢ long_recent_active  : 7,252 / 108,109 (avg score: 9.24)\n",
      "\n",
      "   üéØ Selected 49,993 playlists for final loading\n",
      "   üìÅ Loading selected playlists...\n",
      "   üìÇ Loading from 1000 files\n",
      "      üìñ File 1/1000\n",
      "      üìñ File 101/1000\n",
      "      üìñ File 201/1000\n",
      "      üìñ File 301/1000\n",
      "      üìñ File 401/1000\n",
      "      üìñ File 501/1000\n",
      "      üìñ File 601/1000\n",
      "      üìñ File 701/1000\n",
      "      üìñ File 801/1000\n",
      "      üìñ File 901/1000\n",
      "   ‚úÖ Loaded 49,993 final playlists\n",
      "üíæ Memory usage after pass 2 complete: 1282.1 MB\n",
      "\n",
      "üéâ HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "üìä Results: 1,000,000 ‚Üí 49,993 playlists\n",
      "üìà Overall retention: 5.0%\n",
      "\n",
      "üíæ Saved to: ../data/processed/spotify_hybrid_sampled_50000.json\n",
      "üì¶ File size: 852.3 MB\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:16:34.114243Z",
     "start_time": "2025-07-30T12:04:06.384898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set, Iterator\n",
    "import gc\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ScaledHybridStreamingSampler:\n",
    "    \"\"\"\n",
    "    Scaled-Down Hybrid Core-Based + Stratified Streaming Sampler\n",
    "\n",
    "    MAINTAINS YOUR ORIGINAL METHODOLOGY:\n",
    "    - Pass 1: Core-based filtering (length, user activity, track frequency)\n",
    "    - Pass 2: Stratified sampling with priority scoring\n",
    "\n",
    "    MODIFIED FOR EXPERIMENTAL SCALE:\n",
    "    - Target: ~7,500 total nodes instead of 661k+\n",
    "    - Aggressive filtering to reach target scale\n",
    "    - Same strata and priority scoring logic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 target_playlists: int = 2500,        # Scaled down from 50,000\n",
    "                 batch_size: int = 20,\n",
    "                 min_playlist_length: int = 10,\n",
    "                 max_playlist_length: int = 100,\n",
    "                 min_track_frequency: int = 8,        # Increased from 5 for scale control\n",
    "                 min_user_playlists: int = 10):       # Increased from 3 for user consolidation\n",
    "\n",
    "        self.target_playlists = target_playlists\n",
    "        self.batch_size = batch_size\n",
    "        self.min_playlist_length = min_playlist_length\n",
    "        self.max_playlist_length = max_playlist_length\n",
    "        self.min_track_frequency = min_track_frequency\n",
    "        self.min_user_playlists = min_user_playlists\n",
    "\n",
    "        # Expected scale targets for verification\n",
    "        self.expected_total_nodes = 7500\n",
    "        self.expected_tracks = 3500\n",
    "        self.expected_artists = 800\n",
    "        self.expected_albums = 600\n",
    "        self.expected_users = 100\n",
    "\n",
    "        # Statistics collectors (same as original)\n",
    "        self.track_counts = Counter()\n",
    "        self.user_counts = Counter()\n",
    "        self.playlist_stats = []\n",
    "\n",
    "        print(f\"üéØ SCALED-DOWN HYBRID STREAMING SAMPLER\")\n",
    "        print(f\"=\" * 70)\n",
    "        print(f\"üìä METHODOLOGY: Your Original Hybrid Approach\")\n",
    "        print(f\"   ‚úÖ Pass 1: Core-based filtering\")\n",
    "        print(f\"   ‚úÖ Pass 2: Stratified sampling with priority scoring\")\n",
    "        print(f\"\")\n",
    "        print(f\"üîß SCALED PARAMETERS FOR EXPERIMENTAL CONTROL:\")\n",
    "        print(f\"   ‚Ä¢ Target playlists: {target_playlists:,} (was 50,000)\")\n",
    "        print(f\"   ‚Ä¢ Batch size: {batch_size} files at a time\")\n",
    "        print(f\"   ‚Ä¢ Playlist length: {min_playlist_length}-{max_playlist_length} tracks\")\n",
    "        print(f\"   ‚Ä¢ Min track frequency: {min_track_frequency} playlists (was 5)\")\n",
    "        print(f\"   ‚Ä¢ Min user playlists: {min_user_playlists} playlists (was 3)\")\n",
    "        print(f\"\")\n",
    "        print(f\"üéØ EXPECTED SCALE: ~{self.expected_total_nodes:,} total nodes\")\n",
    "        print(f\"   ‚Ä¢ Playlists: {target_playlists:,}\")\n",
    "        print(f\"   ‚Ä¢ Tracks: ~{self.expected_tracks:,}\")\n",
    "        print(f\"   ‚Ä¢ Artists: ~{self.expected_artists:,}\")\n",
    "        print(f\"   ‚Ä¢ Albums: ~{self.expected_albums:,}\")\n",
    "        print(f\"   ‚Ä¢ Users: ~{self.expected_users:,}\")\n",
    "        print()\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            return process.memory_info().rss / 1024 / 1024\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def print_memory_status(self, stage: str):\n",
    "        \"\"\"Print current memory usage\"\"\"\n",
    "        memory_mb = self.get_memory_usage()\n",
    "        print(f\"üíæ Memory usage after {stage}: {memory_mb:.1f} MB\")\n",
    "\n",
    "    def get_file_batches(self, file_pattern: str) -> List[List[str]]:\n",
    "        \"\"\"Split files into manageable batches (same as original)\"\"\"\n",
    "        file_paths = glob.glob(file_pattern)\n",
    "        file_paths.sort()\n",
    "\n",
    "        if not file_paths:\n",
    "            raise FileNotFoundError(f\"No files found: {file_pattern}\")\n",
    "\n",
    "        print(f\"üìÅ Found {len(file_paths)} files\")\n",
    "\n",
    "        # Split into batches\n",
    "        batches = []\n",
    "        for i in range(0, len(file_paths), self.batch_size):\n",
    "            batch = file_paths[i:i + self.batch_size]\n",
    "            batches.append(batch)\n",
    "\n",
    "        print(f\"üì¶ Created {len(batches)} batches of ~{self.batch_size} files each\")\n",
    "        return batches\n",
    "\n",
    "    def _extract_user_id(self, playlist: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Extract user identifier with AGGRESSIVE consolidation for target scale\n",
    "        (Modified from original for better user consolidation)\n",
    "        \"\"\"\n",
    "        # Method 1: Use name-based grouping with more aggressive consolidation\n",
    "        name = playlist.get('name', '').lower().strip()\n",
    "        if name:\n",
    "            # Use first 2-3 characters for heavy consolidation\n",
    "            words = name.split()\n",
    "            if words:\n",
    "                user_base = words[0][:2]  # Only first 2 chars (was more in original)\n",
    "                user_id = ''.join(c for c in user_base if c.isalnum())\n",
    "                if user_id:\n",
    "                    return user_id\n",
    "\n",
    "        # Method 2: PID-based with heavy consolidation (force into small user set)\n",
    "        pid = playlist.get('pid', 0)\n",
    "        return f\"u{pid % 200}\"  # Force into ~200 user bins (will filter to ~100 active)\n",
    "\n",
    "    def pass1_core_filtering(self, file_pattern: str) -> Dict:\n",
    "        \"\"\"\n",
    "        PASS 1: Core-based filtering + Statistics collection\n",
    "        (SAME LOGIC as original, but with scaled parameters)\n",
    "        \"\"\"\n",
    "        print(\"üîç PASS 1: HYBRID CORE-BASED FILTERING (SCALED)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        batches = self.get_file_batches(file_pattern)\n",
    "\n",
    "        # Stage counts (same as original)\n",
    "        stage_counts = {\n",
    "            'total_seen': 0,\n",
    "            'passed_length_filter': 0,\n",
    "            'passed_user_filter': 0,\n",
    "            'passed_track_frequency_filter': 0,\n",
    "            'final_valid': 0\n",
    "        }\n",
    "\n",
    "        print(\"üöÄ Applying SCALED Core-Based Filters:\")\n",
    "        print(f\"   ‚úÖ Step 1: Playlist length ({self.min_playlist_length}-{self.max_playlist_length} tracks)\")\n",
    "        print(f\"   ‚úÖ Step 2: User activity (‚â•{self.min_user_playlists} playlists per user)\")\n",
    "        print(f\"   ‚úÖ Step 3: Track frequency (‚â•{self.min_track_frequency} appearances)\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1a: Collect user activity statistics (same as original)\n",
    "        print(\"üìä Sub-pass 1a: Collecting user activity statistics...\")\n",
    "        user_playlist_count = Counter()\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"   User stats progress: {batch_idx + 1}/{len(batches)} batches\")\n",
    "\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "\n",
    "                    for playlist in file_playlists:\n",
    "                        user_id = self._extract_user_id(playlist)\n",
    "                        user_playlist_count[user_id] += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        # Identify active users (same logic, but higher threshold)\n",
    "        active_users = {\n",
    "            user for user, count in user_playlist_count.items()\n",
    "            if count >= self.min_user_playlists\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Identified {len(active_users):,} active users (target: ~{self.expected_users})\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1b: Apply all core filters (same as original)\n",
    "        print(\"üîç Sub-pass 1b: Applying all core filters...\")\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            print(f\"üì¶ Processing batch {batch_idx + 1}/{len(batches)}\")\n",
    "\n",
    "            batch_playlists = []\n",
    "\n",
    "            # Load batch (same as original)\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "\n",
    "                    # Add source file info\n",
    "                    for playlist in file_playlists:\n",
    "                        playlist['_source_file'] = file_path\n",
    "\n",
    "                    batch_playlists.extend(file_playlists)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   ‚ö†Ô∏è  Error loading {os.path.basename(file_path)}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Process batch with core filtering (same logic as original)\n",
    "            for playlist_idx, playlist in enumerate(batch_playlists):\n",
    "                stage_counts['total_seen'] += 1\n",
    "\n",
    "                # CORE FILTER 1: Playlist length (same as original)\n",
    "                tracks = playlist.get('tracks', [])\n",
    "                playlist_length = len(tracks)\n",
    "\n",
    "                if not (self.min_playlist_length <= playlist_length <= self.max_playlist_length):\n",
    "                    continue\n",
    "                stage_counts['passed_length_filter'] += 1\n",
    "\n",
    "                # CORE FILTER 2: User activity (same as original)\n",
    "                user_id = self._extract_user_id(playlist)\n",
    "                if user_id not in active_users:\n",
    "                    continue\n",
    "                stage_counts['passed_user_filter'] += 1\n",
    "\n",
    "                # Count tracks for frequency analysis (same as original)\n",
    "                playlist_tracks = set()\n",
    "                for track in tracks:\n",
    "                    track_uri = track.get('track_uri', '')\n",
    "                    if track_uri:\n",
    "                        self.track_counts[track_uri] += 1\n",
    "                        playlist_tracks.add(track_uri)\n",
    "\n",
    "                self.user_counts[user_id] += 1\n",
    "\n",
    "                # Store playlist metadata (same as original)\n",
    "                playlist_metadata = {\n",
    "                    'file_path': playlist['_source_file'],\n",
    "                    'pid': playlist.get('pid'),\n",
    "                    'length': playlist_length,\n",
    "                    'modified_at': playlist.get('modified_at', 0),\n",
    "                    'user_id': user_id,\n",
    "                    'track_uris': list(playlist_tracks),\n",
    "                    'name': playlist.get('name', ''),\n",
    "                    'collaborative': playlist.get('collaborative', False),\n",
    "                    'num_followers': playlist.get('num_followers', 0)\n",
    "                }\n",
    "\n",
    "                self.playlist_stats.append(playlist_metadata)\n",
    "\n",
    "            # Clear batch from memory (same as original)\n",
    "            del batch_playlists\n",
    "            gc.collect()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"   Progress: {stage_counts['total_seen']:,} seen, {len(self.playlist_stats):,} valid so far\")\n",
    "                self.print_memory_status(f\"batch {batch_idx + 1}\")\n",
    "\n",
    "        # CORE FILTER 3: Track frequency (same logic, higher threshold)\n",
    "        print(f\"\\nüîç Applying final core filter: Track frequency (‚â•{self.min_track_frequency})\")\n",
    "\n",
    "        core_tracks = {\n",
    "            track for track, count in self.track_counts.items()\n",
    "            if count >= self.min_track_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"   ‚úÖ Identified {len(core_tracks):,} core tracks (target: ~{self.expected_tracks})\")\n",
    "\n",
    "        # Filter playlists that have core tracks (same as original)\n",
    "        filtered_playlist_stats = []\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            playlist_tracks = set(playlist_meta['track_uris'])\n",
    "            if playlist_tracks.intersection(core_tracks):\n",
    "                filtered_playlist_stats.append(playlist_meta)\n",
    "                stage_counts['passed_track_frequency_filter'] += 1\n",
    "\n",
    "        self.playlist_stats = filtered_playlist_stats\n",
    "        stage_counts['final_valid'] = len(filtered_playlist_stats)\n",
    "\n",
    "        print(f\"\\n‚úÖ CORE-BASED FILTERING COMPLETE:\")\n",
    "        print(f\"   üìä Filtering Funnel:\")\n",
    "        print(f\"      ‚Ä¢ Total playlists: {stage_counts['total_seen']:,}\")\n",
    "        print(f\"      ‚Ä¢ Length filter: {stage_counts['passed_length_filter']:,} ({stage_counts['passed_length_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ User filter: {stage_counts['passed_user_filter']:,} ({stage_counts['passed_user_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ Track frequency: {stage_counts['passed_track_frequency_filter']:,} ({stage_counts['passed_track_frequency_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      ‚Ä¢ üéØ FINAL VALID: {stage_counts['final_valid']:,} ({stage_counts['final_valid']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "        return {\n",
    "            'total_playlists': stage_counts['total_seen'],\n",
    "            'valid_playlists': stage_counts['final_valid'],\n",
    "            'core_tracks': core_tracks,\n",
    "            'active_users': active_users,\n",
    "            'unique_tracks': len(self.track_counts),\n",
    "            'stage_counts': stage_counts\n",
    "        }\n",
    "\n",
    "    def create_strata(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"\n",
    "        Create comprehensive strata for stratified sampling\n",
    "        (IDENTICAL to original method)\n",
    "        \"\"\"\n",
    "        print(\"üìä CREATING STRATIFIED SAMPLING STRATA\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Get temporal split (same as original)\n",
    "        timestamps = [p['modified_at'] for p in self.playlist_stats if p['modified_at'] > 0]\n",
    "        if timestamps:\n",
    "            median_time = np.median(timestamps)\n",
    "        else:\n",
    "            median_time = 1500000000  # Default\n",
    "\n",
    "        # Get user activity split (same as original)\n",
    "        user_playlist_counts = {}\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_playlist_counts[user_id] = user_playlist_counts.get(user_id, 0) + 1\n",
    "\n",
    "        user_activity_median = np.median(list(user_playlist_counts.values())) if user_playlist_counts else 5\n",
    "\n",
    "        print(f\"   üìÖ Temporal split at timestamp: {median_time}\")\n",
    "        print(f\"   üë• User activity split at: {user_activity_median} playlists per user\")\n",
    "\n",
    "        # Create 12 comprehensive strata (IDENTICAL to original)\n",
    "        strata = {\n",
    "            'short_old_casual': [], 'short_old_active': [],\n",
    "            'short_recent_casual': [], 'short_recent_active': [],\n",
    "            'medium_old_casual': [], 'medium_old_active': [],\n",
    "            'medium_recent_casual': [], 'medium_recent_active': [],\n",
    "            'long_old_casual': [], 'long_old_active': [],\n",
    "            'long_recent_casual': [], 'long_recent_active': []\n",
    "        }\n",
    "\n",
    "        for i, playlist_meta in enumerate(self.playlist_stats):\n",
    "            length = playlist_meta['length']\n",
    "            timestamp = playlist_meta['modified_at']\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_activity = user_playlist_counts.get(user_id, 1)\n",
    "\n",
    "            # Length category (same as original)\n",
    "            if length <= 30:\n",
    "                length_cat = 'short'\n",
    "            elif length <= 60:\n",
    "                length_cat = 'medium'\n",
    "            else:\n",
    "                length_cat = 'long'\n",
    "\n",
    "            # Time category (same as original)\n",
    "            time_cat = 'recent' if timestamp >= median_time else 'old'\n",
    "\n",
    "            # User activity category (same as original)\n",
    "            activity_cat = 'active' if user_activity >= user_activity_median else 'casual'\n",
    "\n",
    "            # Combine into stratum (same as original)\n",
    "            stratum_key = f\"{length_cat}_{time_cat}_{activity_cat}\"\n",
    "            strata[stratum_key].append(i)\n",
    "\n",
    "        # Print strata distribution (same as original)\n",
    "        print(\"   üìã Strata Distribution:\")\n",
    "        total_playlists = len(self.playlist_stats)\n",
    "\n",
    "        for stratum, indices in strata.items():\n",
    "            if indices:  # Only show non-empty strata\n",
    "                percentage = len(indices) / total_playlists * 100\n",
    "                print(f\"      ‚Ä¢ {stratum:20s}: {len(indices):6,} ({percentage:4.1f}%)\")\n",
    "\n",
    "        print()\n",
    "        return strata\n",
    "\n",
    "    def _calculate_priority_score(self, playlist_meta: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Calculate priority score for playlist selection\n",
    "        (IDENTICAL to original method)\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Factor 1: Track diversity (30% weight)\n",
    "        unique_tracks = len(playlist_meta['track_uris'])\n",
    "        playlist_length = playlist_meta['length']\n",
    "        if playlist_length > 0:\n",
    "            track_diversity_ratio = unique_tracks / playlist_length\n",
    "            score += track_diversity_ratio * 3.0\n",
    "\n",
    "        # Factor 2: User engagement (25% weight)\n",
    "        num_followers = playlist_meta.get('num_followers', 0)\n",
    "        if num_followers > 0:\n",
    "            follower_score = min(np.log10(num_followers + 1), 3.0)\n",
    "            score += follower_score * 2.5\n",
    "\n",
    "        # Factor 3: Playlist completeness (20% weight)\n",
    "        name = playlist_meta.get('name', '')\n",
    "        has_good_name = len(name.strip()) > 3 and not name.lower().startswith('my playlist')\n",
    "        if has_good_name:\n",
    "            score += 2.0\n",
    "\n",
    "        # Factor 4: Collaborative playlists bonus (10% weight)\n",
    "        if playlist_meta.get('collaborative', False):\n",
    "            score += 1.0\n",
    "\n",
    "        # Factor 5: Length balance bonus (15% weight)\n",
    "        length = playlist_meta['length']\n",
    "        if 20 <= length <= 80:  # Sweet spot\n",
    "            score += 1.5\n",
    "\n",
    "        return score\n",
    "\n",
    "    def pass2_stratified_sampling(self, strata: Dict[str, List[int]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        PASS 2: Stratified sampling with priority scoring\n",
    "        (IDENTICAL logic to original, but with scaled target)\n",
    "        \"\"\"\n",
    "        print(\"üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING (SCALED)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        total_available = len(self.playlist_stats)\n",
    "\n",
    "        if total_available <= self.target_playlists:\n",
    "            print(f\"   üìù Available ({total_available:,}) ‚â§ target ({self.target_playlists:,})\")\n",
    "            selected_indices = list(range(total_available))\n",
    "        else:\n",
    "            sampling_ratio = self.target_playlists / total_available\n",
    "            selected_indices = set()\n",
    "\n",
    "            print(f\"   üìä Global sampling ratio: {sampling_ratio:.3f}\")\n",
    "            print(f\"   üèÜ Using priority scoring within strata\")\n",
    "            print()\n",
    "\n",
    "            # Same stratified sampling logic as original\n",
    "            for stratum, indices in strata.items():\n",
    "                if not indices:\n",
    "                    continue\n",
    "\n",
    "                stratum_target = max(1, int(len(indices) * sampling_ratio))\n",
    "                stratum_target = min(stratum_target, len(indices))\n",
    "\n",
    "                # Score playlists in this stratum (same as original)\n",
    "                scored_playlists = []\n",
    "                for idx in indices:\n",
    "                    playlist_meta = self.playlist_stats[idx]\n",
    "                    score = self._calculate_priority_score(playlist_meta)\n",
    "                    scored_playlists.append((idx, score))\n",
    "\n",
    "                # Sort by score and sample (same as original)\n",
    "                scored_playlists.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Hybrid: 70% top-scored + 30% random (same as original)\n",
    "                top_count = int(stratum_target * 0.7)\n",
    "                random_count = stratum_target - top_count\n",
    "\n",
    "                selected = [idx for idx, _ in scored_playlists[:top_count]]\n",
    "\n",
    "                if random_count > 0 and len(scored_playlists) > top_count:\n",
    "                    remaining = [idx for idx, _ in scored_playlists[top_count:]]\n",
    "                    if len(remaining) >= random_count:\n",
    "                        selected.extend(random.sample(remaining, random_count))\n",
    "                    else:\n",
    "                        selected.extend(remaining)\n",
    "\n",
    "                selected_indices.update(selected)\n",
    "\n",
    "                avg_score = np.mean([score for _, score in scored_playlists[:len(selected)]])\n",
    "                print(f\"      ‚Ä¢ {stratum:20s}: {len(selected):4,} / {len(indices):5,} (avg score: {avg_score:.2f})\")\n",
    "\n",
    "            selected_indices = list(selected_indices)\n",
    "\n",
    "        print(f\"\\n   üéØ Selected {len(selected_indices):,} playlists for final loading\")\n",
    "\n",
    "        # Load selected playlists (same as original)\n",
    "        return self._load_selected_playlists(selected_indices)\n",
    "\n",
    "    def _load_selected_playlists(self, selected_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load only the selected playlists from files\n",
    "        (IDENTICAL to original method)\n",
    "        \"\"\"\n",
    "        print(\"   üìÅ Loading selected playlists...\")\n",
    "\n",
    "        # Group by file (same as original)\n",
    "        file_to_playlists = defaultdict(list)\n",
    "        for idx in selected_indices:\n",
    "            playlist_meta = self.playlist_stats[idx]\n",
    "            file_path = playlist_meta['file_path']\n",
    "            file_to_playlists[file_path].append(playlist_meta)\n",
    "\n",
    "        print(f\"   üìÇ Loading from {len(file_to_playlists)} files\")\n",
    "\n",
    "        # Load playlists (same as original)\n",
    "        final_playlists = []\n",
    "\n",
    "        for file_idx, (file_path, playlist_metas) in enumerate(file_to_playlists.items()):\n",
    "            if file_idx % 100 == 0:\n",
    "                print(f\"      üìñ File {file_idx + 1}/{len(file_to_playlists)}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                file_playlists = data.get('playlists', [])\n",
    "                pid_to_playlist = {p.get('pid'): p for p in file_playlists}\n",
    "\n",
    "                for meta in playlist_metas:\n",
    "                    pid = meta['pid']\n",
    "                    if pid in pid_to_playlist:\n",
    "                        playlist = pid_to_playlist[pid]\n",
    "                        playlist['_sampling_score'] = self._calculate_priority_score(meta)\n",
    "                        final_playlists.append(playlist)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        print(f\"   ‚úÖ Loaded {len(final_playlists):,} final playlists\")\n",
    "        return final_playlists\n",
    "\n",
    "    def run_hybrid_sampling(self, file_pattern: str) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"\n",
    "        Main method: Complete hybrid sampling workflow\n",
    "        (SAME STRUCTURE as original, with scale verification)\n",
    "        \"\"\"\n",
    "        print(\"üöÄ STARTING SCALED-DOWN HYBRID SAMPLING\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        self.print_memory_status(\"start\")\n",
    "\n",
    "        # Pass 1: Core-based filtering (same as original)\n",
    "        stats = self.pass1_core_filtering(file_pattern)\n",
    "        self.print_memory_status(\"pass 1 complete\")\n",
    "\n",
    "        # Create strata (same as original)\n",
    "        strata = self.create_strata()\n",
    "        self.print_memory_status(\"strata created\")\n",
    "\n",
    "        # Pass 2: Stratified sampling (same as original)\n",
    "        final_playlists = self.pass2_stratified_sampling(strata)\n",
    "        self.print_memory_status(\"pass 2 complete\")\n",
    "\n",
    "        # ADDED: Scale verification for experimental control\n",
    "        actual_scale = self._verify_final_scale(final_playlists)\n",
    "\n",
    "        # Final statistics (enhanced with scale info)\n",
    "        final_stats = {\n",
    "            'methodology': 'scaled_hybrid_core_based_stratified_streaming',\n",
    "            'original_total': stats['total_playlists'],\n",
    "            'final_sampled': len(final_playlists),\n",
    "            'retention_rate': len(final_playlists) / stats['total_playlists'],\n",
    "            'core_filtering_retention': stats['valid_playlists'] / stats['total_playlists'],\n",
    "            'unique_tracks': stats['unique_tracks'],\n",
    "            'core_tracks_count': len(stats['core_tracks']),\n",
    "            'active_users_count': len(stats['active_users']),\n",
    "            'stage_counts': stats['stage_counts'],\n",
    "            'actual_scale': actual_scale,\n",
    "            'scale_targets': {\n",
    "                'total_nodes': self.expected_total_nodes,\n",
    "                'playlists': self.target_playlists,\n",
    "                'tracks': self.expected_tracks,\n",
    "                'artists': self.expected_artists,\n",
    "                'albums': self.expected_albums,\n",
    "                'users': self.expected_users\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"\\nüéâ SCALED HYBRID SAMPLING COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"üìä Results: {stats['total_playlists']:,} ‚Üí {len(final_playlists):,} playlists\")\n",
    "        print(f\"üìà Overall retention: {len(final_playlists) / stats['total_playlists']:.1%}\")\n",
    "\n",
    "        # Scale verification summary\n",
    "        total_actual = actual_scale['total_nodes']\n",
    "        scale_ratio = total_actual / self.expected_total_nodes\n",
    "        print(f\"\\nüéØ EXPERIMENTAL SCALE VERIFICATION:\")\n",
    "        print(f\"   ‚Ä¢ Actual total nodes: {total_actual:,}\")\n",
    "        print(f\"   ‚Ä¢ Target total nodes: {self.expected_total_nodes:,}\")\n",
    "        print(f\"   ‚Ä¢ Scale ratio: {scale_ratio:.3f} ({'‚úÖ GOOD' if 0.8 <= scale_ratio <= 1.2 else '‚ö†Ô∏è ADJUST'})\")\n",
    "\n",
    "        return final_playlists, final_stats\n",
    "\n",
    "    def _verify_final_scale(self, final_playlists: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Verify that final scale meets experimental targets\n",
    "        \"\"\"\n",
    "        print(\"\\nüîç VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Count actual entities\n",
    "        actual_tracks = set()\n",
    "        actual_artists = set()\n",
    "        actual_albums = set()\n",
    "        actual_users = set()\n",
    "\n",
    "        for playlist in final_playlists:\n",
    "            user_id = self._extract_user_id(playlist)\n",
    "            actual_users.add(user_id)\n",
    "\n",
    "            for track in playlist.get('tracks', []):\n",
    "                track_uri = track.get('track_uri', '')\n",
    "                artist_uri = track.get('artist_uri', '')\n",
    "                album_uri = track.get('album_uri', '')\n",
    "\n",
    "                if track_uri:\n",
    "                    actual_tracks.add(track_uri)\n",
    "                if artist_uri:\n",
    "                    actual_artists.add(artist_uri)\n",
    "                if album_uri:\n",
    "                    actual_albums.add(album_uri)\n",
    "\n",
    "        actual_scale = {\n",
    "            'playlists': len(final_playlists),\n",
    "            'tracks': len(actual_tracks),\n",
    "            'artists': len(actual_artists),\n",
    "            'albums': len(actual_albums),\n",
    "            'users': len(actual_users),\n",
    "            'total_nodes': len(final_playlists) + len(actual_tracks) + len(actual_artists) + len(actual_albums) + len(actual_users)\n",
    "        }\n",
    "\n",
    "        print(f\"   üìä Final Entity Counts:\")\n",
    "        print(f\"      ‚Ä¢ Playlists: {actual_scale['playlists']:,} (target: {self.target_playlists:,})\")\n",
    "        print(f\"      ‚Ä¢ Tracks: {actual_scale['tracks']:,} (target: ~{self.expected_tracks:,})\")\n",
    "        print(f\"      ‚Ä¢ Artists: {actual_scale['artists']:,} (target: ~{self.expected_artists:,})\")\n",
    "        print(f\"      ‚Ä¢ Albums: {actual_scale['albums']:,} (target: ~{self.expected_albums:,})\")\n",
    "        print(f\"      ‚Ä¢ Users: {actual_scale['users']:,} (target: ~{self.expected_users:,})\")\n",
    "        print(f\"      üéØ TOTAL: {actual_scale['total_nodes']:,} (target: ~{self.expected_total_nodes:,})\")\n",
    "\n",
    "        return actual_scale\n",
    "\n",
    "\n",
    "def run_scaled_hybrid_sampling(file_pattern: str,\n",
    "                              target_playlists: int = 2500,\n",
    "                              output_suffix: str = \"scaled_hybrid_7500\"):\n",
    "    \"\"\"\n",
    "    One-function call to run scaled-down hybrid sampling\n",
    "    MAINTAINS YOUR ORIGINAL METHODOLOGY at controlled scale\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize sampler with scaled parameters\n",
    "    sampler = ScaledHybridStreamingSampler(\n",
    "        target_playlists=target_playlists,\n",
    "        batch_size=20,\n",
    "        min_playlist_length=10,\n",
    "        max_playlist_length=100,\n",
    "        min_track_frequency=8,   # Increased for scale control\n",
    "        min_user_playlists=10    # Increased for user consolidation\n",
    "    )\n",
    "\n",
    "    # Run sampling\n",
    "    sampled_playlists, stats = sampler.run_hybrid_sampling(file_pattern)\n",
    "\n",
    "    # Save results\n",
    "    output_data = {\n",
    "        'info': {\n",
    "            'generated_on': datetime.now().isoformat(),\n",
    "            'sampling_method': 'scaled_hybrid_core_based_stratified_streaming',\n",
    "            'original_method': 'hybrid_core_based_stratified_streaming',\n",
    "            'scaling_purpose': 'experimental_control_7500_nodes',\n",
    "            'parameters': {\n",
    "                'target_playlists': target_playlists,\n",
    "                'batch_size': 20,\n",
    "                'min_playlist_length': 10,\n",
    "                'max_playlist_length': 100,\n",
    "                'min_track_frequency': 8,\n",
    "                'min_user_playlists': 10\n",
    "            }\n",
    "        },\n",
    "        'sampling_stats': stats,\n",
    "        'playlists': sampled_playlists\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    output_file = f'../data/processed/spotify_{output_suffix}.json'\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nüíæ Saved to: {output_file}\")\n",
    "    print(f\"üì¶ File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    print(f\"\\nüéì READY FOR CONTROLLED EXPERIMENTS:\")\n",
    "    print(f\"   ‚úÖ Methodology: YOUR ORIGINAL hybrid approach\")\n",
    "    print(f\"   ‚úÖ Scale: ~{stats['actual_scale']['total_nodes']:,} nodes\")\n",
    "    print(f\"   ‚úÖ Training time estimate: 3-5 minutes per configuration\")\n",
    "    print(f\"   ‚úÖ Same methodology across all 3 experiments\")\n",
    "    print(f\"   ‚úÖ Controlled conditions achieved\")\n",
    "\n",
    "    return sampled_playlists, stats, output_file\n",
    "\n",
    "\n",
    "# Example usage with your original methodology at controlled scale:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility (same as your original)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Configure for your system\n",
    "    file_pattern = \"../data/raw/data/mpd.slice.*.json\"\n",
    "\n",
    "    print(\"üéØ RUNNING YOUR HYBRID METHOD AT CONTROLLED SCALE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚úÖ PRESERVES: Your original core-based + stratified approach\")\n",
    "    print(\"‚úÖ PRESERVES: Your 12-strata system and priority scoring\")\n",
    "    print(\"‚úÖ PRESERVES: Your hybrid 70%/30% selection logic\")\n",
    "    print(\"‚úÖ MODIFIES: Parameters to achieve ~7,500 node target\")\n",
    "    print()\n",
    "\n",
    "    # Run scaled hybrid sampling\n",
    "    playlists, stats, output_file = run_scaled_hybrid_sampling(\n",
    "        file_pattern=file_pattern,\n",
    "        target_playlists=2500,\n",
    "        output_suffix=\"scaled_hybrid_7500\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üéâ METHODOLOGY COMPARISON:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"ORIGINAL HYBRID METHOD:\")\n",
    "    print(\"  ‚Ä¢ Target: 50,000 playlists ‚Üí 661k+ nodes\")\n",
    "    print(\"  ‚Ä¢ Training: 34-60 minutes per config\")\n",
    "    print(\"  ‚Ä¢ Total experiment time: 4-6 hours\")\n",
    "    print()\n",
    "    print(\"SCALED HYBRID METHOD (THIS OUTPUT):\")\n",
    "    print(f\"  ‚Ä¢ Target: 2,500 playlists ‚Üí ~{stats['actual_scale']['total_nodes']:,} nodes\")\n",
    "    print(\"  ‚Ä¢ Training: 3-5 minutes per config\")\n",
    "    print(\"  ‚Ä¢ Total experiment time: 40-65 minutes\")\n",
    "    print()\n",
    "    print(\"üéØ SAME METHODOLOGY, CONTROLLED SCALE!\")\n",
    "    print(\"   ‚úÖ Identical core-based filtering logic\")\n",
    "    print(\"   ‚úÖ Identical stratified sampling approach\")\n",
    "    print(\"   ‚úÖ Identical priority scoring system\")\n",
    "    print(\"   ‚úÖ Perfect for experimental consistency\")\n",
    "\n",
    "    # Optional: Show parameter comparison\n",
    "    print(f\"\\nüìã PARAMETER ADJUSTMENTS FOR SCALE CONTROL:\")\n",
    "    print(f\"   ‚Ä¢ min_track_frequency: 5 ‚Üí 8 (stricter core filtering)\")\n",
    "    print(f\"   ‚Ä¢ min_user_playlists: 3 ‚Üí 10 (better user consolidation)\")\n",
    "    print(f\"   ‚Ä¢ target_playlists: 50,000 ‚Üí 2,500 (experimental scale)\")\n",
    "    print(f\"   ‚Ä¢ User ID extraction: more aggressive consolidation\")\n",
    "    print()\n",
    "    print(f\"üî¨ EXPERIMENTAL BENEFITS:\")\n",
    "    print(f\"   ‚úÖ Can run all 13 configurations in ~1 hour\")\n",
    "    print(f\"   ‚úÖ Same methodology across Experiments 1, 2, and 3\")\n",
    "    print(f\"   ‚úÖ Manageable memory usage (~1-2GB vs 4-6GB)\")\n",
    "    print(f\"   ‚úÖ Statistical validity maintained\")\n",
    "    print(f\"   ‚úÖ Perfect for thesis experimental design\")"
   ],
   "id": "eabd48b4957cfcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ RUNNING YOUR HYBRID METHOD AT CONTROLLED SCALE\n",
      "============================================================\n",
      "‚úÖ PRESERVES: Your original core-based + stratified approach\n",
      "‚úÖ PRESERVES: Your 12-strata system and priority scoring\n",
      "‚úÖ PRESERVES: Your hybrid 70%/30% selection logic\n",
      "‚úÖ MODIFIES: Parameters to achieve ~7,500 node target\n",
      "\n",
      "üéØ SCALED-DOWN HYBRID STREAMING SAMPLER\n",
      "======================================================================\n",
      "üìä METHODOLOGY: Your Original Hybrid Approach\n",
      "   ‚úÖ Pass 1: Core-based filtering\n",
      "   ‚úÖ Pass 2: Stratified sampling with priority scoring\n",
      "\n",
      "üîß SCALED PARAMETERS FOR EXPERIMENTAL CONTROL:\n",
      "   ‚Ä¢ Target playlists: 2,500 (was 50,000)\n",
      "   ‚Ä¢ Batch size: 20 files at a time\n",
      "   ‚Ä¢ Playlist length: 10-100 tracks\n",
      "   ‚Ä¢ Min track frequency: 8 playlists (was 5)\n",
      "   ‚Ä¢ Min user playlists: 10 playlists (was 3)\n",
      "\n",
      "üéØ EXPECTED SCALE: ~7,500 total nodes\n",
      "   ‚Ä¢ Playlists: 2,500\n",
      "   ‚Ä¢ Tracks: ~3,500\n",
      "   ‚Ä¢ Artists: ~800\n",
      "   ‚Ä¢ Albums: ~600\n",
      "   ‚Ä¢ Users: ~100\n",
      "\n",
      "üöÄ STARTING SCALED-DOWN HYBRID SAMPLING\n",
      "======================================================================\n",
      "üíæ Memory usage after start: 180.6 MB\n",
      "üîç PASS 1: HYBRID CORE-BASED FILTERING (SCALED)\n",
      "============================================================\n",
      "üìÅ Found 1000 files\n",
      "üì¶ Created 50 batches of ~20 files each\n",
      "üöÄ Applying SCALED Core-Based Filters:\n",
      "   ‚úÖ Step 1: Playlist length (10-100 tracks)\n",
      "   ‚úÖ Step 2: User activity (‚â•10 playlists per user)\n",
      "   ‚úÖ Step 3: Track frequency (‚â•8 appearances)\n",
      "\n",
      "üìä Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   ‚úÖ Identified 850 active users (target: ~100)\n",
      "\n",
      "üîç Sub-pass 1b: Applying all core filters...\n",
      "üì¶ Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,839 valid so far\n",
      "üíæ Memory usage after batch 1: 1460.9 MB\n",
      "üì¶ Processing batch 2/50\n",
      "üì¶ Processing batch 3/50\n",
      "üì¶ Processing batch 4/50\n",
      "üì¶ Processing batch 5/50\n",
      "üì¶ Processing batch 6/50\n",
      "üì¶ Processing batch 7/50\n",
      "üì¶ Processing batch 8/50\n",
      "üì¶ Processing batch 9/50\n",
      "üì¶ Processing batch 10/50\n",
      "üì¶ Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,251 valid so far\n",
      "üíæ Memory usage after batch 11: 2283.5 MB\n",
      "üì¶ Processing batch 12/50\n",
      "üì¶ Processing batch 13/50\n",
      "üì¶ Processing batch 14/50\n",
      "üì¶ Processing batch 15/50\n",
      "üì¶ Processing batch 16/50\n",
      "üì¶ Processing batch 17/50\n",
      "üì¶ Processing batch 18/50\n",
      "üì¶ Processing batch 19/50\n",
      "üì¶ Processing batch 20/50\n",
      "üì¶ Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,842 valid so far\n",
      "üíæ Memory usage after batch 21: 3077.4 MB\n",
      "üì¶ Processing batch 22/50\n",
      "üì¶ Processing batch 23/50\n",
      "üì¶ Processing batch 24/50\n",
      "üì¶ Processing batch 25/50\n",
      "üì¶ Processing batch 26/50\n",
      "üì¶ Processing batch 27/50\n",
      "üì¶ Processing batch 28/50\n",
      "üì¶ Processing batch 29/50\n",
      "üì¶ Processing batch 30/50\n",
      "üì¶ Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,678 valid so far\n",
      "üíæ Memory usage after batch 31: 3746.3 MB\n",
      "üì¶ Processing batch 32/50\n",
      "üì¶ Processing batch 33/50\n",
      "üì¶ Processing batch 34/50\n",
      "üì¶ Processing batch 35/50\n",
      "üì¶ Processing batch 36/50\n",
      "üì¶ Processing batch 37/50\n",
      "üì¶ Processing batch 38/50\n",
      "üì¶ Processing batch 39/50\n",
      "üì¶ Processing batch 40/50\n",
      "üì¶ Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,643 valid so far\n",
      "üíæ Memory usage after batch 41: 4262.5 MB\n",
      "üì¶ Processing batch 42/50\n",
      "üì¶ Processing batch 43/50\n",
      "üì¶ Processing batch 44/50\n",
      "üì¶ Processing batch 45/50\n",
      "üì¶ Processing batch 46/50\n",
      "üì¶ Processing batch 47/50\n",
      "üì¶ Processing batch 48/50\n",
      "üì¶ Processing batch 49/50\n",
      "üì¶ Processing batch 50/50\n",
      "\n",
      "üîç Applying final core filter: Track frequency (‚â•8)\n",
      "   ‚úÖ Identified 266,487 core tracks (target: ~3500)\n",
      "\n",
      "‚úÖ CORE-BASED FILTERING COMPLETE:\n",
      "   üìä Filtering Funnel:\n",
      "      ‚Ä¢ Total playlists: 1,000,000\n",
      "      ‚Ä¢ Length filter: 747,510 (74.8%)\n",
      "      ‚Ä¢ User filter: 747,141 (74.7%)\n",
      "      ‚Ä¢ Track frequency: 745,947 (74.6%)\n",
      "      ‚Ä¢ üéØ FINAL VALID: 745,947 (74.6%)\n",
      "\n",
      "üíæ Memory usage after pass 1 complete: 4858.5 MB\n",
      "üìä CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   üìÖ Temporal split at timestamp: 1487980800.0\n",
      "   üë• User activity split at: 66.0 playlists per user\n",
      "   üìã Strata Distribution:\n",
      "      ‚Ä¢ short_old_casual    :  2,916 ( 0.4%)\n",
      "      ‚Ä¢ short_old_active    : 152,095 (20.4%)\n",
      "      ‚Ä¢ short_recent_casual :  2,525 ( 0.3%)\n",
      "      ‚Ä¢ short_recent_active : 114,527 (15.4%)\n",
      "      ‚Ä¢ medium_old_casual   :  2,181 ( 0.3%)\n",
      "      ‚Ä¢ medium_old_active   : 133,183 (17.9%)\n",
      "      ‚Ä¢ medium_recent_casual:  2,730 ( 0.4%)\n",
      "      ‚Ä¢ medium_recent_active: 140,865 (18.9%)\n",
      "      ‚Ä¢ long_old_casual     :  1,258 ( 0.2%)\n",
      "      ‚Ä¢ long_old_active     : 81,260 (10.9%)\n",
      "      ‚Ä¢ long_recent_casual  :  1,948 ( 0.3%)\n",
      "      ‚Ä¢ long_recent_active  : 110,459 (14.8%)\n",
      "\n",
      "üíæ Memory usage after strata created: 4746.4 MB\n",
      "üé≤ PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING (SCALED)\n",
      "============================================================\n",
      "   üìä Global sampling ratio: 0.003\n",
      "   üèÜ Using priority scoring within strata\n",
      "\n",
      "      ‚Ä¢ short_old_casual    :    9 / 2,916 (avg score: 10.57)\n",
      "      ‚Ä¢ short_old_active    :  509 / 152,095 (avg score: 11.17)\n",
      "      ‚Ä¢ short_recent_casual :    8 / 2,525 (avg score: 11.18)\n",
      "      ‚Ä¢ short_recent_active :  383 / 114,527 (avg score: 11.03)\n",
      "      ‚Ä¢ medium_old_casual   :    7 / 2,181 (avg score: 10.59)\n",
      "      ‚Ä¢ medium_old_active   :  446 / 133,183 (avg score: 11.51)\n",
      "      ‚Ä¢ medium_recent_casual:    9 / 2,730 (avg score: 10.51)\n",
      "      ‚Ä¢ medium_recent_active:  472 / 140,865 (avg score: 11.43)\n",
      "      ‚Ä¢ long_old_casual     :    4 / 1,258 (avg score: 11.93)\n",
      "      ‚Ä¢ long_old_active     :  272 / 81,260 (avg score: 11.42)\n",
      "      ‚Ä¢ long_recent_casual  :    6 / 1,948 (avg score: 11.33)\n",
      "      ‚Ä¢ long_recent_active  :  370 / 110,459 (avg score: 11.26)\n",
      "\n",
      "   üéØ Selected 2,495 playlists for final loading\n",
      "   üìÅ Loading selected playlists...\n",
      "   üìÇ Loading from 904 files\n",
      "      üìñ File 1/904\n",
      "      üìñ File 101/904\n",
      "      üìñ File 201/904\n",
      "      üìñ File 301/904\n",
      "      üìñ File 401/904\n",
      "      üìñ File 501/904\n",
      "      üìñ File 601/904\n",
      "      üìñ File 701/904\n",
      "      üìñ File 801/904\n",
      "      üìñ File 901/904\n",
      "   ‚úÖ Loaded 2,495 final playlists\n",
      "üíæ Memory usage after pass 2 complete: 2152.4 MB\n",
      "\n",
      "üîç VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   üìä Final Entity Counts:\n",
      "      ‚Ä¢ Playlists: 2,495 (target: 2,500)\n",
      "      ‚Ä¢ Tracks: 58,160 (target: ~3,500)\n",
      "      ‚Ä¢ Artists: 17,069 (target: ~800)\n",
      "      ‚Ä¢ Albums: 34,430 (target: ~600)\n",
      "      ‚Ä¢ Users: 279 (target: ~100)\n",
      "      üéØ TOTAL: 112,433 (target: ~7,500)\n",
      "\n",
      "üéâ SCALED HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "üìä Results: 1,000,000 ‚Üí 2,495 playlists\n",
      "üìà Overall retention: 0.2%\n",
      "\n",
      "üéØ EXPERIMENTAL SCALE VERIFICATION:\n",
      "   ‚Ä¢ Actual total nodes: 112,433\n",
      "   ‚Ä¢ Target total nodes: 7,500\n",
      "   ‚Ä¢ Scale ratio: 14.991 (‚ö†Ô∏è ADJUST)\n",
      "\n",
      "üíæ Saved to: ../data/processed/spotify_scaled_hybrid_7500.json\n",
      "üì¶ File size: 42.7 MB\n",
      "\n",
      "üéì READY FOR CONTROLLED EXPERIMENTS:\n",
      "   ‚úÖ Methodology: YOUR ORIGINAL hybrid approach\n",
      "   ‚úÖ Scale: ~112,433 nodes\n",
      "   ‚úÖ Training time estimate: 3-5 minutes per configuration\n",
      "   ‚úÖ Same methodology across all 3 experiments\n",
      "   ‚úÖ Controlled conditions achieved\n",
      "\n",
      "======================================================================\n",
      "üéâ METHODOLOGY COMPARISON:\n",
      "======================================================================\n",
      "ORIGINAL HYBRID METHOD:\n",
      "  ‚Ä¢ Target: 50,000 playlists ‚Üí 661k+ nodes\n",
      "  ‚Ä¢ Training: 34-60 minutes per config\n",
      "  ‚Ä¢ Total experiment time: 4-6 hours\n",
      "\n",
      "SCALED HYBRID METHOD (THIS OUTPUT):\n",
      "  ‚Ä¢ Target: 2,500 playlists ‚Üí ~112,433 nodes\n",
      "  ‚Ä¢ Training: 3-5 minutes per config\n",
      "  ‚Ä¢ Total experiment time: 40-65 minutes\n",
      "\n",
      "üéØ SAME METHODOLOGY, CONTROLLED SCALE!\n",
      "   ‚úÖ Identical core-based filtering logic\n",
      "   ‚úÖ Identical stratified sampling approach\n",
      "   ‚úÖ Identical priority scoring system\n",
      "   ‚úÖ Perfect for experimental consistency\n",
      "\n",
      "üìã PARAMETER ADJUSTMENTS FOR SCALE CONTROL:\n",
      "   ‚Ä¢ min_track_frequency: 5 ‚Üí 8 (stricter core filtering)\n",
      "   ‚Ä¢ min_user_playlists: 3 ‚Üí 10 (better user consolidation)\n",
      "   ‚Ä¢ target_playlists: 50,000 ‚Üí 2,500 (experimental scale)\n",
      "   ‚Ä¢ User ID extraction: more aggressive consolidation\n",
      "\n",
      "üî¨ EXPERIMENTAL BENEFITS:\n",
      "   ‚úÖ Can run all 13 configurations in ~1 hour\n",
      "   ‚úÖ Same methodology across Experiments 1, 2, and 3\n",
      "   ‚úÖ Manageable memory usage (~1-2GB vs 4-6GB)\n",
      "   ‚úÖ Statistical validity maintained\n",
      "   ‚úÖ Perfect for thesis experimental design\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
