{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hybird Core-based + Stratified Sampling",
   "id": "784adb025faa243f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:16:34.114243Z",
     "start_time": "2025-07-30T12:04:06.384898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import glob\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set, Iterator\n",
    "import gc\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ScaledHybridStreamingSampler:\n",
    "    \"\"\"\n",
    "    Scaled-Down Hybrid Core-Based + Stratified Streaming Sampler\n",
    "\n",
    "    MAINTAINS YOUR ORIGINAL METHODOLOGY:\n",
    "    - Pass 1: Core-based filtering (length, user activity, track frequency)\n",
    "    - Pass 2: Stratified sampling with priority scoring\n",
    "\n",
    "    MODIFIED FOR EXPERIMENTAL SCALE:\n",
    "    - Target: ~7,500 total nodes instead of 661k+\n",
    "    - Aggressive filtering to reach target scale\n",
    "    - Same strata and priority scoring logic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 target_playlists: int = 2500,        # Scaled down from 50,000\n",
    "                 batch_size: int = 20,\n",
    "                 min_playlist_length: int = 10,\n",
    "                 max_playlist_length: int = 100,\n",
    "                 min_track_frequency: int = 8,        # Increased from 5 for scale control\n",
    "                 min_user_playlists: int = 10):       # Increased from 3 for user consolidation\n",
    "\n",
    "        self.target_playlists = target_playlists\n",
    "        self.batch_size = batch_size\n",
    "        self.min_playlist_length = min_playlist_length\n",
    "        self.max_playlist_length = max_playlist_length\n",
    "        self.min_track_frequency = min_track_frequency\n",
    "        self.min_user_playlists = min_user_playlists\n",
    "\n",
    "        # Expected scale targets for verification\n",
    "        self.expected_total_nodes = 7500\n",
    "        self.expected_tracks = 3500\n",
    "        self.expected_artists = 800\n",
    "        self.expected_albums = 600\n",
    "        self.expected_users = 100\n",
    "\n",
    "        # Statistics collectors (same as original)\n",
    "        self.track_counts = Counter()\n",
    "        self.user_counts = Counter()\n",
    "        self.playlist_stats = []\n",
    "\n",
    "        print(f\"ğŸ¯ SCALED-DOWN HYBRID STREAMING SAMPLER\")\n",
    "        print(f\"=\" * 70)\n",
    "        print(f\"ğŸ“Š METHODOLOGY: Your Original Hybrid Approach\")\n",
    "        print(f\"   âœ… Pass 1: Core-based filtering\")\n",
    "        print(f\"   âœ… Pass 2: Stratified sampling with priority scoring\")\n",
    "        print(f\"\")\n",
    "        print(f\"ğŸ”§ SCALED PARAMETERS FOR EXPERIMENTAL CONTROL:\")\n",
    "        print(f\"   â€¢ Target playlists: {target_playlists:,} (was 50,000)\")\n",
    "        print(f\"   â€¢ Batch size: {batch_size} files at a time\")\n",
    "        print(f\"   â€¢ Playlist length: {min_playlist_length}-{max_playlist_length} tracks\")\n",
    "        print(f\"   â€¢ Min track frequency: {min_track_frequency} playlists (was 5)\")\n",
    "        print(f\"   â€¢ Min user playlists: {min_user_playlists} playlists (was 3)\")\n",
    "        print(f\"\")\n",
    "        print(f\"ğŸ¯ EXPECTED SCALE: ~{self.expected_total_nodes:,} total nodes\")\n",
    "        print(f\"   â€¢ Playlists: {target_playlists:,}\")\n",
    "        print(f\"   â€¢ Tracks: ~{self.expected_tracks:,}\")\n",
    "        print(f\"   â€¢ Artists: ~{self.expected_artists:,}\")\n",
    "        print(f\"   â€¢ Albums: ~{self.expected_albums:,}\")\n",
    "        print(f\"   â€¢ Users: ~{self.expected_users:,}\")\n",
    "        print()\n",
    "\n",
    "    def get_memory_usage(self):\n",
    "        \"\"\"Get current memory usage in MB\"\"\"\n",
    "        try:\n",
    "            process = psutil.Process(os.getpid())\n",
    "            return process.memory_info().rss / 1024 / 1024\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "    def print_memory_status(self, stage: str):\n",
    "        \"\"\"Print current memory usage\"\"\"\n",
    "        memory_mb = self.get_memory_usage()\n",
    "        print(f\"ğŸ’¾ Memory usage after {stage}: {memory_mb:.1f} MB\")\n",
    "\n",
    "    def get_file_batches(self, file_pattern: str) -> List[List[str]]:\n",
    "        \"\"\"Split files into manageable batches (same as original)\"\"\"\n",
    "        file_paths = glob.glob(file_pattern)\n",
    "        file_paths.sort()\n",
    "\n",
    "        if not file_paths:\n",
    "            raise FileNotFoundError(f\"No files found: {file_pattern}\")\n",
    "\n",
    "        print(f\"ğŸ“ Found {len(file_paths)} files\")\n",
    "\n",
    "        # Split into batches\n",
    "        batches = []\n",
    "        for i in range(0, len(file_paths), self.batch_size):\n",
    "            batch = file_paths[i:i + self.batch_size]\n",
    "            batches.append(batch)\n",
    "\n",
    "        print(f\"ğŸ“¦ Created {len(batches)} batches of ~{self.batch_size} files each\")\n",
    "        return batches\n",
    "\n",
    "    def _extract_user_id(self, playlist: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Extract user identifier with AGGRESSIVE consolidation for target scale\n",
    "        (Modified from original for better user consolidation)\n",
    "        \"\"\"\n",
    "        # Method 1: Use name-based grouping with more aggressive consolidation\n",
    "        name = playlist.get('name', '').lower().strip()\n",
    "        if name:\n",
    "            # Use first 2-3 characters for heavy consolidation\n",
    "            words = name.split()\n",
    "            if words:\n",
    "                user_base = words[0][:2]  # Only first 2 chars (was more in original)\n",
    "                user_id = ''.join(c for c in user_base if c.isalnum())\n",
    "                if user_id:\n",
    "                    return user_id\n",
    "\n",
    "        # Method 2: PID-based with heavy consolidation (force into small user set)\n",
    "        pid = playlist.get('pid', 0)\n",
    "        return f\"u{pid % 200}\"  # Force into ~200 user bins (will filter to ~100 active)\n",
    "\n",
    "    def pass1_core_filtering(self, file_pattern: str) -> Dict:\n",
    "        \"\"\"\n",
    "        PASS 1: Core-based filtering + Statistics collection\n",
    "        (SAME LOGIC as original, but with scaled parameters)\n",
    "        \"\"\"\n",
    "        print(\"ğŸ” PASS 1: HYBRID CORE-BASED FILTERING (SCALED)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        batches = self.get_file_batches(file_pattern)\n",
    "\n",
    "        # Stage counts (same as original)\n",
    "        stage_counts = {\n",
    "            'total_seen': 0,\n",
    "            'passed_length_filter': 0,\n",
    "            'passed_user_filter': 0,\n",
    "            'passed_track_frequency_filter': 0,\n",
    "            'final_valid': 0\n",
    "        }\n",
    "\n",
    "        print(\"ğŸš€ Applying SCALED Core-Based Filters:\")\n",
    "        print(f\"   âœ… Step 1: Playlist length ({self.min_playlist_length}-{self.max_playlist_length} tracks)\")\n",
    "        print(f\"   âœ… Step 2: User activity (â‰¥{self.min_user_playlists} playlists per user)\")\n",
    "        print(f\"   âœ… Step 3: Track frequency (â‰¥{self.min_track_frequency} appearances)\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1a: Collect user activity statistics (same as original)\n",
    "        print(\"ğŸ“Š Sub-pass 1a: Collecting user activity statistics...\")\n",
    "        user_playlist_count = Counter()\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            if batch_idx % 20 == 0:\n",
    "                print(f\"   User stats progress: {batch_idx + 1}/{len(batches)} batches\")\n",
    "\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "\n",
    "                    for playlist in file_playlists:\n",
    "                        user_id = self._extract_user_id(playlist)\n",
    "                        user_playlist_count[user_id] += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        # Identify active users (same logic, but higher threshold)\n",
    "        active_users = {\n",
    "            user for user, count in user_playlist_count.items()\n",
    "            if count >= self.min_user_playlists\n",
    "        }\n",
    "\n",
    "        print(f\"   âœ… Identified {len(active_users):,} active users (target: ~{self.expected_users})\")\n",
    "        print()\n",
    "\n",
    "        # Sub-pass 1b: Apply all core filters (same as original)\n",
    "        print(\"ğŸ” Sub-pass 1b: Applying all core filters...\")\n",
    "\n",
    "        for batch_idx, file_batch in enumerate(batches):\n",
    "            print(f\"ğŸ“¦ Processing batch {batch_idx + 1}/{len(batches)}\")\n",
    "\n",
    "            batch_playlists = []\n",
    "\n",
    "            # Load batch (same as original)\n",
    "            for file_path in file_batch:\n",
    "                try:\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                    file_playlists = data.get('playlists', [])\n",
    "\n",
    "                    # Add source file info\n",
    "                    for playlist in file_playlists:\n",
    "                        playlist['_source_file'] = file_path\n",
    "\n",
    "                    batch_playlists.extend(file_playlists)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Error loading {os.path.basename(file_path)}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Process batch with core filtering (same logic as original)\n",
    "            for playlist_idx, playlist in enumerate(batch_playlists):\n",
    "                stage_counts['total_seen'] += 1\n",
    "\n",
    "                # CORE FILTER 1: Playlist length (same as original)\n",
    "                tracks = playlist.get('tracks', [])\n",
    "                playlist_length = len(tracks)\n",
    "\n",
    "                if not (self.min_playlist_length <= playlist_length <= self.max_playlist_length):\n",
    "                    continue\n",
    "                stage_counts['passed_length_filter'] += 1\n",
    "\n",
    "                # CORE FILTER 2: User activity (same as original)\n",
    "                user_id = self._extract_user_id(playlist)\n",
    "                if user_id not in active_users:\n",
    "                    continue\n",
    "                stage_counts['passed_user_filter'] += 1\n",
    "\n",
    "                # Count tracks for frequency analysis (same as original)\n",
    "                playlist_tracks = set()\n",
    "                for track in tracks:\n",
    "                    track_uri = track.get('track_uri', '')\n",
    "                    if track_uri:\n",
    "                        self.track_counts[track_uri] += 1\n",
    "                        playlist_tracks.add(track_uri)\n",
    "\n",
    "                self.user_counts[user_id] += 1\n",
    "\n",
    "                # Store playlist metadata (same as original)\n",
    "                playlist_metadata = {\n",
    "                    'file_path': playlist['_source_file'],\n",
    "                    'pid': playlist.get('pid'),\n",
    "                    'length': playlist_length,\n",
    "                    'modified_at': playlist.get('modified_at', 0),\n",
    "                    'user_id': user_id,\n",
    "                    'track_uris': list(playlist_tracks),\n",
    "                    'name': playlist.get('name', ''),\n",
    "                    'collaborative': playlist.get('collaborative', False),\n",
    "                    'num_followers': playlist.get('num_followers', 0)\n",
    "                }\n",
    "\n",
    "                self.playlist_stats.append(playlist_metadata)\n",
    "\n",
    "            # Clear batch from memory (same as original)\n",
    "            del batch_playlists\n",
    "            gc.collect()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"   Progress: {stage_counts['total_seen']:,} seen, {len(self.playlist_stats):,} valid so far\")\n",
    "                self.print_memory_status(f\"batch {batch_idx + 1}\")\n",
    "\n",
    "        # CORE FILTER 3: Track frequency (same logic, higher threshold)\n",
    "        print(f\"\\nğŸ” Applying final core filter: Track frequency (â‰¥{self.min_track_frequency})\")\n",
    "\n",
    "        core_tracks = {\n",
    "            track for track, count in self.track_counts.items()\n",
    "            if count >= self.min_track_frequency\n",
    "        }\n",
    "\n",
    "        print(f\"   âœ… Identified {len(core_tracks):,} core tracks (target: ~{self.expected_tracks})\")\n",
    "\n",
    "        # Filter playlists that have core tracks (same as original)\n",
    "        filtered_playlist_stats = []\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            playlist_tracks = set(playlist_meta['track_uris'])\n",
    "            if playlist_tracks.intersection(core_tracks):\n",
    "                filtered_playlist_stats.append(playlist_meta)\n",
    "                stage_counts['passed_track_frequency_filter'] += 1\n",
    "\n",
    "        self.playlist_stats = filtered_playlist_stats\n",
    "        stage_counts['final_valid'] = len(filtered_playlist_stats)\n",
    "\n",
    "        print(f\"\\nâœ… CORE-BASED FILTERING COMPLETE:\")\n",
    "        print(f\"   ğŸ“Š Filtering Funnel:\")\n",
    "        print(f\"      â€¢ Total playlists: {stage_counts['total_seen']:,}\")\n",
    "        print(f\"      â€¢ Length filter: {stage_counts['passed_length_filter']:,} ({stage_counts['passed_length_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      â€¢ User filter: {stage_counts['passed_user_filter']:,} ({stage_counts['passed_user_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      â€¢ Track frequency: {stage_counts['passed_track_frequency_filter']:,} ({stage_counts['passed_track_frequency_filter']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print(f\"      â€¢ ğŸ¯ FINAL VALID: {stage_counts['final_valid']:,} ({stage_counts['final_valid']/stage_counts['total_seen']*100:.1f}%)\")\n",
    "        print()\n",
    "\n",
    "        return {\n",
    "            'total_playlists': stage_counts['total_seen'],\n",
    "            'valid_playlists': stage_counts['final_valid'],\n",
    "            'core_tracks': core_tracks,\n",
    "            'active_users': active_users,\n",
    "            'unique_tracks': len(self.track_counts),\n",
    "            'stage_counts': stage_counts\n",
    "        }\n",
    "\n",
    "    def create_strata(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"\n",
    "        Create comprehensive strata for stratified sampling\n",
    "        (IDENTICAL to original method)\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“Š CREATING STRATIFIED SAMPLING STRATA\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Get temporal split (same as original)\n",
    "        timestamps = [p['modified_at'] for p in self.playlist_stats if p['modified_at'] > 0]\n",
    "        if timestamps:\n",
    "            median_time = np.median(timestamps)\n",
    "        else:\n",
    "            median_time = 1500000000  # Default\n",
    "\n",
    "        # Get user activity split (same as original)\n",
    "        user_playlist_counts = {}\n",
    "        for playlist_meta in self.playlist_stats:\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_playlist_counts[user_id] = user_playlist_counts.get(user_id, 0) + 1\n",
    "\n",
    "        user_activity_median = np.median(list(user_playlist_counts.values())) if user_playlist_counts else 5\n",
    "\n",
    "        print(f\"   ğŸ“… Temporal split at timestamp: {median_time}\")\n",
    "        print(f\"   ğŸ‘¥ User activity split at: {user_activity_median} playlists per user\")\n",
    "\n",
    "        # Create 12 comprehensive strata (IDENTICAL to original)\n",
    "        strata = {\n",
    "            'short_old_casual': [], 'short_old_active': [],\n",
    "            'short_recent_casual': [], 'short_recent_active': [],\n",
    "            'medium_old_casual': [], 'medium_old_active': [],\n",
    "            'medium_recent_casual': [], 'medium_recent_active': [],\n",
    "            'long_old_casual': [], 'long_old_active': [],\n",
    "            'long_recent_casual': [], 'long_recent_active': []\n",
    "        }\n",
    "\n",
    "        for i, playlist_meta in enumerate(self.playlist_stats):\n",
    "            length = playlist_meta['length']\n",
    "            timestamp = playlist_meta['modified_at']\n",
    "            user_id = playlist_meta['user_id']\n",
    "            user_activity = user_playlist_counts.get(user_id, 1)\n",
    "\n",
    "            # Length category (same as original)\n",
    "            if length <= 30:\n",
    "                length_cat = 'short'\n",
    "            elif length <= 60:\n",
    "                length_cat = 'medium'\n",
    "            else:\n",
    "                length_cat = 'long'\n",
    "\n",
    "            # Time category (same as original)\n",
    "            time_cat = 'recent' if timestamp >= median_time else 'old'\n",
    "\n",
    "            # User activity category (same as original)\n",
    "            activity_cat = 'active' if user_activity >= user_activity_median else 'casual'\n",
    "\n",
    "            # Combine into stratum (same as original)\n",
    "            stratum_key = f\"{length_cat}_{time_cat}_{activity_cat}\"\n",
    "            strata[stratum_key].append(i)\n",
    "\n",
    "        # Print strata distribution (same as original)\n",
    "        print(\"   ğŸ“‹ Strata Distribution:\")\n",
    "        total_playlists = len(self.playlist_stats)\n",
    "\n",
    "        for stratum, indices in strata.items():\n",
    "            if indices:  # Only show non-empty strata\n",
    "                percentage = len(indices) / total_playlists * 100\n",
    "                print(f\"      â€¢ {stratum:20s}: {len(indices):6,} ({percentage:4.1f}%)\")\n",
    "\n",
    "        print()\n",
    "        return strata\n",
    "\n",
    "    def _calculate_priority_score(self, playlist_meta: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Calculate priority score for playlist selection\n",
    "        (IDENTICAL to original method)\n",
    "        \"\"\"\n",
    "        score = 0.0\n",
    "\n",
    "        # Factor 1: Track diversity (30% weight)\n",
    "        unique_tracks = len(playlist_meta['track_uris'])\n",
    "        playlist_length = playlist_meta['length']\n",
    "        if playlist_length > 0:\n",
    "            track_diversity_ratio = unique_tracks / playlist_length\n",
    "            score += track_diversity_ratio * 3.0\n",
    "\n",
    "        # Factor 2: User engagement (25% weight)\n",
    "        num_followers = playlist_meta.get('num_followers', 0)\n",
    "        if num_followers > 0:\n",
    "            follower_score = min(np.log10(num_followers + 1), 3.0)\n",
    "            score += follower_score * 2.5\n",
    "\n",
    "        # Factor 3: Playlist completeness (20% weight)\n",
    "        name = playlist_meta.get('name', '')\n",
    "        has_good_name = len(name.strip()) > 3 and not name.lower().startswith('my playlist')\n",
    "        if has_good_name:\n",
    "            score += 2.0\n",
    "\n",
    "        # Factor 4: Collaborative playlists bonus (10% weight)\n",
    "        if playlist_meta.get('collaborative', False):\n",
    "            score += 1.0\n",
    "\n",
    "        # Factor 5: Length balance bonus (15% weight)\n",
    "        length = playlist_meta['length']\n",
    "        if 20 <= length <= 80:  # Sweet spot\n",
    "            score += 1.5\n",
    "\n",
    "        return score\n",
    "\n",
    "    def pass2_stratified_sampling(self, strata: Dict[str, List[int]]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        PASS 2: Stratified sampling with priority scoring\n",
    "        (IDENTICAL logic to original, but with scaled target)\n",
    "        \"\"\"\n",
    "        print(\"ğŸ² PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING (SCALED)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        total_available = len(self.playlist_stats)\n",
    "\n",
    "        if total_available <= self.target_playlists:\n",
    "            print(f\"   ğŸ“ Available ({total_available:,}) â‰¤ target ({self.target_playlists:,})\")\n",
    "            selected_indices = list(range(total_available))\n",
    "        else:\n",
    "            sampling_ratio = self.target_playlists / total_available\n",
    "            selected_indices = set()\n",
    "\n",
    "            print(f\"   ğŸ“Š Global sampling ratio: {sampling_ratio:.3f}\")\n",
    "            print(f\"   ğŸ† Using priority scoring within strata\")\n",
    "            print()\n",
    "\n",
    "            # Same stratified sampling logic as original\n",
    "            for stratum, indices in strata.items():\n",
    "                if not indices:\n",
    "                    continue\n",
    "\n",
    "                stratum_target = max(1, int(len(indices) * sampling_ratio))\n",
    "                stratum_target = min(stratum_target, len(indices))\n",
    "\n",
    "                # Score playlists in this stratum (same as original)\n",
    "                scored_playlists = []\n",
    "                for idx in indices:\n",
    "                    playlist_meta = self.playlist_stats[idx]\n",
    "                    score = self._calculate_priority_score(playlist_meta)\n",
    "                    scored_playlists.append((idx, score))\n",
    "\n",
    "                # Sort by score and sample (same as original)\n",
    "                scored_playlists.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "                # Hybrid: 70% top-scored + 30% random (same as original)\n",
    "                top_count = int(stratum_target * 0.7)\n",
    "                random_count = stratum_target - top_count\n",
    "\n",
    "                selected = [idx for idx, _ in scored_playlists[:top_count]]\n",
    "\n",
    "                if random_count > 0 and len(scored_playlists) > top_count:\n",
    "                    remaining = [idx for idx, _ in scored_playlists[top_count:]]\n",
    "                    if len(remaining) >= random_count:\n",
    "                        selected.extend(random.sample(remaining, random_count))\n",
    "                    else:\n",
    "                        selected.extend(remaining)\n",
    "\n",
    "                selected_indices.update(selected)\n",
    "\n",
    "                avg_score = np.mean([score for _, score in scored_playlists[:len(selected)]])\n",
    "                print(f\"      â€¢ {stratum:20s}: {len(selected):4,} / {len(indices):5,} (avg score: {avg_score:.2f})\")\n",
    "\n",
    "            selected_indices = list(selected_indices)\n",
    "\n",
    "        print(f\"\\n   ğŸ¯ Selected {len(selected_indices):,} playlists for final loading\")\n",
    "\n",
    "        # Load selected playlists (same as original)\n",
    "        return self._load_selected_playlists(selected_indices)\n",
    "\n",
    "    def _load_selected_playlists(self, selected_indices: List[int]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Load only the selected playlists from files\n",
    "        (IDENTICAL to original method)\n",
    "        \"\"\"\n",
    "        print(\"   ğŸ“ Loading selected playlists...\")\n",
    "\n",
    "        # Group by file (same as original)\n",
    "        file_to_playlists = defaultdict(list)\n",
    "        for idx in selected_indices:\n",
    "            playlist_meta = self.playlist_stats[idx]\n",
    "            file_path = playlist_meta['file_path']\n",
    "            file_to_playlists[file_path].append(playlist_meta)\n",
    "\n",
    "        print(f\"   ğŸ“‚ Loading from {len(file_to_playlists)} files\")\n",
    "\n",
    "        # Load playlists (same as original)\n",
    "        final_playlists = []\n",
    "\n",
    "        for file_idx, (file_path, playlist_metas) in enumerate(file_to_playlists.items()):\n",
    "            if file_idx % 100 == 0:\n",
    "                print(f\"      ğŸ“– File {file_idx + 1}/{len(file_to_playlists)}\")\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                file_playlists = data.get('playlists', [])\n",
    "                pid_to_playlist = {p.get('pid'): p for p in file_playlists}\n",
    "\n",
    "                for meta in playlist_metas:\n",
    "                    pid = meta['pid']\n",
    "                    if pid in pid_to_playlist:\n",
    "                        playlist = pid_to_playlist[pid]\n",
    "                        playlist['_sampling_score'] = self._calculate_priority_score(meta)\n",
    "                        final_playlists.append(playlist)\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        print(f\"   âœ… Loaded {len(final_playlists):,} final playlists\")\n",
    "        return final_playlists\n",
    "\n",
    "    def run_hybrid_sampling(self, file_pattern: str) -> Tuple[List[Dict], Dict]:\n",
    "        \"\"\"\n",
    "        Main method: Complete hybrid sampling workflow\n",
    "        (SAME STRUCTURE as original, with scale verification)\n",
    "        \"\"\"\n",
    "        print(\"ğŸš€ STARTING SCALED-DOWN HYBRID SAMPLING\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        self.print_memory_status(\"start\")\n",
    "\n",
    "        # Pass 1: Core-based filtering (same as original)\n",
    "        stats = self.pass1_core_filtering(file_pattern)\n",
    "        self.print_memory_status(\"pass 1 complete\")\n",
    "\n",
    "        # Create strata (same as original)\n",
    "        strata = self.create_strata()\n",
    "        self.print_memory_status(\"strata created\")\n",
    "\n",
    "        # Pass 2: Stratified sampling (same as original)\n",
    "        final_playlists = self.pass2_stratified_sampling(strata)\n",
    "        self.print_memory_status(\"pass 2 complete\")\n",
    "\n",
    "        # ADDED: Scale verification for experimental control\n",
    "        actual_scale = self._verify_final_scale(final_playlists)\n",
    "\n",
    "        # Final statistics (enhanced with scale info)\n",
    "        final_stats = {\n",
    "            'methodology': 'scaled_hybrid_core_based_stratified_streaming',\n",
    "            'original_total': stats['total_playlists'],\n",
    "            'final_sampled': len(final_playlists),\n",
    "            'retention_rate': len(final_playlists) / stats['total_playlists'],\n",
    "            'core_filtering_retention': stats['valid_playlists'] / stats['total_playlists'],\n",
    "            'unique_tracks': stats['unique_tracks'],\n",
    "            'core_tracks_count': len(stats['core_tracks']),\n",
    "            'active_users_count': len(stats['active_users']),\n",
    "            'stage_counts': stats['stage_counts'],\n",
    "            'actual_scale': actual_scale,\n",
    "            'scale_targets': {\n",
    "                'total_nodes': self.expected_total_nodes,\n",
    "                'playlists': self.target_playlists,\n",
    "                'tracks': self.expected_tracks,\n",
    "                'artists': self.expected_artists,\n",
    "                'albums': self.expected_albums,\n",
    "                'users': self.expected_users\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(\"\\nğŸ‰ SCALED HYBRID SAMPLING COMPLETE!\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"ğŸ“Š Results: {stats['total_playlists']:,} â†’ {len(final_playlists):,} playlists\")\n",
    "        print(f\"ğŸ“ˆ Overall retention: {len(final_playlists) / stats['total_playlists']:.1%}\")\n",
    "\n",
    "        # Scale verification summary\n",
    "        total_actual = actual_scale['total_nodes']\n",
    "        scale_ratio = total_actual / self.expected_total_nodes\n",
    "        print(f\"\\nğŸ¯ EXPERIMENTAL SCALE VERIFICATION:\")\n",
    "        print(f\"   â€¢ Actual total nodes: {total_actual:,}\")\n",
    "        print(f\"   â€¢ Target total nodes: {self.expected_total_nodes:,}\")\n",
    "        print(f\"   â€¢ Scale ratio: {scale_ratio:.3f} ({'âœ… GOOD' if 0.8 <= scale_ratio <= 1.2 else 'âš ï¸ ADJUST'})\")\n",
    "\n",
    "        return final_playlists, final_stats\n",
    "\n",
    "    def _verify_final_scale(self, final_playlists: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Verify that final scale meets experimental targets\n",
    "        \"\"\"\n",
    "        print(\"\\nğŸ” VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Count actual entities\n",
    "        actual_tracks = set()\n",
    "        actual_artists = set()\n",
    "        actual_albums = set()\n",
    "        actual_users = set()\n",
    "\n",
    "        for playlist in final_playlists:\n",
    "            user_id = self._extract_user_id(playlist)\n",
    "            actual_users.add(user_id)\n",
    "\n",
    "            for track in playlist.get('tracks', []):\n",
    "                track_uri = track.get('track_uri', '')\n",
    "                artist_uri = track.get('artist_uri', '')\n",
    "                album_uri = track.get('album_uri', '')\n",
    "\n",
    "                if track_uri:\n",
    "                    actual_tracks.add(track_uri)\n",
    "                if artist_uri:\n",
    "                    actual_artists.add(artist_uri)\n",
    "                if album_uri:\n",
    "                    actual_albums.add(album_uri)\n",
    "\n",
    "        actual_scale = {\n",
    "            'playlists': len(final_playlists),\n",
    "            'tracks': len(actual_tracks),\n",
    "            'artists': len(actual_artists),\n",
    "            'albums': len(actual_albums),\n",
    "            'users': len(actual_users),\n",
    "            'total_nodes': len(final_playlists) + len(actual_tracks) + len(actual_artists) + len(actual_albums) + len(actual_users)\n",
    "        }\n",
    "\n",
    "        print(f\"   ğŸ“Š Final Entity Counts:\")\n",
    "        print(f\"      â€¢ Playlists: {actual_scale['playlists']:,} (target: {self.target_playlists:,})\")\n",
    "        print(f\"      â€¢ Tracks: {actual_scale['tracks']:,} (target: ~{self.expected_tracks:,})\")\n",
    "        print(f\"      â€¢ Artists: {actual_scale['artists']:,} (target: ~{self.expected_artists:,})\")\n",
    "        print(f\"      â€¢ Albums: {actual_scale['albums']:,} (target: ~{self.expected_albums:,})\")\n",
    "        print(f\"      â€¢ Users: {actual_scale['users']:,} (target: ~{self.expected_users:,})\")\n",
    "        print(f\"      ğŸ¯ TOTAL: {actual_scale['total_nodes']:,} (target: ~{self.expected_total_nodes:,})\")\n",
    "\n",
    "        return actual_scale\n",
    "\n",
    "\n",
    "def run_scaled_hybrid_sampling(file_pattern: str,\n",
    "                              target_playlists: int = 2500,\n",
    "                              output_suffix: str = \"scaled_hybrid_7500\"):\n",
    "    \"\"\"\n",
    "    One-function call to run scaled-down hybrid sampling\n",
    "    MAINTAINS YOUR ORIGINAL METHODOLOGY at controlled scale\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize sampler with scaled parameters\n",
    "    sampler = ScaledHybridStreamingSampler(\n",
    "        target_playlists=target_playlists,\n",
    "        batch_size=20,\n",
    "        min_playlist_length=10,\n",
    "        max_playlist_length=100,\n",
    "        min_track_frequency=8,   # Increased for scale control\n",
    "        min_user_playlists=10    # Increased for user consolidation\n",
    "    )\n",
    "\n",
    "    # Run sampling\n",
    "    sampled_playlists, stats = sampler.run_hybrid_sampling(file_pattern)\n",
    "\n",
    "    # Save results\n",
    "    output_data = {\n",
    "        'info': {\n",
    "            'generated_on': datetime.now().isoformat(),\n",
    "            'sampling_method': 'scaled_hybrid_core_based_stratified_streaming',\n",
    "            'original_method': 'hybrid_core_based_stratified_streaming',\n",
    "            'scaling_purpose': 'experimental_control_7500_nodes',\n",
    "            'parameters': {\n",
    "                'target_playlists': target_playlists,\n",
    "                'batch_size': 20,\n",
    "                'min_playlist_length': 10,\n",
    "                'max_playlist_length': 100,\n",
    "                'min_track_frequency': 8,\n",
    "                'min_user_playlists': 10\n",
    "            }\n",
    "        },\n",
    "        'sampling_stats': stats,\n",
    "        'playlists': sampled_playlists\n",
    "    }\n",
    "\n",
    "    # Save\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    output_file = f'../data/processed/spotify_{output_suffix}.json'\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "\n",
    "    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"\\nğŸ’¾ Saved to: {output_file}\")\n",
    "    print(f\"ğŸ“¦ File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    print(f\"\\nğŸ“ READY FOR CONTROLLED EXPERIMENTS:\")\n",
    "    print(f\"   âœ… Methodology: YOUR ORIGINAL hybrid approach\")\n",
    "    print(f\"   âœ… Scale: ~{stats['actual_scale']['total_nodes']:,} nodes\")\n",
    "    print(f\"   âœ… Training time estimate: 3-5 minutes per configuration\")\n",
    "    print(f\"   âœ… Same methodology across all 3 experiments\")\n",
    "    print(f\"   âœ… Controlled conditions achieved\")\n",
    "\n",
    "    return sampled_playlists, stats, output_file\n",
    "\n",
    "\n",
    "# Example usage with your original methodology at controlled scale:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility (same as your original)\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Configure for your system\n",
    "    file_pattern = \"../data/raw/data/mpd.slice.*.json\"\n",
    "\n",
    "    print(\"ğŸ¯ RUNNING YOUR HYBRID METHOD AT CONTROLLED SCALE\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… PRESERVES: Your original core-based + stratified approach\")\n",
    "    print(\"âœ… PRESERVES: Your 12-strata system and priority scoring\")\n",
    "    print(\"âœ… PRESERVES: Your hybrid 70%/30% selection logic\")\n",
    "    print(\"âœ… MODIFIES: Parameters to achieve ~7,500 node target\")\n",
    "    print()\n",
    "\n",
    "    # Run scaled hybrid sampling\n",
    "    playlists, stats, output_file = run_scaled_hybrid_sampling(\n",
    "        file_pattern=file_pattern,\n",
    "        target_playlists=2500,\n",
    "        output_suffix=\"scaled_hybrid_7500\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ METHODOLOGY COMPARISON:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"ORIGINAL HYBRID METHOD:\")\n",
    "    print(\"  â€¢ Target: 50,000 playlists â†’ 661k+ nodes\")\n",
    "    print(\"  â€¢ Training: 34-60 minutes per config\")\n",
    "    print(\"  â€¢ Total experiment time: 4-6 hours\")\n",
    "    print()\n",
    "    print(\"SCALED HYBRID METHOD (THIS OUTPUT):\")\n",
    "    print(f\"  â€¢ Target: 2,500 playlists â†’ ~{stats['actual_scale']['total_nodes']:,} nodes\")\n",
    "    print(\"  â€¢ Training: 3-5 minutes per config\")\n",
    "    print(\"  â€¢ Total experiment time: 40-65 minutes\")\n",
    "    print()\n",
    "    print(\"ğŸ¯ SAME METHODOLOGY, CONTROLLED SCALE!\")\n",
    "    print(\"   âœ… Identical core-based filtering logic\")\n",
    "    print(\"   âœ… Identical stratified sampling approach\")\n",
    "    print(\"   âœ… Identical priority scoring system\")\n",
    "    print(\"   âœ… Perfect for experimental consistency\")\n",
    "\n",
    "    # Optional: Show parameter comparison\n",
    "    print(f\"\\nğŸ“‹ PARAMETER ADJUSTMENTS FOR SCALE CONTROL:\")\n",
    "    print(f\"   â€¢ min_track_frequency: 5 â†’ 8 (stricter core filtering)\")\n",
    "    print(f\"   â€¢ min_user_playlists: 3 â†’ 10 (better user consolidation)\")\n",
    "    print(f\"   â€¢ target_playlists: 50,000 â†’ 2,500 (experimental scale)\")\n",
    "    print(f\"   â€¢ User ID extraction: more aggressive consolidation\")\n",
    "    print()\n",
    "    print(f\"ğŸ”¬ EXPERIMENTAL BENEFITS:\")\n",
    "    print(f\"   âœ… Can run all 13 configurations in ~1 hour\")\n",
    "    print(f\"   âœ… Same methodology across Experiments 1, 2, and 3\")\n",
    "    print(f\"   âœ… Manageable memory usage (~1-2GB vs 4-6GB)\")\n",
    "    print(f\"   âœ… Statistical validity maintained\")\n",
    "    print(f\"   âœ… Perfect for thesis experimental design\")"
   ],
   "id": "eabd48b4957cfcb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ RUNNING YOUR HYBRID METHOD AT CONTROLLED SCALE\n",
      "============================================================\n",
      "âœ… PRESERVES: Your original core-based + stratified approach\n",
      "âœ… PRESERVES: Your 12-strata system and priority scoring\n",
      "âœ… PRESERVES: Your hybrid 70%/30% selection logic\n",
      "âœ… MODIFIES: Parameters to achieve ~7,500 node target\n",
      "\n",
      "ğŸ¯ SCALED-DOWN HYBRID STREAMING SAMPLER\n",
      "======================================================================\n",
      "ğŸ“Š METHODOLOGY: Your Original Hybrid Approach\n",
      "   âœ… Pass 1: Core-based filtering\n",
      "   âœ… Pass 2: Stratified sampling with priority scoring\n",
      "\n",
      "ğŸ”§ SCALED PARAMETERS FOR EXPERIMENTAL CONTROL:\n",
      "   â€¢ Target playlists: 2,500 (was 50,000)\n",
      "   â€¢ Batch size: 20 files at a time\n",
      "   â€¢ Playlist length: 10-100 tracks\n",
      "   â€¢ Min track frequency: 8 playlists (was 5)\n",
      "   â€¢ Min user playlists: 10 playlists (was 3)\n",
      "\n",
      "ğŸ¯ EXPECTED SCALE: ~7,500 total nodes\n",
      "   â€¢ Playlists: 2,500\n",
      "   â€¢ Tracks: ~3,500\n",
      "   â€¢ Artists: ~800\n",
      "   â€¢ Albums: ~600\n",
      "   â€¢ Users: ~100\n",
      "\n",
      "ğŸš€ STARTING SCALED-DOWN HYBRID SAMPLING\n",
      "======================================================================\n",
      "ğŸ’¾ Memory usage after start: 180.6 MB\n",
      "ğŸ” PASS 1: HYBRID CORE-BASED FILTERING (SCALED)\n",
      "============================================================\n",
      "ğŸ“ Found 1000 files\n",
      "ğŸ“¦ Created 50 batches of ~20 files each\n",
      "ğŸš€ Applying SCALED Core-Based Filters:\n",
      "   âœ… Step 1: Playlist length (10-100 tracks)\n",
      "   âœ… Step 2: User activity (â‰¥10 playlists per user)\n",
      "   âœ… Step 3: Track frequency (â‰¥8 appearances)\n",
      "\n",
      "ğŸ“Š Sub-pass 1a: Collecting user activity statistics...\n",
      "   User stats progress: 1/50 batches\n",
      "   User stats progress: 21/50 batches\n",
      "   User stats progress: 41/50 batches\n",
      "   âœ… Identified 850 active users (target: ~100)\n",
      "\n",
      "ğŸ” Sub-pass 1b: Applying all core filters...\n",
      "ğŸ“¦ Processing batch 1/50\n",
      "   Progress: 20,000 seen, 14,839 valid so far\n",
      "ğŸ’¾ Memory usage after batch 1: 1460.9 MB\n",
      "ğŸ“¦ Processing batch 2/50\n",
      "ğŸ“¦ Processing batch 3/50\n",
      "ğŸ“¦ Processing batch 4/50\n",
      "ğŸ“¦ Processing batch 5/50\n",
      "ğŸ“¦ Processing batch 6/50\n",
      "ğŸ“¦ Processing batch 7/50\n",
      "ğŸ“¦ Processing batch 8/50\n",
      "ğŸ“¦ Processing batch 9/50\n",
      "ğŸ“¦ Processing batch 10/50\n",
      "ğŸ“¦ Processing batch 11/50\n",
      "   Progress: 220,000 seen, 164,251 valid so far\n",
      "ğŸ’¾ Memory usage after batch 11: 2283.5 MB\n",
      "ğŸ“¦ Processing batch 12/50\n",
      "ğŸ“¦ Processing batch 13/50\n",
      "ğŸ“¦ Processing batch 14/50\n",
      "ğŸ“¦ Processing batch 15/50\n",
      "ğŸ“¦ Processing batch 16/50\n",
      "ğŸ“¦ Processing batch 17/50\n",
      "ğŸ“¦ Processing batch 18/50\n",
      "ğŸ“¦ Processing batch 19/50\n",
      "ğŸ“¦ Processing batch 20/50\n",
      "ğŸ“¦ Processing batch 21/50\n",
      "   Progress: 420,000 seen, 313,842 valid so far\n",
      "ğŸ’¾ Memory usage after batch 21: 3077.4 MB\n",
      "ğŸ“¦ Processing batch 22/50\n",
      "ğŸ“¦ Processing batch 23/50\n",
      "ğŸ“¦ Processing batch 24/50\n",
      "ğŸ“¦ Processing batch 25/50\n",
      "ğŸ“¦ Processing batch 26/50\n",
      "ğŸ“¦ Processing batch 27/50\n",
      "ğŸ“¦ Processing batch 28/50\n",
      "ğŸ“¦ Processing batch 29/50\n",
      "ğŸ“¦ Processing batch 30/50\n",
      "ğŸ“¦ Processing batch 31/50\n",
      "   Progress: 620,000 seen, 463,678 valid so far\n",
      "ğŸ’¾ Memory usage after batch 31: 3746.3 MB\n",
      "ğŸ“¦ Processing batch 32/50\n",
      "ğŸ“¦ Processing batch 33/50\n",
      "ğŸ“¦ Processing batch 34/50\n",
      "ğŸ“¦ Processing batch 35/50\n",
      "ğŸ“¦ Processing batch 36/50\n",
      "ğŸ“¦ Processing batch 37/50\n",
      "ğŸ“¦ Processing batch 38/50\n",
      "ğŸ“¦ Processing batch 39/50\n",
      "ğŸ“¦ Processing batch 40/50\n",
      "ğŸ“¦ Processing batch 41/50\n",
      "   Progress: 820,000 seen, 612,643 valid so far\n",
      "ğŸ’¾ Memory usage after batch 41: 4262.5 MB\n",
      "ğŸ“¦ Processing batch 42/50\n",
      "ğŸ“¦ Processing batch 43/50\n",
      "ğŸ“¦ Processing batch 44/50\n",
      "ğŸ“¦ Processing batch 45/50\n",
      "ğŸ“¦ Processing batch 46/50\n",
      "ğŸ“¦ Processing batch 47/50\n",
      "ğŸ“¦ Processing batch 48/50\n",
      "ğŸ“¦ Processing batch 49/50\n",
      "ğŸ“¦ Processing batch 50/50\n",
      "\n",
      "ğŸ” Applying final core filter: Track frequency (â‰¥8)\n",
      "   âœ… Identified 266,487 core tracks (target: ~3500)\n",
      "\n",
      "âœ… CORE-BASED FILTERING COMPLETE:\n",
      "   ğŸ“Š Filtering Funnel:\n",
      "      â€¢ Total playlists: 1,000,000\n",
      "      â€¢ Length filter: 747,510 (74.8%)\n",
      "      â€¢ User filter: 747,141 (74.7%)\n",
      "      â€¢ Track frequency: 745,947 (74.6%)\n",
      "      â€¢ ğŸ¯ FINAL VALID: 745,947 (74.6%)\n",
      "\n",
      "ğŸ’¾ Memory usage after pass 1 complete: 4858.5 MB\n",
      "ğŸ“Š CREATING STRATIFIED SAMPLING STRATA\n",
      "==================================================\n",
      "   ğŸ“… Temporal split at timestamp: 1487980800.0\n",
      "   ğŸ‘¥ User activity split at: 66.0 playlists per user\n",
      "   ğŸ“‹ Strata Distribution:\n",
      "      â€¢ short_old_casual    :  2,916 ( 0.4%)\n",
      "      â€¢ short_old_active    : 152,095 (20.4%)\n",
      "      â€¢ short_recent_casual :  2,525 ( 0.3%)\n",
      "      â€¢ short_recent_active : 114,527 (15.4%)\n",
      "      â€¢ medium_old_casual   :  2,181 ( 0.3%)\n",
      "      â€¢ medium_old_active   : 133,183 (17.9%)\n",
      "      â€¢ medium_recent_casual:  2,730 ( 0.4%)\n",
      "      â€¢ medium_recent_active: 140,865 (18.9%)\n",
      "      â€¢ long_old_casual     :  1,258 ( 0.2%)\n",
      "      â€¢ long_old_active     : 81,260 (10.9%)\n",
      "      â€¢ long_recent_casual  :  1,948 ( 0.3%)\n",
      "      â€¢ long_recent_active  : 110,459 (14.8%)\n",
      "\n",
      "ğŸ’¾ Memory usage after strata created: 4746.4 MB\n",
      "ğŸ² PASS 2: STRATIFIED SAMPLING WITH PRIORITY SCORING (SCALED)\n",
      "============================================================\n",
      "   ğŸ“Š Global sampling ratio: 0.003\n",
      "   ğŸ† Using priority scoring within strata\n",
      "\n",
      "      â€¢ short_old_casual    :    9 / 2,916 (avg score: 10.57)\n",
      "      â€¢ short_old_active    :  509 / 152,095 (avg score: 11.17)\n",
      "      â€¢ short_recent_casual :    8 / 2,525 (avg score: 11.18)\n",
      "      â€¢ short_recent_active :  383 / 114,527 (avg score: 11.03)\n",
      "      â€¢ medium_old_casual   :    7 / 2,181 (avg score: 10.59)\n",
      "      â€¢ medium_old_active   :  446 / 133,183 (avg score: 11.51)\n",
      "      â€¢ medium_recent_casual:    9 / 2,730 (avg score: 10.51)\n",
      "      â€¢ medium_recent_active:  472 / 140,865 (avg score: 11.43)\n",
      "      â€¢ long_old_casual     :    4 / 1,258 (avg score: 11.93)\n",
      "      â€¢ long_old_active     :  272 / 81,260 (avg score: 11.42)\n",
      "      â€¢ long_recent_casual  :    6 / 1,948 (avg score: 11.33)\n",
      "      â€¢ long_recent_active  :  370 / 110,459 (avg score: 11.26)\n",
      "\n",
      "   ğŸ¯ Selected 2,495 playlists for final loading\n",
      "   ğŸ“ Loading selected playlists...\n",
      "   ğŸ“‚ Loading from 904 files\n",
      "      ğŸ“– File 1/904\n",
      "      ğŸ“– File 101/904\n",
      "      ğŸ“– File 201/904\n",
      "      ğŸ“– File 301/904\n",
      "      ğŸ“– File 401/904\n",
      "      ğŸ“– File 501/904\n",
      "      ğŸ“– File 601/904\n",
      "      ğŸ“– File 701/904\n",
      "      ğŸ“– File 801/904\n",
      "      ğŸ“– File 901/904\n",
      "   âœ… Loaded 2,495 final playlists\n",
      "ğŸ’¾ Memory usage after pass 2 complete: 2152.4 MB\n",
      "\n",
      "ğŸ” VERIFYING FINAL SCALE FOR EXPERIMENTAL CONTROL\n",
      "==================================================\n",
      "   ğŸ“Š Final Entity Counts:\n",
      "      â€¢ Playlists: 2,495 (target: 2,500)\n",
      "      â€¢ Tracks: 58,160 (target: ~3,500)\n",
      "      â€¢ Artists: 17,069 (target: ~800)\n",
      "      â€¢ Albums: 34,430 (target: ~600)\n",
      "      â€¢ Users: 279 (target: ~100)\n",
      "      ğŸ¯ TOTAL: 112,433 (target: ~7,500)\n",
      "\n",
      "ğŸ‰ SCALED HYBRID SAMPLING COMPLETE!\n",
      "======================================================================\n",
      "ğŸ“Š Results: 1,000,000 â†’ 2,495 playlists\n",
      "ğŸ“ˆ Overall retention: 0.2%\n",
      "\n",
      "ğŸ¯ EXPERIMENTAL SCALE VERIFICATION:\n",
      "   â€¢ Actual total nodes: 112,433\n",
      "   â€¢ Target total nodes: 7,500\n",
      "   â€¢ Scale ratio: 14.991 (âš ï¸ ADJUST)\n",
      "\n",
      "ğŸ’¾ Saved to: ../data/processed/spotify_scaled_hybrid_7500.json\n",
      "ğŸ“¦ File size: 42.7 MB\n",
      "\n",
      "ğŸ“ READY FOR CONTROLLED EXPERIMENTS:\n",
      "   âœ… Methodology: YOUR ORIGINAL hybrid approach\n",
      "   âœ… Scale: ~112,433 nodes\n",
      "   âœ… Training time estimate: 3-5 minutes per configuration\n",
      "   âœ… Same methodology across all 3 experiments\n",
      "   âœ… Controlled conditions achieved\n",
      "\n",
      "======================================================================\n",
      "ğŸ‰ METHODOLOGY COMPARISON:\n",
      "======================================================================\n",
      "ORIGINAL HYBRID METHOD:\n",
      "  â€¢ Target: 50,000 playlists â†’ 661k+ nodes\n",
      "  â€¢ Training: 34-60 minutes per config\n",
      "  â€¢ Total experiment time: 4-6 hours\n",
      "\n",
      "SCALED HYBRID METHOD (THIS OUTPUT):\n",
      "  â€¢ Target: 2,500 playlists â†’ ~112,433 nodes\n",
      "  â€¢ Training: 3-5 minutes per config\n",
      "  â€¢ Total experiment time: 40-65 minutes\n",
      "\n",
      "ğŸ¯ SAME METHODOLOGY, CONTROLLED SCALE!\n",
      "   âœ… Identical core-based filtering logic\n",
      "   âœ… Identical stratified sampling approach\n",
      "   âœ… Identical priority scoring system\n",
      "   âœ… Perfect for experimental consistency\n",
      "\n",
      "ğŸ“‹ PARAMETER ADJUSTMENTS FOR SCALE CONTROL:\n",
      "   â€¢ min_track_frequency: 5 â†’ 8 (stricter core filtering)\n",
      "   â€¢ min_user_playlists: 3 â†’ 10 (better user consolidation)\n",
      "   â€¢ target_playlists: 50,000 â†’ 2,500 (experimental scale)\n",
      "   â€¢ User ID extraction: more aggressive consolidation\n",
      "\n",
      "ğŸ”¬ EXPERIMENTAL BENEFITS:\n",
      "   âœ… Can run all 13 configurations in ~1 hour\n",
      "   âœ… Same methodology across Experiments 1, 2, and 3\n",
      "   âœ… Manageable memory usage (~1-2GB vs 4-6GB)\n",
      "   âœ… Statistical validity maintained\n",
      "   âœ… Perfect for thesis experimental design\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
