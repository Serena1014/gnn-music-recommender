{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Preprocessing",
   "id": "afd3068e6260650f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-30T12:21:36.305857Z",
     "start_time": "2025-07-30T12:21:36.302919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pickle\n",
    "import os\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:21:37.554034Z",
     "start_time": "2025-07-30T12:21:37.549317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ğŸµ GNN DATA PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ Goal: Transform sampled playlists into GNN-ready graph structure\")\n",
    "print(\"ğŸ“Š Input: Sampled playlist JSON file\")\n",
    "print(\"ğŸ“ˆ Output: Node mappings, edge lists, features, train/val/test splits\")\n",
    "print(\"ğŸ“‹ Split Ratio: 70% train / 15% validation / 15% test\")\n",
    "print()"
   ],
   "id": "77cb3192e29bc70a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸµ GNN DATA PREPROCESSING PIPELINE\n",
      "============================================================\n",
      "ğŸ¯ Goal: Transform sampled playlists into GNN-ready graph structure\n",
      "ğŸ“Š Input: Sampled playlist JSON file\n",
      "ğŸ“ˆ Output: Node mappings, edge lists, features, train/val/test splits\n",
      "ğŸ“‹ Split Ratio: 70% train / 15% validation / 15% test\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Load and Explore Sampled Data",
   "id": "7564e7d7c5851bd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:21:39.388134Z",
     "start_time": "2025-07-30T12:21:39.377256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_sampled_data(file_path: str) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Load the sampled dataset\"\"\"\n",
    "    print(f\"ğŸ“‚ Loading sampled data from: {file_path}\")\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    playlists = data.get('playlists', [])\n",
    "    info = data.get('info', {})\n",
    "    sampling_stats = data.get('sampling_stats', {})\n",
    "\n",
    "    print(f\"âœ… Loaded {len(playlists):,} playlists\")\n",
    "    print(f\"ğŸ“Š Sampling method: {info.get('sampling_method', 'unknown')}\")\n",
    "\n",
    "    return playlists, {'info': info, 'sampling_stats': sampling_stats}\n",
    "\n",
    "def explore_data_structure(playlists: List[Dict]) -> Dict:\n",
    "    \"\"\"Explore the structure of sampled data\"\"\"\n",
    "    print(\"ğŸ” EXPLORING DATA STRUCTURE\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Basic statistics\n",
    "    total_playlists = len(playlists)\n",
    "    playlist_lengths = [len(p.get('tracks', [])) for p in playlists]\n",
    "\n",
    "    # Extract all entities\n",
    "    all_tracks = set()\n",
    "    all_artists = set()\n",
    "    all_albums = set()\n",
    "    user_playlist_count = Counter()\n",
    "\n",
    "    for playlist in playlists:\n",
    "        # Extract user ID (simplified)\n",
    "        name = playlist.get('name', '').strip()\n",
    "        user_id = name.split()[0] if name else f\"user_{playlist.get('pid', 0) % 1000}\"\n",
    "        user_playlist_count[user_id] += 1\n",
    "\n",
    "        # Extract track info\n",
    "        tracks = playlist.get('tracks', [])\n",
    "        for track in tracks:\n",
    "            track_uri = track.get('track_uri', '')\n",
    "            artist_uri = track.get('artist_uri', '')\n",
    "            album_uri = track.get('album_uri', '')\n",
    "\n",
    "            if track_uri:\n",
    "                all_tracks.add(track_uri)\n",
    "            if artist_uri:\n",
    "                all_artists.add(artist_uri)\n",
    "            if album_uri:\n",
    "                all_albums.add(album_uri)\n",
    "\n",
    "    stats = {\n",
    "        'num_playlists': total_playlists,\n",
    "        'num_unique_tracks': len(all_tracks),\n",
    "        'num_unique_artists': len(all_artists),\n",
    "        'num_unique_albums': len(all_albums),\n",
    "        'num_unique_users': len(user_playlist_count),\n",
    "        'avg_playlist_length': np.mean(playlist_lengths),\n",
    "        'min_playlist_length': min(playlist_lengths),\n",
    "        'max_playlist_length': max(playlist_lengths),\n",
    "        'total_track_occurrences': sum(playlist_lengths)\n",
    "    }\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"ğŸ“Š Dataset Statistics:\")\n",
    "    print(f\"   â€¢ Playlists: {stats['num_playlists']:,}\")\n",
    "    print(f\"   â€¢ Unique tracks: {stats['num_unique_tracks']:,}\")\n",
    "    print(f\"   â€¢ Unique artists: {stats['num_unique_artists']:,}\")\n",
    "    print(f\"   â€¢ Unique albums: {stats['num_unique_albums']:,}\")\n",
    "    print(f\"   â€¢ Unique users: {stats['num_unique_users']:,}\")\n",
    "    print(f\"   â€¢ Avg playlist length: {stats['avg_playlist_length']:.1f}\")\n",
    "    print(f\"   â€¢ Playlist length range: {stats['min_playlist_length']}-{stats['max_playlist_length']}\")\n",
    "    print(f\"   â€¢ Total track occurrences: {stats['total_track_occurrences']:,}\")\n",
    "    print()\n",
    "\n",
    "    return stats"
   ],
   "id": "e570e9e60bf98cc8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:21:43.349030Z",
     "start_time": "2025-07-30T12:21:42.949051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sampled_data_path = \"../data/processed/spotify_scaled_hybrid_7500.json\"\n",
    "\n",
    "playlists, metadata = load_sampled_data(sampled_data_path)\n",
    "data_stats = explore_data_structure(playlists)"
   ],
   "id": "5f0ebbfbe291c63a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Loading sampled data from: ../data/processed/spotify_scaled_hybrid_7500.json\n",
      "âœ… Loaded 2,495 playlists\n",
      "ğŸ“Š Sampling method: scaled_hybrid_core_based_stratified_streaming\n",
      "ğŸ” EXPLORING DATA STRUCTURE\n",
      "========================================\n",
      "ğŸ“Š Dataset Statistics:\n",
      "   â€¢ Playlists: 2,495\n",
      "   â€¢ Unique tracks: 58,160\n",
      "   â€¢ Unique artists: 17,069\n",
      "   â€¢ Unique albums: 34,430\n",
      "   â€¢ Unique users: 1,446\n",
      "   â€¢ Avg playlist length: 44.4\n",
      "   â€¢ Playlist length range: 10-100\n",
      "   â€¢ Total track occurrences: 110,785\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Create Entity Mappings",
   "id": "1e6d2a1c789c8604"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:21:50.512251Z",
     "start_time": "2025-07-30T12:21:50.501824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EntityMapper:\n",
    "    \"\"\"Create bidirectional mappings between entities and integer IDs\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mappings = {}\n",
    "        self.reverse_mappings = {}\n",
    "        self.entity_counts = {}\n",
    "\n",
    "    def create_mappings(self, playlists: List[Dict]) -> Dict:\n",
    "        \"\"\"Create mappings for all entities\"\"\"\n",
    "        print(\"ğŸ—ºï¸  CREATING ENTITY MAPPINGS\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Collect all entities\n",
    "        entities = {\n",
    "            'playlists': {},\n",
    "            'tracks': {},\n",
    "            'artists': {},\n",
    "            'albums': {},\n",
    "            'users': {}\n",
    "        }\n",
    "\n",
    "        # Extract entities from playlists\n",
    "        print(\"ğŸ“Š Extracting entities...\")\n",
    "\n",
    "        for playlist in playlists:\n",
    "            # Playlist ID\n",
    "            pid = playlist.get('pid')\n",
    "            if pid is not None:\n",
    "                entities['playlists'][str(pid)] = playlist\n",
    "\n",
    "            # User ID (simplified extraction)\n",
    "            name = playlist.get('name', '').strip()\n",
    "            user_id = name.split()[0] if name else f\"user_{pid % 1000}\"\n",
    "            entities['users'][user_id] = entities['users'].get(user_id, 0) + 1\n",
    "\n",
    "            # Track, artist, album info\n",
    "            tracks = playlist.get('tracks', [])\n",
    "            for track in tracks:\n",
    "                track_uri = track.get('track_uri', '')\n",
    "                artist_uri = track.get('artist_uri', '')\n",
    "                album_uri = track.get('album_uri', '')\n",
    "\n",
    "                if track_uri:\n",
    "                    entities['tracks'][track_uri] = track\n",
    "                if artist_uri:\n",
    "                    entities['artists'][artist_uri] = {\n",
    "                        'artist_name': track.get('artist_name', ''),\n",
    "                        'artist_uri': artist_uri\n",
    "                    }\n",
    "                if album_uri:\n",
    "                    entities['albums'][album_uri] = {\n",
    "                        'album_name': track.get('album_name', ''),\n",
    "                        'album_uri': album_uri\n",
    "                    }\n",
    "\n",
    "        # Create integer mappings\n",
    "        print(\"ğŸ”¢ Creating integer ID mappings...\")\n",
    "\n",
    "        for entity_type, entity_dict in entities.items():\n",
    "            entity_list = list(entity_dict.keys())\n",
    "\n",
    "            # Create forward mapping (entity -> int)\n",
    "            self.mappings[entity_type] = {\n",
    "                entity: idx for idx, entity in enumerate(entity_list)\n",
    "            }\n",
    "\n",
    "            # Create reverse mapping (int -> entity)\n",
    "            self.reverse_mappings[entity_type] = {\n",
    "                idx: entity for entity, idx in self.mappings[entity_type].items()\n",
    "            }\n",
    "\n",
    "            # Store counts\n",
    "            self.entity_counts[entity_type] = len(entity_list)\n",
    "\n",
    "            print(f\"   âœ… {entity_type}: {len(entity_list):,} entities\")\n",
    "\n",
    "        print()\n",
    "        return self.mappings\n",
    "\n",
    "    def get_mapping_stats(self) -> Dict:\n",
    "        \"\"\"Get statistics about the mappings\"\"\"\n",
    "        return {\n",
    "            'entity_counts': self.entity_counts,\n",
    "            'total_nodes': sum(self.entity_counts.values())\n",
    "        }"
   ],
   "id": "cd83fd2f740e144c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:21:55.159340Z",
     "start_time": "2025-07-30T12:21:55.025008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create entity mappings\n",
    "mapper = EntityMapper()\n",
    "mappings = mapper.create_mappings(playlists)\n",
    "mapping_stats = mapper.get_mapping_stats()\n",
    "\n",
    "print(f\"ğŸ“ˆ Total graph nodes: {mapping_stats['total_nodes']:,}\")"
   ],
   "id": "c064d1d5461ebe65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ºï¸  CREATING ENTITY MAPPINGS\n",
      "========================================\n",
      "ğŸ“Š Extracting entities...\n",
      "ğŸ”¢ Creating integer ID mappings...\n",
      "   âœ… playlists: 2,495 entities\n",
      "   âœ… tracks: 58,160 entities\n",
      "   âœ… artists: 17,069 entities\n",
      "   âœ… albums: 34,430 entities\n",
      "   âœ… users: 1,446 entities\n",
      "\n",
      "ğŸ“ˆ Total graph nodes: 113,600\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Build Graph Edges",
   "id": "528db56ea60b39a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:21:58.400991Z",
     "start_time": "2025-07-30T12:21:58.389248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraphBuilder:\n",
    "    \"\"\"Build edges for the music recommendation graph\"\"\"\n",
    "\n",
    "    def __init__(self, mappings: Dict, entity_counts: Dict):\n",
    "        self.mappings = mappings\n",
    "        self.entity_counts = entity_counts\n",
    "        self.edges = {}\n",
    "\n",
    "    def build_edges(self, playlists: List[Dict]) -> Dict:\n",
    "        \"\"\"Build all edge types for the graph\"\"\"\n",
    "        print(\"ğŸ”— BUILDING GRAPH EDGES\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Initialize edge lists\n",
    "        self.edges = {\n",
    "            'playlist_track': [],      # Playlist contains track\n",
    "            'track_artist': [],        # Track by artist\n",
    "            'track_album': [],         # Track in album\n",
    "            'user_playlist': [],       # User created playlist\n",
    "            'playlist_user': []        # Reverse of user_playlist\n",
    "        }\n",
    "\n",
    "        print(\"ğŸ”— Extracting relationships...\")\n",
    "\n",
    "        for playlist in playlists:\n",
    "            pid = playlist.get('pid')\n",
    "            playlist_name = playlist.get('name', '').strip()\n",
    "\n",
    "            # Get mapped IDs\n",
    "            playlist_id = self.mappings['playlists'].get(str(pid))\n",
    "            if playlist_id is None:\n",
    "                continue\n",
    "\n",
    "            # Extract user\n",
    "            user_name = playlist_name.split()[0] if playlist_name else f\"user_{pid % 1000}\"\n",
    "            user_id = self.mappings['users'].get(user_name)\n",
    "\n",
    "            # User-Playlist edges\n",
    "            if user_id is not None:\n",
    "                self.edges['user_playlist'].append([user_id, playlist_id])\n",
    "                self.edges['playlist_user'].append([playlist_id, user_id])\n",
    "\n",
    "            # Process tracks\n",
    "            tracks = playlist.get('tracks', [])\n",
    "            for track in tracks:\n",
    "                track_uri = track.get('track_uri', '')\n",
    "                artist_uri = track.get('artist_uri', '')\n",
    "                album_uri = track.get('album_uri', '')\n",
    "\n",
    "                track_id = self.mappings['tracks'].get(track_uri)\n",
    "\n",
    "                if track_id is not None:\n",
    "                    # Playlist-Track edge\n",
    "                    self.edges['playlist_track'].append([playlist_id, track_id])\n",
    "\n",
    "                    # Track-Artist edge\n",
    "                    artist_id = self.mappings['artists'].get(artist_uri)\n",
    "                    if artist_id is not None:\n",
    "                        self.edges['track_artist'].append([track_id, artist_id])\n",
    "\n",
    "                    # Track-Album edge\n",
    "                    album_id = self.mappings['albums'].get(album_uri)\n",
    "                    if album_id is not None:\n",
    "                        self.edges['track_album'].append([track_id, album_id])\n",
    "\n",
    "        # Convert to numpy arrays and remove duplicates\n",
    "        print(\"ğŸ”§ Processing edge lists...\")\n",
    "\n",
    "        for edge_type, edge_list in self.edges.items():\n",
    "            if edge_list:\n",
    "                # Convert to numpy array\n",
    "                edges_array = np.array(edge_list)\n",
    "\n",
    "                # Remove duplicates\n",
    "                unique_edges = np.unique(edges_array, axis=0)\n",
    "\n",
    "                self.edges[edge_type] = unique_edges\n",
    "\n",
    "                print(f\"   âœ… {edge_type}: {len(unique_edges):,} edges\")\n",
    "            else:\n",
    "                self.edges[edge_type] = np.array([]).reshape(0, 2)\n",
    "                print(f\"   âš ï¸  {edge_type}: 0 edges\")\n",
    "\n",
    "        print()\n",
    "        return self.edges\n",
    "\n",
    "    def get_graph_statistics(self) -> Dict:\n",
    "        \"\"\"Calculate graph statistics\"\"\"\n",
    "        stats = {}\n",
    "\n",
    "        for edge_type, edges in self.edges.items():\n",
    "            stats[edge_type] = {\n",
    "                'num_edges': len(edges),\n",
    "                'density': len(edges) / (self.entity_counts.get('playlists', 1) * self.entity_counts.get('tracks', 1)) if 'playlist' in edge_type and 'track' in edge_type else 0\n",
    "            }\n",
    "\n",
    "        return stats"
   ],
   "id": "ecbf5c1b988b6ed9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:03.217507Z",
     "start_time": "2025-07-30T12:22:02.621682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_builder = GraphBuilder(mappings, mapper.entity_counts)\n",
    "edges = graph_builder.build_edges(playlists)\n",
    "graph_stats = graph_builder.get_graph_statistics()\n",
    "\n",
    "# Print graph statistics\n",
    "print(\"ğŸ“Š Graph Statistics:\")\n",
    "for edge_type, stats in graph_stats.items():\n",
    "    print(f\"   â€¢ {edge_type}: {stats['num_edges']:,} edges\")"
   ],
   "id": "44a6b66b2a1aa521",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— BUILDING GRAPH EDGES\n",
      "========================================\n",
      "ğŸ”— Extracting relationships...\n",
      "ğŸ”§ Processing edge lists...\n",
      "   âœ… playlist_track: 109,716 edges\n",
      "   âœ… track_artist: 58,160 edges\n",
      "   âœ… track_album: 58,160 edges\n",
      "   âœ… user_playlist: 2,495 edges\n",
      "   âœ… playlist_user: 2,495 edges\n",
      "\n",
      "ğŸ“Š Graph Statistics:\n",
      "   â€¢ playlist_track: 109,716 edges\n",
      "   â€¢ track_artist: 58,160 edges\n",
      "   â€¢ track_album: 58,160 edges\n",
      "   â€¢ user_playlist: 2,495 edges\n",
      "   â€¢ playlist_user: 2,495 edges\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Node Features",
   "id": "5219ebfa5bd8c80a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:06.775602Z",
     "start_time": "2025-07-30T12:22:06.757109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"Extract features for graph nodes\"\"\"\n",
    "\n",
    "    def __init__(self, mappings: Dict, entity_counts: Dict):\n",
    "        self.mappings = mappings\n",
    "        self.entity_counts = entity_counts\n",
    "        self.features = {}\n",
    "\n",
    "    def extract_playlist_features(self, playlists: List[Dict]) -> np.ndarray:\n",
    "        \"\"\"Extract features for playlist nodes\"\"\"\n",
    "        print(\"ğŸµ Extracting playlist features...\")\n",
    "\n",
    "        num_playlists = self.entity_counts['playlists']\n",
    "\n",
    "        # Feature dimensions\n",
    "        features = []\n",
    "\n",
    "        # Create mapping from PID to playlist data\n",
    "        pid_to_playlist = {str(p.get('pid')): p for p in playlists}\n",
    "\n",
    "        for playlist_idx in range(num_playlists):\n",
    "            # Get original playlist ID\n",
    "            original_pid = mapper.reverse_mappings['playlists'][playlist_idx]\n",
    "            playlist_data = pid_to_playlist.get(original_pid, {})\n",
    "\n",
    "            # Extract features\n",
    "            playlist_features = []\n",
    "\n",
    "            # Basic features\n",
    "            num_tracks = len(playlist_data.get('tracks', []))\n",
    "            num_followers = playlist_data.get('num_followers', 0)\n",
    "            is_collaborative = 1 if playlist_data.get('collaborative', False) else 0\n",
    "\n",
    "            # Temporal features\n",
    "            modified_at = playlist_data.get('modified_at', 0)\n",
    "            # Normalize timestamp (simple approach)\n",
    "            normalized_time = (modified_at - 1400000000) / 100000000 if modified_at > 0 else 0\n",
    "\n",
    "            # Text features (simple)\n",
    "            name = playlist_data.get('name', '')\n",
    "            has_name = 1 if len(name.strip()) > 0 else 0\n",
    "            name_length = len(name.strip())\n",
    "\n",
    "            # Combine features\n",
    "            playlist_features = [\n",
    "                num_tracks,\n",
    "                np.log1p(num_followers),  # Log transform followers\n",
    "                is_collaborative,\n",
    "                normalized_time,\n",
    "                has_name,\n",
    "                name_length\n",
    "            ]\n",
    "\n",
    "            features.append(playlist_features)\n",
    "\n",
    "        features_array = np.array(features, dtype=np.float32)\n",
    "        print(f\"   âœ… Playlist features shape: {features_array.shape}\")\n",
    "\n",
    "        return features_array\n",
    "\n",
    "    def extract_track_features(self, playlists: List[Dict]) -> np.ndarray:\n",
    "        \"\"\"Extract features for track nodes\"\"\"\n",
    "        print(\"ğŸ¼ Extracting track features...\")\n",
    "\n",
    "        num_tracks = self.entity_counts['tracks']\n",
    "\n",
    "        # Track statistics\n",
    "        track_stats = defaultdict(lambda: {\n",
    "            'playlist_count': 0,\n",
    "            'total_position': 0,\n",
    "            'positions': [],\n",
    "            'durations': []\n",
    "        })\n",
    "\n",
    "        # Collect track statistics\n",
    "        for playlist in playlists:\n",
    "            tracks = playlist.get('tracks', [])\n",
    "            for pos, track in enumerate(tracks):\n",
    "                track_uri = track.get('track_uri', '')\n",
    "                duration = track.get('duration_ms', 0)\n",
    "\n",
    "                if track_uri:\n",
    "                    track_stats[track_uri]['playlist_count'] += 1\n",
    "                    track_stats[track_uri]['total_position'] += pos\n",
    "                    track_stats[track_uri]['positions'].append(pos)\n",
    "                    if duration > 0:\n",
    "                        track_stats[track_uri]['durations'].append(duration)\n",
    "\n",
    "        # Create feature matrix\n",
    "        features = []\n",
    "\n",
    "        for track_idx in range(num_tracks):\n",
    "            # Get original track URI\n",
    "            track_uri = mapper.reverse_mappings['tracks'][track_idx]\n",
    "            stats = track_stats[track_uri]\n",
    "\n",
    "            # Extract features\n",
    "            playlist_count = stats['playlist_count']\n",
    "            avg_position = stats['total_position'] / max(playlist_count, 1)\n",
    "            position_std = np.std(stats['positions']) if stats['positions'] else 0\n",
    "            avg_duration = np.mean(stats['durations']) if stats['durations'] else 180000  # Default 3 min\n",
    "\n",
    "            track_features = [\n",
    "                np.log1p(playlist_count),  # Log of popularity\n",
    "                avg_position,              # Average position in playlists\n",
    "                position_std,              # Position variability\n",
    "                avg_duration / 60000,      # Duration in minutes\n",
    "            ]\n",
    "\n",
    "            features.append(track_features)\n",
    "\n",
    "        features_array = np.array(features, dtype=np.float32)\n",
    "        print(f\"   âœ… Track features shape: {features_array.shape}\")\n",
    "\n",
    "        return features_array\n",
    "\n",
    "    def extract_user_features(self, playlists: List[Dict]) -> np.ndarray:\n",
    "        \"\"\"Extract features for user nodes\"\"\"\n",
    "        print(\"ğŸ‘¥ Extracting user features...\")\n",
    "\n",
    "        num_users = self.entity_counts['users']\n",
    "\n",
    "        # User statistics\n",
    "        user_stats = defaultdict(lambda: {\n",
    "            'playlist_count': 0,\n",
    "            'total_tracks': 0,\n",
    "            'unique_tracks': set(),\n",
    "            'collaborative_count': 0\n",
    "        })\n",
    "\n",
    "        # Collect user statistics\n",
    "        for playlist in playlists:\n",
    "            name = playlist.get('name', '').strip()\n",
    "            user_name = name.split()[0] if name else f\"user_{playlist.get('pid', 0) % 1000}\"\n",
    "\n",
    "            tracks = playlist.get('tracks', [])\n",
    "            is_collaborative = playlist.get('collaborative', False)\n",
    "\n",
    "            user_stats[user_name]['playlist_count'] += 1\n",
    "            user_stats[user_name]['total_tracks'] += len(tracks)\n",
    "            user_stats[user_name]['unique_tracks'].update([t.get('track_uri', '') for t in tracks])\n",
    "            if is_collaborative:\n",
    "                user_stats[user_name]['collaborative_count'] += 1\n",
    "\n",
    "        # Create feature matrix\n",
    "        features = []\n",
    "\n",
    "        for user_idx in range(num_users):\n",
    "            # Get original user name\n",
    "            user_name = mapper.reverse_mappings['users'][user_idx]\n",
    "            stats = user_stats[user_name]\n",
    "\n",
    "            # Extract features\n",
    "            playlist_count = stats['playlist_count']\n",
    "            avg_playlist_length = stats['total_tracks'] / max(playlist_count, 1)\n",
    "            unique_track_count = len(stats['unique_tracks'])\n",
    "            collaborative_ratio = stats['collaborative_count'] / max(playlist_count, 1)\n",
    "\n",
    "            user_features = [\n",
    "                np.log1p(playlist_count),     # Log of activity level\n",
    "                avg_playlist_length,          # Average playlist length\n",
    "                np.log1p(unique_track_count), # Log of music diversity\n",
    "                collaborative_ratio           # Collaboration tendency\n",
    "            ]\n",
    "\n",
    "            features.append(user_features)\n",
    "\n",
    "        features_array = np.array(features, dtype=np.float32)\n",
    "        print(f\"   âœ… User features shape: {features_array.shape}\")\n",
    "\n",
    "        return features_array\n",
    "\n",
    "    def extract_all_features(self, playlists: List[Dict]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Extract features for all node types\"\"\"\n",
    "        print(\"ğŸ¨ EXTRACTING NODE FEATURES\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # Extract features for each node type\n",
    "        features['playlist'] = self.extract_playlist_features(playlists)\n",
    "        features['track'] = self.extract_track_features(playlists)\n",
    "        features['user'] = self.extract_user_features(playlists)\n",
    "\n",
    "        # Simple features for artists and albums (placeholder)\n",
    "        features['artist'] = np.random.randn(self.entity_counts['artists'], 4).astype(np.float32)\n",
    "        features['album'] = np.random.randn(self.entity_counts['albums'], 4).astype(np.float32)\n",
    "\n",
    "        print(f\"   âš ï¸  Artist/Album features: Using random placeholders\")\n",
    "        print()\n",
    "\n",
    "        return features"
   ],
   "id": "5b01aa0c7a721f27",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:12.773085Z",
     "start_time": "2025-07-30T12:22:11.054208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_extractor = FeatureExtractor(mappings, mapper.entity_counts)\n",
    "node_features = feature_extractor.extract_all_features(playlists)"
   ],
   "id": "806f217a5060197e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¨ EXTRACTING NODE FEATURES\n",
      "========================================\n",
      "ğŸµ Extracting playlist features...\n",
      "   âœ… Playlist features shape: (2495, 6)\n",
      "ğŸ¼ Extracting track features...\n",
      "   âœ… Track features shape: (58160, 4)\n",
      "ğŸ‘¥ Extracting user features...\n",
      "   âœ… User features shape: (1446, 4)\n",
      "   âš ï¸  Artist/Album features: Using random placeholders\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Train/Validation/Test Splits (70/15/15)",
   "id": "5f8cfe9145296e87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:18.051595Z",
     "start_time": "2025-07-30T12:22:18.040328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataSplitter:\n",
    "    \"\"\"Create train/validation/test splits for the recommendation task\"\"\"\n",
    "\n",
    "    def __init__(self, edges: Dict, mappings: Dict):\n",
    "        self.edges = edges\n",
    "        self.mappings = mappings\n",
    "\n",
    "    def create_playlist_track_splits(self, train_ratio: float = 0.7,\n",
    "                                   val_ratio: float = 0.15,\n",
    "                                   test_ratio: float = 0.15) -> Dict:\n",
    "        \"\"\"Create splits for playlist-track edges with 70/15/15 ratio\"\"\"\n",
    "        print(\"âœ‚ï¸  CREATING TRAIN/VALIDATION/TEST SPLITS (70/15/15)\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Get playlist-track edges\n",
    "        playlist_track_edges = self.edges['playlist_track']\n",
    "        num_edges = len(playlist_track_edges)\n",
    "\n",
    "        print(f\"ğŸ“Š Total playlist-track edges: {num_edges:,}\")\n",
    "        print(f\"ğŸ“Š Split ratios: {train_ratio:.0%} train, {val_ratio:.0%} val, {test_ratio:.0%} test\")\n",
    "        print(f\"ğŸ“Š This follows the standard Graph ML practice for robust evaluation\")\n",
    "\n",
    "        # Shuffle edges\n",
    "        indices = np.random.permutation(num_edges)\n",
    "\n",
    "        # Calculate split sizes\n",
    "        train_size = int(num_edges * train_ratio)\n",
    "        val_size = int(num_edges * val_ratio)\n",
    "        test_size = num_edges - train_size - val_size\n",
    "\n",
    "        print(f\"ğŸ“ˆ Calculated split sizes:\")\n",
    "        print(f\"   â€¢ Train: {train_size:,} edges ({train_size/num_edges:.1%})\")\n",
    "        print(f\"   â€¢ Validation: {val_size:,} edges ({val_size/num_edges:.1%})\")\n",
    "        print(f\"   â€¢ Test: {test_size:,} edges ({test_size/num_edges:.1%})\")\n",
    "\n",
    "        # Create splits\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "\n",
    "        splits = {\n",
    "            'train_edges': playlist_track_edges[train_indices],\n",
    "            'val_edges': playlist_track_edges[val_indices],\n",
    "            'test_edges': playlist_track_edges[test_indices],\n",
    "            'train_indices': train_indices,\n",
    "            'val_indices': val_indices,\n",
    "            'test_indices': test_indices,\n",
    "            'split_ratios': {\n",
    "                'train': train_ratio,\n",
    "                'val': val_ratio,\n",
    "                'test': test_ratio\n",
    "            }\n",
    "        }\n",
    "\n",
    "        print(f\"âœ… Final split sizes:\")\n",
    "        print(f\"   â€¢ Train edges: {len(splits['train_edges']):,}\")\n",
    "        print(f\"   â€¢ Validation edges: {len(splits['val_edges']):,}\")\n",
    "        print(f\"   â€¢ Test edges: {len(splits['test_edges']):,}\")\n",
    "        print()\n",
    "\n",
    "        return splits\n",
    "\n",
    "    def create_negative_samples(self, positive_edges: np.ndarray,\n",
    "                              num_playlists: int, num_tracks: int,\n",
    "                              num_negative: int = None) -> np.ndarray:\n",
    "        \"\"\"Create negative samples for link prediction\"\"\"\n",
    "        if num_negative is None:\n",
    "            num_negative = len(positive_edges)\n",
    "\n",
    "        # Create set of positive edges for efficient lookup\n",
    "        positive_set = set(map(tuple, positive_edges))\n",
    "\n",
    "        # Sample negative edges\n",
    "        negative_edges = []\n",
    "        max_attempts = num_negative * 10  # Prevent infinite loop\n",
    "        attempts = 0\n",
    "\n",
    "        while len(negative_edges) < num_negative and attempts < max_attempts:\n",
    "            # Random playlist and track\n",
    "            playlist_id = np.random.randint(0, num_playlists)\n",
    "            track_id = np.random.randint(0, num_tracks)\n",
    "\n",
    "            # Check if this is not a positive edge\n",
    "            if (playlist_id, track_id) not in positive_set:\n",
    "                negative_edges.append([playlist_id, track_id])\n",
    "\n",
    "            attempts += 1\n",
    "\n",
    "        return np.array(negative_edges)\n"
   ],
   "id": "c52761b0db7bad0b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:21.458641Z",
     "start_time": "2025-07-30T12:22:21.264596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "splitter = DataSplitter(edges, mappings)\n",
    "splits = splitter.create_playlist_track_splits(train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "\n",
    "# Create negative samples for evaluation\n",
    "print(\"ğŸ”„ Creating negative samples for evaluation...\")\n",
    "negative_val = splitter.create_negative_samples(\n",
    "    splits['val_edges'],\n",
    "    mapper.entity_counts['playlists'],\n",
    "    mapper.entity_counts['tracks'],\n",
    "    num_negative=len(splits['val_edges'])\n",
    ")\n",
    "\n",
    "negative_test = splitter.create_negative_samples(\n",
    "    splits['test_edges'],\n",
    "    mapper.entity_counts['playlists'],\n",
    "    mapper.entity_counts['tracks'],\n",
    "    num_negative=len(splits['test_edges'])\n",
    ")\n",
    "\n",
    "print(f\"âœ… Negative validation samples: {len(negative_val):,}\")\n",
    "print(f\"âœ… Negative test samples: {len(negative_test):,}\")\n",
    "print(f\"ğŸ’¡ Negative samples ensure balanced evaluation (1:1 pos:neg ratio)\")"
   ],
   "id": "456891f96a53c1b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸  CREATING TRAIN/VALIDATION/TEST SPLITS (70/15/15)\n",
      "==================================================\n",
      "ğŸ“Š Total playlist-track edges: 109,716\n",
      "ğŸ“Š Split ratios: 70% train, 15% val, 15% test\n",
      "ğŸ“Š This follows the standard Graph ML practice for robust evaluation\n",
      "ğŸ“ˆ Calculated split sizes:\n",
      "   â€¢ Train: 76,801 edges (70.0%)\n",
      "   â€¢ Validation: 16,457 edges (15.0%)\n",
      "   â€¢ Test: 16,458 edges (15.0%)\n",
      "âœ… Final split sizes:\n",
      "   â€¢ Train edges: 76,801\n",
      "   â€¢ Validation edges: 16,457\n",
      "   â€¢ Test edges: 16,458\n",
      "\n",
      "ğŸ”„ Creating negative samples for evaluation...\n",
      "âœ… Negative validation samples: 16,457\n",
      "âœ… Negative test samples: 16,458\n",
      "ğŸ’¡ Negative samples ensure balanced evaluation (1:1 pos:neg ratio)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save Preprocessed Data",
   "id": "6086ddefe8804e50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:25.241408Z",
     "start_time": "2025-07-30T12:22:25.233510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_preprocessed_data(output_dir: str, mappings: Dict, edges: Dict,\n",
    "                         features: Dict, splits: Dict,\n",
    "                         negative_val: np.ndarray, negative_test: np.ndarray,\n",
    "                         entity_counts: Dict) -> str:\n",
    "    \"\"\"Save all preprocessed data\"\"\"\n",
    "    print(\"ğŸ’¾ SAVING PREPROCESSED DATA\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save mappings\n",
    "    with open(f\"{output_dir}/mappings.pkl\", 'wb') as f:\n",
    "        pickle.dump(mappings, f)\n",
    "    print(f\"âœ… Saved mappings to {output_dir}/mappings.pkl\")\n",
    "\n",
    "    # Save entity counts\n",
    "    with open(f\"{output_dir}/entity_counts.pkl\", 'wb') as f:\n",
    "        pickle.dump(entity_counts, f)\n",
    "    print(f\"âœ… Saved entity counts to {output_dir}/entity_counts.pkl\")\n",
    "\n",
    "    # Save edges\n",
    "    np.savez(f\"{output_dir}/edges.npz\", **edges)\n",
    "    print(f\"âœ… Saved edges to {output_dir}/edges.npz\")\n",
    "\n",
    "    # Save features\n",
    "    np.savez(f\"{output_dir}/features.npz\", **features)\n",
    "    print(f\"âœ… Saved features to {output_dir}/features.npz\")\n",
    "\n",
    "    # Save splits\n",
    "    splits_with_negatives = {\n",
    "        **splits,\n",
    "        'negative_val': negative_val,\n",
    "        'negative_test': negative_test\n",
    "    }\n",
    "    np.savez(f\"{output_dir}/splits.npz\", **splits_with_negatives)\n",
    "    print(f\"âœ… Saved splits to {output_dir}/splits.npz\")\n",
    "\n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'entity_counts': entity_counts,\n",
    "        'feature_dimensions': {k: v.shape for k, v in features.items()},\n",
    "        'edge_counts': {k: len(v) for k, v in edges.items()},\n",
    "        'split_sizes': {\n",
    "            'train': len(splits['train_edges']),\n",
    "            'val': len(splits['val_edges']),\n",
    "            'test': len(splits['test_edges'])\n",
    "        },\n",
    "        'split_ratios': splits['split_ratios'],\n",
    "        'preprocessing_notes': {\n",
    "            'split_strategy': '70/15/15 ratio following Graph ML best practices',\n",
    "            'negative_sampling': '1:1 positive to negative ratio',\n",
    "            'edge_type_focus': 'playlist-track edges for recommendation task'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(f\"{output_dir}/metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"âœ… Saved metadata to {output_dir}/metadata.json\")\n",
    "\n",
    "    print(f\"\\nğŸ‰ All preprocessed data saved to: {output_dir}\")\n",
    "    return output_dir"
   ],
   "id": "8fdf1eb9fdc7434",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:29.125443Z",
     "start_time": "2025-07-30T12:22:29.078104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_directory = \"../data/processed/gnn_ready\"\n",
    "\n",
    "saved_path = save_preprocessed_data(\n",
    "    output_dir=output_directory,\n",
    "    mappings=mappings,\n",
    "    edges=edges,\n",
    "    features=node_features,\n",
    "    splits=splits,\n",
    "    negative_val=negative_val,\n",
    "    negative_test=negative_test,\n",
    "    entity_counts=mapper.entity_counts\n",
    ")"
   ],
   "id": "c302ed9b27f21e9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ SAVING PREPROCESSED DATA\n",
      "========================================\n",
      "âœ… Saved mappings to ../data/processed/gnn_ready/mappings.pkl\n",
      "âœ… Saved entity counts to ../data/processed/gnn_ready/entity_counts.pkl\n",
      "âœ… Saved edges to ../data/processed/gnn_ready/edges.npz\n",
      "âœ… Saved features to ../data/processed/gnn_ready/features.npz\n",
      "âœ… Saved splits to ../data/processed/gnn_ready/splits.npz\n",
      "âœ… Saved metadata to ../data/processed/gnn_ready/metadata.json\n",
      "\n",
      "ğŸ‰ All preprocessed data saved to: ../data/processed/gnn_ready\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Verification and Summary",
   "id": "3ea4f49b1730b33"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-30T12:22:44.004682Z",
     "start_time": "2025-07-30T12:22:43.997750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def verify_preprocessed_data(data_dir: str):\n",
    "    \"\"\"Verify the preprocessed data\"\"\"\n",
    "    print(\"âœ… VERIFICATION SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Load and check each file\n",
    "    files_to_check = [\n",
    "        'mappings.pkl',\n",
    "        'entity_counts.pkl',\n",
    "        'edges.npz',\n",
    "        'features.npz',\n",
    "        'splits.npz',\n",
    "        'metadata.json'\n",
    "    ]\n",
    "\n",
    "    for file_name in files_to_check:\n",
    "        file_path = f\"{data_dir}/{file_name}\"\n",
    "        if os.path.exists(file_path):\n",
    "            file_size = os.path.getsize(file_path) / 1024 / 1024  # MB\n",
    "            print(f\"âœ… {file_name}: {file_size:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"âŒ {file_name}: Missing!\")\n",
    "\n",
    "    # Load metadata and print summary\n",
    "    try:\n",
    "        with open(f\"{data_dir}/metadata.json\", 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        print(f\"\\nğŸ“Š PREPROCESSING SUMMARY:\")\n",
    "        print(f\"   ğŸµ Playlists: {metadata['entity_counts']['playlists']:,}\")\n",
    "        print(f\"   ğŸ¼ Tracks: {metadata['entity_counts']['tracks']:,}\")\n",
    "        print(f\"   ğŸ¤ Artists: {metadata['entity_counts']['artists']:,}\")\n",
    "        print(f\"   ğŸ’¿ Albums: {metadata['entity_counts']['albums']:,}\")\n",
    "        print(f\"   ğŸ‘¥ Users: {metadata['entity_counts']['users']:,}\")\n",
    "        print(f\"   ğŸ”— Total edges: {sum(metadata['edge_counts'].values()):,}\")\n",
    "        print(f\"   ğŸ“š Training edges: {metadata['split_sizes']['train']:,} ({metadata['split_ratios']['train']:.0%})\")\n",
    "        print(f\"   ğŸ” Validation edges: {metadata['split_sizes']['val']:,} ({metadata['split_ratios']['val']:.0%})\")\n",
    "        print(f\"   ğŸ§ª Test edges: {metadata['split_sizes']['test']:,} ({metadata['split_ratios']['test']:.0%})\")\n",
    "        print(f\"   âš–ï¸  Split strategy: {metadata['preprocessing_notes']['split_strategy']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not load metadata: {e}\")"
   ],
   "id": "2fa4d2f12197aef6",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T19:30:22.608033Z",
     "start_time": "2025-06-24T19:30:22.600504Z"
    }
   },
   "cell_type": "code",
   "source": "verify_preprocessed_data(output_directory)",
   "id": "d6fef959c972b54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VERIFICATION SUMMARY\n",
      "========================================\n",
      "âœ… mappings.pkl: 25.60 MB\n",
      "âœ… entity_counts.pkl: 0.00 MB\n",
      "âœ… edges.npz: 46.05 MB\n",
      "âœ… features.npz: 10.47 MB\n",
      "âœ… splits.npz: 60.53 MB\n",
      "âœ… metadata.json: 0.00 MB\n",
      "\n",
      "ğŸ“Š PREPROCESSING SUMMARY:\n",
      "   ğŸµ Playlists: 49,993\n",
      "   ğŸ¼ Tracks: 356,998\n",
      "   ğŸ¤ Artists: 72,209\n",
      "   ğŸ’¿ Albums: 171,695\n",
      "   ğŸ‘¥ Users: 10,430\n",
      "   ğŸ”— Total edges: 3,017,685\n",
      "   ğŸ“š Training edges: 1,542,592 (70%)\n",
      "   ğŸ” Validation edges: 330,555 (15%)\n",
      "   ğŸ§ª Test edges: 330,556 (15%)\n",
      "   âš–ï¸  Split strategy: 70/15/15 ratio following Graph ML best practices\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
