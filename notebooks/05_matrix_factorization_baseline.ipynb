{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-07T13:27:52.206641Z",
     "start_time": "2025-07-07T13:27:52.179155Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:28:30.930377Z",
     "start_time": "2025-07-07T13:28:30.918370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import only basic torch components to avoid version conflicts\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    # Avoid importing torch.optim to bypass the error\n",
    "    print(\"PyTorch imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"PyTorch import error: {e}\")\n",
    "    print(\"Please try: pip install torch==2.1.0 --force-reinstall\")"
   ],
   "id": "4c889aaa41817762",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch imported successfully\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T00:21:55.294386Z",
     "start_time": "2025-07-07T00:21:55.281380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Matrix Factorization Baseline for GNN Music Recommender\")\n",
    "print(\"=\" * 60)"
   ],
   "id": "83834413ffa8e566",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Factorization Baseline for GNN Music Recommender\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Data Loading and Preprocessing",
   "id": "c24baa2038b6bb7c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T13:30:23.620505Z",
     "start_time": "2025-07-07T13:30:08.166239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_spotify_data(file_path):\n",
    "    \"\"\"Load and preprocess Spotify dataset\"\"\"\n",
    "    print(\"Loading Spotify dataset...\")\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    interactions = []\n",
    "\n",
    "    for playlist in data['playlists']:\n",
    "        playlist_id = playlist['pid']\n",
    "        for track in playlist['tracks']:\n",
    "            interactions.append({\n",
    "                'playlist_id': playlist_id,\n",
    "                'track_uri': track['track_uri'],\n",
    "                'track_name': track['track_name'],\n",
    "                'artist_name': track['artist_name'],\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(interactions)\n",
    "    print(f\"Loaded {len(df)} interactions\")\n",
    "    return df\n",
    "\n",
    "# Load your data\n",
    "data_path = \"../data/processed/spotify_hybrid_sampled_50000.json\"\n",
    "df = load_spotify_data(data_path)\n"
   ],
   "id": "2b86eadb5f6bf596",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Spotify dataset...\n",
      "Loaded 2217713 interactions\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Create User-item Interaction Matrix",
   "id": "1aa77e761b11d314"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T21:52:56.779920Z",
     "start_time": "2025-07-08T21:52:18.021410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_interaction_matrix_simple(df, min_interactions=3):\n",
    "    \"\"\"Create interaction matrix - simplified version\"\"\"\n",
    "\n",
    "    # Filter\n",
    "    playlist_counts = df['playlist_id'].value_counts()\n",
    "    track_counts = df['track_uri'].value_counts()\n",
    "\n",
    "    valid_playlists = playlist_counts[playlist_counts >= min_interactions].index\n",
    "    valid_tracks = track_counts[track_counts >= min_interactions].index\n",
    "\n",
    "    df_filtered = df[\n",
    "        (df['playlist_id'].isin(valid_playlists)) &\n",
    "        (df['track_uri'].isin(valid_tracks))\n",
    "    ].copy()\n",
    "\n",
    "    print(f\"After filtering: {df_filtered['playlist_id'].nunique()} playlists, {df_filtered['track_uri'].nunique()} tracks\")\n",
    "\n",
    "    # Create mappings\n",
    "    playlist_encoder = LabelEncoder()\n",
    "    track_encoder = LabelEncoder()\n",
    "\n",
    "    df_filtered['playlist_idx'] = playlist_encoder.fit_transform(df_filtered['playlist_id'])\n",
    "    df_filtered['track_idx'] = track_encoder.fit_transform(df_filtered['track_uri'])\n",
    "\n",
    "    return df_filtered, playlist_encoder, track_encoder\n",
    "\n",
    "df_processed, playlist_encoder, track_encoder = create_interaction_matrix_simple(df)\n",
    "\n",
    "# Get dimensions\n",
    "n_users = df_processed['playlist_idx'].nunique()\n",
    "n_items = df_processed['track_idx'].nunique()\n",
    "print(f\"Matrix dimensions: {n_users} x {n_items}\")"
   ],
   "id": "5375f7244e74948c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 49863 playlists, 95365 tracks\n",
      "Matrix dimensions: 49863 x 95365\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Matrix Factorization Model",
   "id": "ae421b5d6beb7943"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T21:53:14.791489Z",
     "start_time": "2025-07-08T21:53:14.738628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleMatrixFactorization:\n",
    "    def __init__(self, n_users, n_items, n_factors=50, learning_rate=0.01, reg=0.01):\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_factors = n_factors\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg = reg\n",
    "\n",
    "        # Initialize embeddings\n",
    "        np.random.seed(42)\n",
    "        self.user_factors = np.random.normal(0, 0.1, (n_users, n_factors))\n",
    "        self.item_factors = np.random.normal(0, 0.1, (n_items, n_factors))\n",
    "        self.user_bias = np.zeros(n_users)\n",
    "        self.item_bias = np.zeros(n_items)\n",
    "        self.global_bias = 0.0\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"Predict rating for user-item pair\"\"\"\n",
    "        prediction = (self.global_bias +\n",
    "                     self.user_bias[user_id] +\n",
    "                     self.item_bias[item_id] +\n",
    "                     np.dot(self.user_factors[user_id], self.item_factors[item_id]))\n",
    "        return 1.0 / (1.0 + np.exp(-prediction))  # Sigmoid\n",
    "\n",
    "    def train_step(self, user_id, item_id, rating):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # Get prediction\n",
    "        pred = self.predict(user_id, item_id)\n",
    "        error = rating - pred\n",
    "\n",
    "        # Gradients\n",
    "        user_factor = self.user_factors[user_id].copy()\n",
    "        item_factor = self.item_factors[item_id].copy()\n",
    "\n",
    "        # Update factors\n",
    "        self.user_factors[user_id] += self.learning_rate * (error * item_factor - self.reg * user_factor)\n",
    "        self.item_factors[item_id] += self.learning_rate * (error * user_factor - self.reg * item_factor)\n",
    "\n",
    "        # Update biases\n",
    "        self.user_bias[user_id] += self.learning_rate * (error - self.reg * self.user_bias[user_id])\n",
    "        self.item_bias[item_id] += self.learning_rate * (error - self.reg * self.item_bias[item_id])\n",
    "\n",
    "        return error ** 2\n",
    "\n",
    "    def fit(self, interactions_df, epochs=20, verbose=True):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "\n",
    "        # Prepare training data\n",
    "        positive_pairs = interactions_df[['playlist_idx', 'track_idx']].values\n",
    "        positive_set = set(map(tuple, positive_pairs))\n",
    "\n",
    "        # Generate negative samples\n",
    "        print(\"Generating negative samples...\")\n",
    "        negative_pairs = []\n",
    "        target_negatives = len(positive_pairs)\n",
    "\n",
    "        while len(negative_pairs) < target_negatives:\n",
    "            user = np.random.randint(0, self.n_users)\n",
    "            item = np.random.randint(0, self.n_items)\n",
    "\n",
    "            if (user, item) not in positive_set:\n",
    "                negative_pairs.append([user, item])\n",
    "\n",
    "        # Combine training data\n",
    "        all_pairs = np.vstack([positive_pairs, negative_pairs])\n",
    "        all_labels = np.hstack([np.ones(len(positive_pairs)), np.zeros(len(negative_pairs))])\n",
    "\n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(len(all_pairs))\n",
    "        all_pairs = all_pairs[indices]\n",
    "        all_labels = all_labels[indices]\n",
    "\n",
    "        print(f\"Training on {len(all_pairs)} samples...\")\n",
    "\n",
    "        # Training loop\n",
    "        losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for i, (user_id, item_id) in enumerate(all_pairs):\n",
    "                rating = all_labels[i]\n",
    "                loss = self.train_step(user_id, item_id, rating)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                if i % 10000 == 0 and verbose:\n",
    "                    print(f\"Epoch {epoch+1}, Sample {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "            avg_loss = epoch_loss / len(all_pairs)\n",
    "            losses.append(avg_loss)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict_all_for_user(self, user_id):\n",
    "        \"\"\"Predict ratings for all items for a user\"\"\"\n",
    "        predictions = []\n",
    "        for item_id in range(self.n_items):\n",
    "            pred = self.predict(user_id, item_id)\n",
    "            predictions.append(pred)\n",
    "        return np.array(predictions)"
   ],
   "id": "53ef7b98c596be6",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Train-test Split",
   "id": "56f327a56b3206da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T21:55:02.616233Z",
     "start_time": "2025-07-08T21:53:33.698296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def simple_train_test_split(df_processed, test_ratio=0.2):\n",
    "    \"\"\"Simple train-test split\"\"\"\n",
    "\n",
    "    # Group by user\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    for user_id in df_processed['playlist_idx'].unique():\n",
    "        user_tracks = df_processed[df_processed['playlist_idx'] == user_id]['track_idx'].values\n",
    "\n",
    "        if len(user_tracks) > 1:\n",
    "            n_test = max(1, int(len(user_tracks) * test_ratio))\n",
    "            test_tracks = np.random.choice(user_tracks, n_test, replace=False)\n",
    "            train_tracks = [t for t in user_tracks if t not in test_tracks]\n",
    "\n",
    "            # Add to train\n",
    "            for track in train_tracks:\n",
    "                train_data.append({'playlist_idx': user_id, 'track_idx': track})\n",
    "\n",
    "            # Add to test\n",
    "            for track in test_tracks:\n",
    "                test_data.append({'playlist_idx': user_id, 'track_idx': track})\n",
    "        else:\n",
    "            # All to train if only 1 interaction\n",
    "            for track in user_tracks:\n",
    "                train_data.append({'playlist_idx': user_id, 'track_idx': track})\n",
    "\n",
    "    train_df = pd.DataFrame(train_data)\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "\n",
    "    print(f\"Train: {len(train_df)} interactions\")\n",
    "    print(f\"Test: {len(test_df)} interactions\")\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Create train-test split\n",
    "train_df, test_df = simple_train_test_split(df_processed)"
   ],
   "id": "eb01c591392a83b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1539283 interactions\n",
      "Test: 361646 interactions\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Training",
   "id": "382718778db0d100"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T22:02:11.470588Z",
     "start_time": "2025-07-08T21:55:10.454717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nTraining Matrix Factorization model...\")\n",
    "model = SimpleMatrixFactorization(\n",
    "    n_users=n_users,\n",
    "    n_items=n_items,\n",
    "    n_factors=32,  # Smaller for speed\n",
    "    learning_rate=0.01,\n",
    "    reg=0.01\n",
    ")\n",
    "\n",
    "# Train\n",
    "losses = model.fit(train_df, epochs=10, verbose=True)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "fc5e733068d5a9c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Matrix Factorization model...\n",
      "Generating negative samples...\n",
      "Training on 3078566 samples...\n",
      "Epoch 1, Sample 0, Loss: 0.2783\n",
      "Epoch 1, Sample 10000, Loss: 0.2486\n",
      "Epoch 1, Sample 20000, Loss: 0.2648\n",
      "Epoch 1, Sample 30000, Loss: 0.2334\n",
      "Epoch 1, Sample 40000, Loss: 0.2474\n",
      "Epoch 1, Sample 50000, Loss: 0.2107\n",
      "Epoch 1, Sample 60000, Loss: 0.2554\n",
      "Epoch 1, Sample 70000, Loss: 0.2545\n",
      "Epoch 1, Sample 80000, Loss: 0.2576\n",
      "Epoch 1, Sample 90000, Loss: 0.2458\n",
      "Epoch 1, Sample 100000, Loss: 0.2475\n",
      "Epoch 1, Sample 110000, Loss: 0.2235\n",
      "Epoch 1, Sample 120000, Loss: 0.2372\n",
      "Epoch 1, Sample 130000, Loss: 0.2429\n",
      "Epoch 1, Sample 140000, Loss: 0.2576\n",
      "Epoch 1, Sample 150000, Loss: 0.2496\n",
      "Epoch 1, Sample 160000, Loss: 0.2297\n",
      "Epoch 1, Sample 170000, Loss: 0.2682\n",
      "Epoch 1, Sample 180000, Loss: 0.2540\n",
      "Epoch 1, Sample 190000, Loss: 0.2549\n",
      "Epoch 1, Sample 200000, Loss: 0.2272\n",
      "Epoch 1, Sample 210000, Loss: 0.2664\n",
      "Epoch 1, Sample 220000, Loss: 0.2522\n",
      "Epoch 1, Sample 230000, Loss: 0.2506\n",
      "Epoch 1, Sample 240000, Loss: 0.2524\n",
      "Epoch 1, Sample 250000, Loss: 0.2483\n",
      "Epoch 1, Sample 260000, Loss: 0.2561\n",
      "Epoch 1, Sample 270000, Loss: 0.2251\n",
      "Epoch 1, Sample 280000, Loss: 0.2570\n",
      "Epoch 1, Sample 290000, Loss: 0.2363\n",
      "Epoch 1, Sample 300000, Loss: 0.2405\n",
      "Epoch 1, Sample 310000, Loss: 0.1813\n",
      "Epoch 1, Sample 320000, Loss: 0.2398\n",
      "Epoch 1, Sample 330000, Loss: 0.2156\n",
      "Epoch 1, Sample 340000, Loss: 0.2574\n",
      "Epoch 1, Sample 350000, Loss: 0.2471\n",
      "Epoch 1, Sample 360000, Loss: 0.3040\n",
      "Epoch 1, Sample 370000, Loss: 0.2419\n",
      "Epoch 1, Sample 380000, Loss: 0.2708\n",
      "Epoch 1, Sample 390000, Loss: 0.2463\n",
      "Epoch 1, Sample 400000, Loss: 0.2647\n",
      "Epoch 1, Sample 410000, Loss: 0.2195\n",
      "Epoch 1, Sample 420000, Loss: 0.2744\n",
      "Epoch 1, Sample 430000, Loss: 0.2514\n",
      "Epoch 1, Sample 440000, Loss: 0.2551\n",
      "Epoch 1, Sample 450000, Loss: 0.2742\n",
      "Epoch 1, Sample 460000, Loss: 0.2590\n",
      "Epoch 1, Sample 470000, Loss: 0.2661\n",
      "Epoch 1, Sample 480000, Loss: 0.2386\n",
      "Epoch 1, Sample 490000, Loss: 0.2454\n",
      "Epoch 1, Sample 500000, Loss: 0.2228\n",
      "Epoch 1, Sample 510000, Loss: 0.2464\n",
      "Epoch 1, Sample 520000, Loss: 0.2372\n",
      "Epoch 1, Sample 530000, Loss: 0.2456\n",
      "Epoch 1, Sample 540000, Loss: 0.2683\n",
      "Epoch 1, Sample 550000, Loss: 0.2726\n",
      "Epoch 1, Sample 560000, Loss: 0.2634\n",
      "Epoch 1, Sample 570000, Loss: 0.2190\n",
      "Epoch 1, Sample 580000, Loss: 0.1586\n",
      "Epoch 1, Sample 590000, Loss: 0.2028\n",
      "Epoch 1, Sample 600000, Loss: 0.2463\n",
      "Epoch 1, Sample 610000, Loss: 0.2424\n",
      "Epoch 1, Sample 620000, Loss: 0.2219\n",
      "Epoch 1, Sample 630000, Loss: 0.2622\n",
      "Epoch 1, Sample 640000, Loss: 0.2886\n",
      "Epoch 1, Sample 650000, Loss: 0.2615\n",
      "Epoch 1, Sample 660000, Loss: 0.2603\n",
      "Epoch 1, Sample 670000, Loss: 0.2297\n",
      "Epoch 1, Sample 680000, Loss: 0.1964\n",
      "Epoch 1, Sample 690000, Loss: 0.2204\n",
      "Epoch 1, Sample 700000, Loss: 0.2807\n",
      "Epoch 1, Sample 710000, Loss: 0.2502\n",
      "Epoch 1, Sample 720000, Loss: 0.2485\n",
      "Epoch 1, Sample 730000, Loss: 0.2748\n",
      "Epoch 1, Sample 740000, Loss: 0.2637\n",
      "Epoch 1, Sample 750000, Loss: 0.1645\n",
      "Epoch 1, Sample 760000, Loss: 0.2512\n",
      "Epoch 1, Sample 770000, Loss: 0.2567\n",
      "Epoch 1, Sample 780000, Loss: 0.2683\n",
      "Epoch 1, Sample 790000, Loss: 0.2489\n",
      "Epoch 1, Sample 800000, Loss: 0.2272\n",
      "Epoch 1, Sample 810000, Loss: 0.2402\n",
      "Epoch 1, Sample 820000, Loss: 0.2589\n",
      "Epoch 1, Sample 830000, Loss: 0.2626\n",
      "Epoch 1, Sample 840000, Loss: 0.2694\n",
      "Epoch 1, Sample 850000, Loss: 0.2328\n",
      "Epoch 1, Sample 860000, Loss: 0.2129\n",
      "Epoch 1, Sample 870000, Loss: 0.2556\n",
      "Epoch 1, Sample 880000, Loss: 0.2107\n",
      "Epoch 1, Sample 890000, Loss: 0.2636\n",
      "Epoch 1, Sample 900000, Loss: 0.2373\n",
      "Epoch 1, Sample 910000, Loss: 0.2852\n",
      "Epoch 1, Sample 920000, Loss: 0.2231\n",
      "Epoch 1, Sample 930000, Loss: 0.2358\n",
      "Epoch 1, Sample 940000, Loss: 0.2907\n",
      "Epoch 1, Sample 950000, Loss: 0.2409\n",
      "Epoch 1, Sample 960000, Loss: 0.2431\n",
      "Epoch 1, Sample 970000, Loss: 0.1895\n",
      "Epoch 1, Sample 980000, Loss: 0.2391\n",
      "Epoch 1, Sample 990000, Loss: 0.2938\n",
      "Epoch 1, Sample 1000000, Loss: 0.2286\n",
      "Epoch 1, Sample 1010000, Loss: 0.1679\n",
      "Epoch 1, Sample 1020000, Loss: 0.2200\n",
      "Epoch 1, Sample 1030000, Loss: 0.2350\n",
      "Epoch 1, Sample 1040000, Loss: 0.2245\n",
      "Epoch 1, Sample 1050000, Loss: 0.2424\n",
      "Epoch 1, Sample 1060000, Loss: 0.2618\n",
      "Epoch 1, Sample 1070000, Loss: 0.2395\n",
      "Epoch 1, Sample 1080000, Loss: 0.2274\n",
      "Epoch 1, Sample 1090000, Loss: 0.2420\n",
      "Epoch 1, Sample 1100000, Loss: 0.2237\n",
      "Epoch 1, Sample 1110000, Loss: 0.2337\n",
      "Epoch 1, Sample 1120000, Loss: 0.2223\n",
      "Epoch 1, Sample 1130000, Loss: 0.2081\n",
      "Epoch 1, Sample 1140000, Loss: 0.1323\n",
      "Epoch 1, Sample 1150000, Loss: 0.1944\n",
      "Epoch 1, Sample 1160000, Loss: 0.2520\n",
      "Epoch 1, Sample 1170000, Loss: 0.2351\n",
      "Epoch 1, Sample 1180000, Loss: 0.2388\n",
      "Epoch 1, Sample 1190000, Loss: 0.2166\n",
      "Epoch 1, Sample 1200000, Loss: 0.3428\n",
      "Epoch 1, Sample 1210000, Loss: 0.2513\n",
      "Epoch 1, Sample 1220000, Loss: 0.2599\n",
      "Epoch 1, Sample 1230000, Loss: 0.2367\n",
      "Epoch 1, Sample 1240000, Loss: 0.2765\n",
      "Epoch 1, Sample 1250000, Loss: 0.2314\n",
      "Epoch 1, Sample 1260000, Loss: 0.2584\n",
      "Epoch 1, Sample 1270000, Loss: 0.2358\n",
      "Epoch 1, Sample 1280000, Loss: 0.0800\n",
      "Epoch 1, Sample 1290000, Loss: 0.1285\n",
      "Epoch 1, Sample 1300000, Loss: 0.2225\n",
      "Epoch 1, Sample 1310000, Loss: 0.2291\n",
      "Epoch 1, Sample 1320000, Loss: 0.2930\n",
      "Epoch 1, Sample 1330000, Loss: 0.1143\n",
      "Epoch 1, Sample 1340000, Loss: 0.2424\n",
      "Epoch 1, Sample 1350000, Loss: 0.2925\n",
      "Epoch 1, Sample 1360000, Loss: 0.2194\n",
      "Epoch 1, Sample 1370000, Loss: 0.2374\n",
      "Epoch 1, Sample 1380000, Loss: 0.2309\n",
      "Epoch 1, Sample 1390000, Loss: 0.1089\n",
      "Epoch 1, Sample 1400000, Loss: 0.2459\n",
      "Epoch 1, Sample 1410000, Loss: 0.2515\n",
      "Epoch 1, Sample 1420000, Loss: 0.2938\n",
      "Epoch 1, Sample 1430000, Loss: 0.1345\n",
      "Epoch 1, Sample 1440000, Loss: 0.1342\n",
      "Epoch 1, Sample 1450000, Loss: 0.2573\n",
      "Epoch 1, Sample 1460000, Loss: 0.2733\n",
      "Epoch 1, Sample 1470000, Loss: 0.2307\n",
      "Epoch 1, Sample 1480000, Loss: 0.2412\n",
      "Epoch 1, Sample 1490000, Loss: 0.2402\n",
      "Epoch 1, Sample 1500000, Loss: 0.2493\n",
      "Epoch 1, Sample 1510000, Loss: 0.2167\n",
      "Epoch 1, Sample 1520000, Loss: 0.2195\n",
      "Epoch 1, Sample 1530000, Loss: 0.2081\n",
      "Epoch 1, Sample 1540000, Loss: 0.2567\n",
      "Epoch 1, Sample 1550000, Loss: 0.2517\n",
      "Epoch 1, Sample 1560000, Loss: 0.2447\n",
      "Epoch 1, Sample 1570000, Loss: 0.2158\n",
      "Epoch 1, Sample 1580000, Loss: 0.0538\n",
      "Epoch 1, Sample 1590000, Loss: 0.1685\n",
      "Epoch 1, Sample 1600000, Loss: 0.2454\n",
      "Epoch 1, Sample 1610000, Loss: 0.0467\n",
      "Epoch 1, Sample 1620000, Loss: 0.2882\n",
      "Epoch 1, Sample 1630000, Loss: 0.1733\n",
      "Epoch 1, Sample 1640000, Loss: 0.2236\n",
      "Epoch 1, Sample 1650000, Loss: 0.2370\n",
      "Epoch 1, Sample 1660000, Loss: 0.1712\n",
      "Epoch 1, Sample 1670000, Loss: 0.1781\n",
      "Epoch 1, Sample 1680000, Loss: 0.2529\n",
      "Epoch 1, Sample 1690000, Loss: 0.2212\n",
      "Epoch 1, Sample 1700000, Loss: 0.0911\n",
      "Epoch 1, Sample 1710000, Loss: 0.1791\n",
      "Epoch 1, Sample 1720000, Loss: 0.2309\n",
      "Epoch 1, Sample 1730000, Loss: 0.1612\n",
      "Epoch 1, Sample 1740000, Loss: 0.0898\n",
      "Epoch 1, Sample 1750000, Loss: 0.2476\n",
      "Epoch 1, Sample 1760000, Loss: 0.2599\n",
      "Epoch 1, Sample 1770000, Loss: 0.0941\n",
      "Epoch 1, Sample 1780000, Loss: 0.2376\n",
      "Epoch 1, Sample 1790000, Loss: 0.2662\n",
      "Epoch 1, Sample 1800000, Loss: 0.2514\n",
      "Epoch 1, Sample 1810000, Loss: 0.1369\n",
      "Epoch 1, Sample 1820000, Loss: 0.1867\n",
      "Epoch 1, Sample 1830000, Loss: 0.2331\n",
      "Epoch 1, Sample 1840000, Loss: 0.2175\n",
      "Epoch 1, Sample 1850000, Loss: 0.2767\n",
      "Epoch 1, Sample 1860000, Loss: 0.2566\n",
      "Epoch 1, Sample 1870000, Loss: 0.2563\n",
      "Epoch 1, Sample 1880000, Loss: 0.2401\n",
      "Epoch 1, Sample 1890000, Loss: 0.2641\n",
      "Epoch 1, Sample 1900000, Loss: 0.2342\n",
      "Epoch 1, Sample 1910000, Loss: 0.2485\n",
      "Epoch 1, Sample 1920000, Loss: 0.2652\n",
      "Epoch 1, Sample 1930000, Loss: 0.2197\n",
      "Epoch 1, Sample 1940000, Loss: 0.2218\n",
      "Epoch 1, Sample 1950000, Loss: 0.2272\n",
      "Epoch 1, Sample 1960000, Loss: 0.2420\n",
      "Epoch 1, Sample 1970000, Loss: 0.2063\n",
      "Epoch 1, Sample 1980000, Loss: 0.2641\n",
      "Epoch 1, Sample 1990000, Loss: 0.2467\n",
      "Epoch 1, Sample 2000000, Loss: 0.2625\n",
      "Epoch 1, Sample 2010000, Loss: 0.2976\n",
      "Epoch 1, Sample 2020000, Loss: 0.2781\n",
      "Epoch 1, Sample 2030000, Loss: 0.2719\n",
      "Epoch 1, Sample 2040000, Loss: 0.2572\n",
      "Epoch 1, Sample 2050000, Loss: 0.2554\n",
      "Epoch 1, Sample 2060000, Loss: 0.2213\n",
      "Epoch 1, Sample 2070000, Loss: 0.2198\n",
      "Epoch 1, Sample 2080000, Loss: 0.1313\n",
      "Epoch 1, Sample 2090000, Loss: 0.2263\n",
      "Epoch 1, Sample 2100000, Loss: 0.2251\n",
      "Epoch 1, Sample 2110000, Loss: 0.0717\n",
      "Epoch 1, Sample 2120000, Loss: 0.2590\n",
      "Epoch 1, Sample 2130000, Loss: 0.2823\n",
      "Epoch 1, Sample 2140000, Loss: 0.2569\n",
      "Epoch 1, Sample 2150000, Loss: 0.2009\n",
      "Epoch 1, Sample 2160000, Loss: 0.2522\n",
      "Epoch 1, Sample 2170000, Loss: 0.2538\n",
      "Epoch 1, Sample 2180000, Loss: 0.1988\n",
      "Epoch 1, Sample 2190000, Loss: 0.2738\n",
      "Epoch 1, Sample 2200000, Loss: 0.2452\n",
      "Epoch 1, Sample 2210000, Loss: 0.2474\n",
      "Epoch 1, Sample 2220000, Loss: 0.1100\n",
      "Epoch 1, Sample 2230000, Loss: 0.2689\n",
      "Epoch 1, Sample 2240000, Loss: 0.2140\n",
      "Epoch 1, Sample 2250000, Loss: 0.2338\n",
      "Epoch 1, Sample 2260000, Loss: 0.2138\n",
      "Epoch 1, Sample 2270000, Loss: 0.1550\n",
      "Epoch 1, Sample 2280000, Loss: 0.0528\n",
      "Epoch 1, Sample 2290000, Loss: 0.0934\n",
      "Epoch 1, Sample 2300000, Loss: 0.2126\n",
      "Epoch 1, Sample 2310000, Loss: 0.2280\n",
      "Epoch 1, Sample 2320000, Loss: 0.1989\n",
      "Epoch 1, Sample 2330000, Loss: 0.1540\n",
      "Epoch 1, Sample 2340000, Loss: 0.2591\n",
      "Epoch 1, Sample 2350000, Loss: 0.2591\n",
      "Epoch 1, Sample 2360000, Loss: 0.3050\n",
      "Epoch 1, Sample 2370000, Loss: 0.2214\n",
      "Epoch 1, Sample 2380000, Loss: 0.2545\n",
      "Epoch 1, Sample 2390000, Loss: 0.2133\n",
      "Epoch 1, Sample 2400000, Loss: 0.2483\n",
      "Epoch 1, Sample 2410000, Loss: 0.0696\n",
      "Epoch 1, Sample 2420000, Loss: 0.2339\n",
      "Epoch 1, Sample 2430000, Loss: 0.2855\n",
      "Epoch 1, Sample 2440000, Loss: 0.2324\n",
      "Epoch 1, Sample 2450000, Loss: 0.2492\n",
      "Epoch 1, Sample 2460000, Loss: 0.2377\n",
      "Epoch 1, Sample 2470000, Loss: 0.2088\n",
      "Epoch 1, Sample 2480000, Loss: 0.2602\n",
      "Epoch 1, Sample 2490000, Loss: 0.1900\n",
      "Epoch 1, Sample 2500000, Loss: 0.2489\n",
      "Epoch 1, Sample 2510000, Loss: 0.0190\n",
      "Epoch 1, Sample 2520000, Loss: 0.2591\n",
      "Epoch 1, Sample 2530000, Loss: 0.4127\n",
      "Epoch 1, Sample 2540000, Loss: 0.3295\n",
      "Epoch 1, Sample 2550000, Loss: 0.2600\n",
      "Epoch 1, Sample 2560000, Loss: 0.1260\n",
      "Epoch 1, Sample 2570000, Loss: 0.0532\n",
      "Epoch 1, Sample 2580000, Loss: 0.1992\n",
      "Epoch 1, Sample 2590000, Loss: 0.2240\n",
      "Epoch 1, Sample 2600000, Loss: 0.2207\n",
      "Epoch 1, Sample 2610000, Loss: 0.2348\n",
      "Epoch 1, Sample 2620000, Loss: 0.2442\n",
      "Epoch 1, Sample 2630000, Loss: 0.2648\n",
      "Epoch 1, Sample 2640000, Loss: 0.2242\n",
      "Epoch 1, Sample 2650000, Loss: 0.2374\n",
      "Epoch 1, Sample 2660000, Loss: 0.2106\n",
      "Epoch 1, Sample 2670000, Loss: 0.2578\n",
      "Epoch 1, Sample 2680000, Loss: 0.2565\n",
      "Epoch 1, Sample 2690000, Loss: 0.2088\n",
      "Epoch 1, Sample 2700000, Loss: 0.2354\n",
      "Epoch 1, Sample 2710000, Loss: 0.0365\n",
      "Epoch 1, Sample 2720000, Loss: 0.2835\n",
      "Epoch 1, Sample 2730000, Loss: 0.2465\n",
      "Epoch 1, Sample 2740000, Loss: 0.2141\n",
      "Epoch 1, Sample 2750000, Loss: 0.2854\n",
      "Epoch 1, Sample 2760000, Loss: 0.2255\n",
      "Epoch 1, Sample 2770000, Loss: 0.1293\n",
      "Epoch 1, Sample 2780000, Loss: 0.0830\n",
      "Epoch 1, Sample 2790000, Loss: 0.2100\n",
      "Epoch 1, Sample 2800000, Loss: 0.2245\n",
      "Epoch 1, Sample 2810000, Loss: 0.1960\n",
      "Epoch 1, Sample 2820000, Loss: 0.2135\n",
      "Epoch 1, Sample 2830000, Loss: 0.1557\n",
      "Epoch 1, Sample 2840000, Loss: 0.2390\n",
      "Epoch 1, Sample 2850000, Loss: 0.2695\n",
      "Epoch 1, Sample 2860000, Loss: 0.0451\n",
      "Epoch 1, Sample 2870000, Loss: 0.2160\n",
      "Epoch 1, Sample 2880000, Loss: 0.2313\n",
      "Epoch 1, Sample 2890000, Loss: 0.1038\n",
      "Epoch 1, Sample 2900000, Loss: 0.0120\n",
      "Epoch 1, Sample 2910000, Loss: 0.1511\n",
      "Epoch 1, Sample 2920000, Loss: 0.2975\n",
      "Epoch 1, Sample 2930000, Loss: 0.2681\n",
      "Epoch 1, Sample 2940000, Loss: 0.2176\n",
      "Epoch 1, Sample 2950000, Loss: 0.0847\n",
      "Epoch 1, Sample 2960000, Loss: 0.2431\n",
      "Epoch 1, Sample 2970000, Loss: 0.2687\n",
      "Epoch 1, Sample 2980000, Loss: 0.2565\n",
      "Epoch 1, Sample 2990000, Loss: 0.2367\n",
      "Epoch 1, Sample 3000000, Loss: 0.2837\n",
      "Epoch 1, Sample 3010000, Loss: 0.2274\n",
      "Epoch 1, Sample 3020000, Loss: 0.2781\n",
      "Epoch 1, Sample 3030000, Loss: 0.0266\n",
      "Epoch 1, Sample 3040000, Loss: 0.2342\n",
      "Epoch 1, Sample 3050000, Loss: 0.2040\n",
      "Epoch 1, Sample 3060000, Loss: 0.0215\n",
      "Epoch 1, Sample 3070000, Loss: 0.2298\n",
      "Epoch 1/10, Average Loss: 0.2250\n",
      "Epoch 2, Sample 0, Loss: 0.0605\n",
      "Epoch 2, Sample 10000, Loss: 0.2099\n",
      "Epoch 2, Sample 20000, Loss: 0.2559\n",
      "Epoch 2, Sample 30000, Loss: 0.2204\n",
      "Epoch 2, Sample 40000, Loss: 0.2152\n",
      "Epoch 2, Sample 50000, Loss: 0.0837\n",
      "Epoch 2, Sample 60000, Loss: 0.2246\n",
      "Epoch 2, Sample 70000, Loss: 0.2544\n",
      "Epoch 2, Sample 80000, Loss: 0.2587\n",
      "Epoch 2, Sample 90000, Loss: 0.0603\n",
      "Epoch 2, Sample 100000, Loss: 0.2158\n",
      "Epoch 2, Sample 110000, Loss: 0.2029\n",
      "Epoch 2, Sample 120000, Loss: 0.0874\n",
      "Epoch 2, Sample 130000, Loss: 0.2238\n",
      "Epoch 2, Sample 140000, Loss: 0.2505\n",
      "Epoch 2, Sample 150000, Loss: 0.2981\n",
      "Epoch 2, Sample 160000, Loss: 0.2183\n",
      "Epoch 2, Sample 170000, Loss: 0.2185\n",
      "Epoch 2, Sample 180000, Loss: 0.2449\n",
      "Epoch 2, Sample 190000, Loss: 0.2513\n",
      "Epoch 2, Sample 200000, Loss: 0.2156\n",
      "Epoch 2, Sample 210000, Loss: 0.2923\n",
      "Epoch 2, Sample 220000, Loss: 0.2841\n",
      "Epoch 2, Sample 230000, Loss: 0.2193\n",
      "Epoch 2, Sample 240000, Loss: 0.2552\n",
      "Epoch 2, Sample 250000, Loss: 0.2598\n",
      "Epoch 2, Sample 260000, Loss: 0.2450\n",
      "Epoch 2, Sample 270000, Loss: 0.2279\n",
      "Epoch 2, Sample 280000, Loss: 0.2840\n",
      "Epoch 2, Sample 290000, Loss: 0.2067\n",
      "Epoch 2, Sample 300000, Loss: 0.2401\n",
      "Epoch 2, Sample 310000, Loss: 0.0179\n",
      "Epoch 2, Sample 320000, Loss: 0.2442\n",
      "Epoch 2, Sample 330000, Loss: 0.0759\n",
      "Epoch 2, Sample 340000, Loss: 0.2152\n",
      "Epoch 2, Sample 350000, Loss: 0.2391\n",
      "Epoch 2, Sample 360000, Loss: 0.5427\n",
      "Epoch 2, Sample 370000, Loss: 0.1860\n",
      "Epoch 2, Sample 380000, Loss: 0.2465\n",
      "Epoch 2, Sample 390000, Loss: 0.2711\n",
      "Epoch 2, Sample 400000, Loss: 0.2270\n",
      "Epoch 2, Sample 410000, Loss: 0.1487\n",
      "Epoch 2, Sample 420000, Loss: 0.3874\n",
      "Epoch 2, Sample 430000, Loss: 0.2750\n",
      "Epoch 2, Sample 440000, Loss: 0.2472\n",
      "Epoch 2, Sample 450000, Loss: 0.2551\n",
      "Epoch 2, Sample 460000, Loss: 0.2667\n",
      "Epoch 2, Sample 470000, Loss: 0.2495\n",
      "Epoch 2, Sample 480000, Loss: 0.2232\n",
      "Epoch 2, Sample 490000, Loss: 0.2167\n",
      "Epoch 2, Sample 500000, Loss: 0.1421\n",
      "Epoch 2, Sample 510000, Loss: 0.2269\n",
      "Epoch 2, Sample 520000, Loss: 0.1775\n",
      "Epoch 2, Sample 530000, Loss: 0.2433\n",
      "Epoch 2, Sample 540000, Loss: 0.3018\n",
      "Epoch 2, Sample 550000, Loss: 0.2677\n",
      "Epoch 2, Sample 560000, Loss: 0.2761\n",
      "Epoch 2, Sample 570000, Loss: 0.1451\n",
      "Epoch 2, Sample 580000, Loss: 0.0225\n",
      "Epoch 2, Sample 590000, Loss: 0.0967\n",
      "Epoch 2, Sample 600000, Loss: 0.2117\n",
      "Epoch 2, Sample 610000, Loss: 0.1940\n",
      "Epoch 2, Sample 620000, Loss: 0.2402\n",
      "Epoch 2, Sample 630000, Loss: 0.2341\n",
      "Epoch 2, Sample 640000, Loss: 0.2979\n",
      "Epoch 2, Sample 650000, Loss: 0.2682\n",
      "Epoch 2, Sample 660000, Loss: 0.2770\n",
      "Epoch 2, Sample 670000, Loss: 0.2036\n",
      "Epoch 2, Sample 680000, Loss: 0.0978\n",
      "Epoch 2, Sample 690000, Loss: 0.1351\n",
      "Epoch 2, Sample 700000, Loss: 0.3064\n",
      "Epoch 2, Sample 710000, Loss: 0.2644\n",
      "Epoch 2, Sample 720000, Loss: 0.2491\n",
      "Epoch 2, Sample 730000, Loss: 0.2810\n",
      "Epoch 2, Sample 740000, Loss: 0.2463\n",
      "Epoch 2, Sample 750000, Loss: 0.0484\n",
      "Epoch 2, Sample 760000, Loss: 0.2554\n",
      "Epoch 2, Sample 770000, Loss: 0.2506\n",
      "Epoch 2, Sample 780000, Loss: 0.2962\n",
      "Epoch 2, Sample 790000, Loss: 0.2559\n",
      "Epoch 2, Sample 800000, Loss: 0.2274\n",
      "Epoch 2, Sample 810000, Loss: 0.2318\n",
      "Epoch 2, Sample 820000, Loss: 0.2253\n",
      "Epoch 2, Sample 830000, Loss: 0.1788\n",
      "Epoch 2, Sample 840000, Loss: 0.3038\n",
      "Epoch 2, Sample 850000, Loss: 0.1676\n",
      "Epoch 2, Sample 860000, Loss: 0.1786\n",
      "Epoch 2, Sample 870000, Loss: 0.2521\n",
      "Epoch 2, Sample 880000, Loss: 0.1185\n",
      "Epoch 2, Sample 890000, Loss: 0.2742\n",
      "Epoch 2, Sample 900000, Loss: 0.2232\n",
      "Epoch 2, Sample 910000, Loss: 0.2746\n",
      "Epoch 2, Sample 920000, Loss: 0.1794\n",
      "Epoch 2, Sample 930000, Loss: 0.2338\n",
      "Epoch 2, Sample 940000, Loss: 0.3624\n",
      "Epoch 2, Sample 950000, Loss: 0.2143\n",
      "Epoch 2, Sample 960000, Loss: 0.1926\n",
      "Epoch 2, Sample 970000, Loss: 0.1082\n",
      "Epoch 2, Sample 980000, Loss: 0.2438\n",
      "Epoch 2, Sample 990000, Loss: 0.4711\n",
      "Epoch 2, Sample 1000000, Loss: 0.2172\n",
      "Epoch 2, Sample 1010000, Loss: 0.0785\n",
      "Epoch 2, Sample 1020000, Loss: 0.2324\n",
      "Epoch 2, Sample 1030000, Loss: 0.2110\n",
      "Epoch 2, Sample 1040000, Loss: 0.1904\n",
      "Epoch 2, Sample 1050000, Loss: 0.1973\n",
      "Epoch 2, Sample 1060000, Loss: 0.2559\n",
      "Epoch 2, Sample 1070000, Loss: 0.1780\n",
      "Epoch 2, Sample 1080000, Loss: 0.1673\n",
      "Epoch 2, Sample 1090000, Loss: 0.2291\n",
      "Epoch 2, Sample 1100000, Loss: 0.2384\n",
      "Epoch 2, Sample 1110000, Loss: 0.1863\n",
      "Epoch 2, Sample 1120000, Loss: 0.1748\n",
      "Epoch 2, Sample 1130000, Loss: 0.1572\n",
      "Epoch 2, Sample 1140000, Loss: 0.0420\n",
      "Epoch 2, Sample 1150000, Loss: 0.1080\n",
      "Epoch 2, Sample 1160000, Loss: 0.2700\n",
      "Epoch 2, Sample 1170000, Loss: 0.2292\n",
      "Epoch 2, Sample 1180000, Loss: 0.2384\n",
      "Epoch 2, Sample 1190000, Loss: 0.1700\n",
      "Epoch 2, Sample 1200000, Loss: 0.4819\n",
      "Epoch 2, Sample 1210000, Loss: 0.2172\n",
      "Epoch 2, Sample 1220000, Loss: 0.2702\n",
      "Epoch 2, Sample 1230000, Loss: 0.2200\n",
      "Epoch 2, Sample 1240000, Loss: 0.2827\n",
      "Epoch 2, Sample 1250000, Loss: 0.2229\n",
      "Epoch 2, Sample 1260000, Loss: 0.2525\n",
      "Epoch 2, Sample 1270000, Loss: 0.2426\n",
      "Epoch 2, Sample 1280000, Loss: 0.0176\n",
      "Epoch 2, Sample 1290000, Loss: 0.0536\n",
      "Epoch 2, Sample 1300000, Loss: 0.2137\n",
      "Epoch 2, Sample 1310000, Loss: 0.2178\n",
      "Epoch 2, Sample 1320000, Loss: 0.3108\n",
      "Epoch 2, Sample 1330000, Loss: 0.0356\n",
      "Epoch 2, Sample 1340000, Loss: 0.2419\n",
      "Epoch 2, Sample 1350000, Loss: 0.2516\n",
      "Epoch 2, Sample 1360000, Loss: 0.1499\n",
      "Epoch 2, Sample 1370000, Loss: 0.2190\n",
      "Epoch 2, Sample 1380000, Loss: 0.2044\n",
      "Epoch 2, Sample 1390000, Loss: 0.0313\n",
      "Epoch 2, Sample 1400000, Loss: 0.2547\n",
      "Epoch 2, Sample 1410000, Loss: 0.2146\n",
      "Epoch 2, Sample 1420000, Loss: 0.3384\n",
      "Epoch 2, Sample 1430000, Loss: 0.0618\n",
      "Epoch 2, Sample 1440000, Loss: 0.0421\n",
      "Epoch 2, Sample 1450000, Loss: 0.2485\n",
      "Epoch 2, Sample 1460000, Loss: 0.2662\n",
      "Epoch 2, Sample 1470000, Loss: 0.1962\n",
      "Epoch 2, Sample 1480000, Loss: 0.2585\n",
      "Epoch 2, Sample 1490000, Loss: 0.2090\n",
      "Epoch 2, Sample 1500000, Loss: 0.2194\n",
      "Epoch 2, Sample 1510000, Loss: 0.1960\n",
      "Epoch 2, Sample 1520000, Loss: 0.1713\n",
      "Epoch 2, Sample 1530000, Loss: 0.1714\n",
      "Epoch 2, Sample 1540000, Loss: 0.2430\n",
      "Epoch 2, Sample 1550000, Loss: 0.2461\n",
      "Epoch 2, Sample 1560000, Loss: 0.2675\n",
      "Epoch 2, Sample 1570000, Loss: 0.1967\n",
      "Epoch 2, Sample 1580000, Loss: 0.0133\n",
      "Epoch 2, Sample 1590000, Loss: 0.0923\n",
      "Epoch 2, Sample 1600000, Loss: 0.2435\n",
      "Epoch 2, Sample 1610000, Loss: 0.0079\n",
      "Epoch 2, Sample 1620000, Loss: 0.2764\n",
      "Epoch 2, Sample 1630000, Loss: 0.1088\n",
      "Epoch 2, Sample 1640000, Loss: 0.2300\n",
      "Epoch 2, Sample 1650000, Loss: 0.2016\n",
      "Epoch 2, Sample 1660000, Loss: 0.1212\n",
      "Epoch 2, Sample 1670000, Loss: 0.1014\n",
      "Epoch 2, Sample 1680000, Loss: 0.2379\n",
      "Epoch 2, Sample 1690000, Loss: 0.1561\n",
      "Epoch 2, Sample 1700000, Loss: 0.0313\n",
      "Epoch 2, Sample 1710000, Loss: 0.1300\n",
      "Epoch 2, Sample 1720000, Loss: 0.2098\n",
      "Epoch 2, Sample 1730000, Loss: 0.0720\n",
      "Epoch 2, Sample 1740000, Loss: 0.0305\n",
      "Epoch 2, Sample 1750000, Loss: 0.2724\n",
      "Epoch 2, Sample 1760000, Loss: 0.2727\n",
      "Epoch 2, Sample 1770000, Loss: 0.0331\n",
      "Epoch 2, Sample 1780000, Loss: 0.2383\n",
      "Epoch 2, Sample 1790000, Loss: 0.2439\n",
      "Epoch 2, Sample 1800000, Loss: 0.2348\n",
      "Epoch 2, Sample 1810000, Loss: 0.0663\n",
      "Epoch 2, Sample 1820000, Loss: 0.1242\n",
      "Epoch 2, Sample 1830000, Loss: 0.2032\n",
      "Epoch 2, Sample 1840000, Loss: 0.1931\n",
      "Epoch 2, Sample 1850000, Loss: 0.2597\n",
      "Epoch 2, Sample 1860000, Loss: 0.2417\n",
      "Epoch 2, Sample 1870000, Loss: 0.2619\n",
      "Epoch 2, Sample 1880000, Loss: 0.2388\n",
      "Epoch 2, Sample 1890000, Loss: 0.2695\n",
      "Epoch 2, Sample 1900000, Loss: 0.2187\n",
      "Epoch 2, Sample 1910000, Loss: 0.2303\n",
      "Epoch 2, Sample 1920000, Loss: 0.2805\n",
      "Epoch 2, Sample 1930000, Loss: 0.2145\n",
      "Epoch 2, Sample 1940000, Loss: 0.1831\n",
      "Epoch 2, Sample 1950000, Loss: 0.2132\n",
      "Epoch 2, Sample 1960000, Loss: 0.2118\n",
      "Epoch 2, Sample 1970000, Loss: 0.1746\n",
      "Epoch 2, Sample 1980000, Loss: 0.2786\n",
      "Epoch 2, Sample 1990000, Loss: 0.2611\n",
      "Epoch 2, Sample 2000000, Loss: 0.2470\n",
      "Epoch 2, Sample 2010000, Loss: 0.3542\n",
      "Epoch 2, Sample 2020000, Loss: 0.2791\n",
      "Epoch 2, Sample 2030000, Loss: 0.2736\n",
      "Epoch 2, Sample 2040000, Loss: 0.2469\n",
      "Epoch 2, Sample 2050000, Loss: 0.2408\n",
      "Epoch 2, Sample 2060000, Loss: 0.1928\n",
      "Epoch 2, Sample 2070000, Loss: 0.1761\n",
      "Epoch 2, Sample 2080000, Loss: 0.0658\n",
      "Epoch 2, Sample 2090000, Loss: 0.2126\n",
      "Epoch 2, Sample 2100000, Loss: 0.2251\n",
      "Epoch 2, Sample 2110000, Loss: 0.0231\n",
      "Epoch 2, Sample 2120000, Loss: 0.2331\n",
      "Epoch 2, Sample 2130000, Loss: 0.2885\n",
      "Epoch 2, Sample 2140000, Loss: 0.2706\n",
      "Epoch 2, Sample 2150000, Loss: 0.1788\n",
      "Epoch 2, Sample 2160000, Loss: 0.2462\n",
      "Epoch 2, Sample 2170000, Loss: 0.2191\n",
      "Epoch 2, Sample 2180000, Loss: 0.1735\n",
      "Epoch 2, Sample 2190000, Loss: 0.2885\n",
      "Epoch 2, Sample 2200000, Loss: 0.2572\n",
      "Epoch 2, Sample 2210000, Loss: 0.2365\n",
      "Epoch 2, Sample 2220000, Loss: 0.0432\n",
      "Epoch 2, Sample 2230000, Loss: 0.2588\n",
      "Epoch 2, Sample 2240000, Loss: 0.1731\n",
      "Epoch 2, Sample 2250000, Loss: 0.1984\n",
      "Epoch 2, Sample 2260000, Loss: 0.1949\n",
      "Epoch 2, Sample 2270000, Loss: 0.0967\n",
      "Epoch 2, Sample 2280000, Loss: 0.0195\n",
      "Epoch 2, Sample 2290000, Loss: 0.0388\n",
      "Epoch 2, Sample 2300000, Loss: 0.1668\n",
      "Epoch 2, Sample 2310000, Loss: 0.1885\n",
      "Epoch 2, Sample 2320000, Loss: 0.1661\n",
      "Epoch 2, Sample 2330000, Loss: 0.0879\n",
      "Epoch 2, Sample 2340000, Loss: 0.2583\n",
      "Epoch 2, Sample 2350000, Loss: 0.2550\n",
      "Epoch 2, Sample 2360000, Loss: 0.3296\n",
      "Epoch 2, Sample 2370000, Loss: 0.1773\n",
      "Epoch 2, Sample 2380000, Loss: 0.2473\n",
      "Epoch 2, Sample 2390000, Loss: 0.1759\n",
      "Epoch 2, Sample 2400000, Loss: 0.2488\n",
      "Epoch 2, Sample 2410000, Loss: 0.0256\n",
      "Epoch 2, Sample 2420000, Loss: 0.2334\n",
      "Epoch 2, Sample 2430000, Loss: 0.3170\n",
      "Epoch 2, Sample 2440000, Loss: 0.2087\n",
      "Epoch 2, Sample 2450000, Loss: 0.2198\n",
      "Epoch 2, Sample 2460000, Loss: 0.2083\n",
      "Epoch 2, Sample 2470000, Loss: 0.1676\n",
      "Epoch 2, Sample 2480000, Loss: 0.2498\n",
      "Epoch 2, Sample 2490000, Loss: 0.1424\n",
      "Epoch 2, Sample 2500000, Loss: 0.2639\n",
      "Epoch 2, Sample 2510000, Loss: 0.0080\n",
      "Epoch 2, Sample 2520000, Loss: 0.2489\n",
      "Epoch 2, Sample 2530000, Loss: 0.5327\n",
      "Epoch 2, Sample 2540000, Loss: 0.3930\n",
      "Epoch 2, Sample 2550000, Loss: 0.2710\n",
      "Epoch 2, Sample 2560000, Loss: 0.0643\n",
      "Epoch 2, Sample 2570000, Loss: 0.0206\n",
      "Epoch 2, Sample 2580000, Loss: 0.1753\n",
      "Epoch 2, Sample 2590000, Loss: 0.2022\n",
      "Epoch 2, Sample 2600000, Loss: 0.1978\n",
      "Epoch 2, Sample 2610000, Loss: 0.2127\n",
      "Epoch 2, Sample 2620000, Loss: 0.2382\n",
      "Epoch 2, Sample 2630000, Loss: 0.2626\n",
      "Epoch 2, Sample 2640000, Loss: 0.1908\n",
      "Epoch 2, Sample 2650000, Loss: 0.2162\n",
      "Epoch 2, Sample 2660000, Loss: 0.1695\n",
      "Epoch 2, Sample 2670000, Loss: 0.2886\n",
      "Epoch 2, Sample 2680000, Loss: 0.2269\n",
      "Epoch 2, Sample 2690000, Loss: 0.1796\n",
      "Epoch 2, Sample 2700000, Loss: 0.2075\n",
      "Epoch 2, Sample 2710000, Loss: 0.0145\n",
      "Epoch 2, Sample 2720000, Loss: 0.3024\n",
      "Epoch 2, Sample 2730000, Loss: 0.2306\n",
      "Epoch 2, Sample 2740000, Loss: 0.2008\n",
      "Epoch 2, Sample 2750000, Loss: 0.3195\n",
      "Epoch 2, Sample 2760000, Loss: 0.2127\n",
      "Epoch 2, Sample 2770000, Loss: 0.0684\n",
      "Epoch 2, Sample 2780000, Loss: 0.0435\n",
      "Epoch 2, Sample 2790000, Loss: 0.1848\n",
      "Epoch 2, Sample 2800000, Loss: 0.1936\n",
      "Epoch 2, Sample 2810000, Loss: 0.1660\n",
      "Epoch 2, Sample 2820000, Loss: 0.1984\n",
      "Epoch 2, Sample 2830000, Loss: 0.1067\n",
      "Epoch 2, Sample 2840000, Loss: 0.2403\n",
      "Epoch 2, Sample 2850000, Loss: 0.2547\n",
      "Epoch 2, Sample 2860000, Loss: 0.0164\n",
      "Epoch 2, Sample 2870000, Loss: 0.1822\n",
      "Epoch 2, Sample 2880000, Loss: 0.2145\n",
      "Epoch 2, Sample 2890000, Loss: 0.0558\n",
      "Epoch 2, Sample 2900000, Loss: 0.0048\n",
      "Epoch 2, Sample 2910000, Loss: 0.0929\n",
      "Epoch 2, Sample 2920000, Loss: 0.3131\n",
      "Epoch 2, Sample 2930000, Loss: 0.2685\n",
      "Epoch 2, Sample 2940000, Loss: 0.1917\n",
      "Epoch 2, Sample 2950000, Loss: 0.0372\n",
      "Epoch 2, Sample 2960000, Loss: 0.2291\n",
      "Epoch 2, Sample 2970000, Loss: 0.2388\n",
      "Epoch 2, Sample 2980000, Loss: 0.2632\n",
      "Epoch 2, Sample 2990000, Loss: 0.2014\n",
      "Epoch 2, Sample 3000000, Loss: 0.3059\n",
      "Epoch 2, Sample 3010000, Loss: 0.1961\n",
      "Epoch 2, Sample 3020000, Loss: 0.2759\n",
      "Epoch 2, Sample 3030000, Loss: 0.0089\n",
      "Epoch 2, Sample 3040000, Loss: 0.2031\n",
      "Epoch 2, Sample 3050000, Loss: 0.1724\n",
      "Epoch 2, Sample 3060000, Loss: 0.0072\n",
      "Epoch 2, Sample 3070000, Loss: 0.2084\n",
      "Epoch 2/10, Average Loss: 0.1995\n",
      "Epoch 3, Sample 0, Loss: 0.0276\n",
      "Epoch 3, Sample 10000, Loss: 0.1772\n",
      "Epoch 3, Sample 20000, Loss: 0.2512\n",
      "Epoch 3, Sample 30000, Loss: 0.2157\n",
      "Epoch 3, Sample 40000, Loss: 0.1836\n",
      "Epoch 3, Sample 50000, Loss: 0.0461\n",
      "Epoch 3, Sample 60000, Loss: 0.1991\n",
      "Epoch 3, Sample 70000, Loss: 0.2605\n",
      "Epoch 3, Sample 80000, Loss: 0.2449\n",
      "Epoch 3, Sample 90000, Loss: 0.0287\n",
      "Epoch 3, Sample 100000, Loss: 0.1860\n",
      "Epoch 3, Sample 110000, Loss: 0.1882\n",
      "Epoch 3, Sample 120000, Loss: 0.0441\n",
      "Epoch 3, Sample 130000, Loss: 0.2083\n",
      "Epoch 3, Sample 140000, Loss: 0.2499\n",
      "Epoch 3, Sample 150000, Loss: 0.3431\n",
      "Epoch 3, Sample 160000, Loss: 0.2088\n",
      "Epoch 3, Sample 170000, Loss: 0.1812\n",
      "Epoch 3, Sample 180000, Loss: 0.2339\n",
      "Epoch 3, Sample 190000, Loss: 0.2428\n",
      "Epoch 3, Sample 200000, Loss: 0.2042\n",
      "Epoch 3, Sample 210000, Loss: 0.3171\n",
      "Epoch 3, Sample 220000, Loss: 0.3027\n",
      "Epoch 3, Sample 230000, Loss: 0.1972\n",
      "Epoch 3, Sample 240000, Loss: 0.2543\n",
      "Epoch 3, Sample 250000, Loss: 0.2616\n",
      "Epoch 3, Sample 260000, Loss: 0.2300\n",
      "Epoch 3, Sample 270000, Loss: 0.2302\n",
      "Epoch 3, Sample 280000, Loss: 0.3090\n",
      "Epoch 3, Sample 290000, Loss: 0.1795\n",
      "Epoch 3, Sample 300000, Loss: 0.2529\n",
      "Epoch 3, Sample 310000, Loss: 0.0069\n",
      "Epoch 3, Sample 320000, Loss: 0.2424\n",
      "Epoch 3, Sample 330000, Loss: 0.0357\n",
      "Epoch 3, Sample 340000, Loss: 0.1792\n",
      "Epoch 3, Sample 350000, Loss: 0.2315\n",
      "Epoch 3, Sample 360000, Loss: 0.6621\n",
      "Epoch 3, Sample 370000, Loss: 0.1480\n",
      "Epoch 3, Sample 380000, Loss: 0.2190\n",
      "Epoch 3, Sample 390000, Loss: 0.2909\n",
      "Epoch 3, Sample 400000, Loss: 0.1940\n",
      "Epoch 3, Sample 410000, Loss: 0.1106\n",
      "Epoch 3, Sample 420000, Loss: 0.4652\n",
      "Epoch 3, Sample 430000, Loss: 0.2931\n",
      "Epoch 3, Sample 440000, Loss: 0.2417\n",
      "Epoch 3, Sample 450000, Loss: 0.2436\n",
      "Epoch 3, Sample 460000, Loss: 0.2673\n",
      "Epoch 3, Sample 470000, Loss: 0.2358\n",
      "Epoch 3, Sample 480000, Loss: 0.2042\n",
      "Epoch 3, Sample 490000, Loss: 0.1933\n",
      "Epoch 3, Sample 500000, Loss: 0.1044\n",
      "Epoch 3, Sample 510000, Loss: 0.2096\n",
      "Epoch 3, Sample 520000, Loss: 0.1441\n",
      "Epoch 3, Sample 530000, Loss: 0.2331\n",
      "Epoch 3, Sample 540000, Loss: 0.3256\n",
      "Epoch 3, Sample 550000, Loss: 0.2619\n",
      "Epoch 3, Sample 560000, Loss: 0.2893\n",
      "Epoch 3, Sample 570000, Loss: 0.1104\n",
      "Epoch 3, Sample 580000, Loss: 0.0086\n",
      "Epoch 3, Sample 590000, Loss: 0.0614\n",
      "Epoch 3, Sample 600000, Loss: 0.1809\n",
      "Epoch 3, Sample 610000, Loss: 0.1539\n",
      "Epoch 3, Sample 620000, Loss: 0.2438\n",
      "Epoch 3, Sample 630000, Loss: 0.2074\n",
      "Epoch 3, Sample 640000, Loss: 0.3055\n",
      "Epoch 3, Sample 650000, Loss: 0.2682\n",
      "Epoch 3, Sample 660000, Loss: 0.2934\n",
      "Epoch 3, Sample 670000, Loss: 0.1806\n",
      "Epoch 3, Sample 680000, Loss: 0.0595\n",
      "Epoch 3, Sample 690000, Loss: 0.0970\n",
      "Epoch 3, Sample 700000, Loss: 0.3230\n",
      "Epoch 3, Sample 710000, Loss: 0.2769\n",
      "Epoch 3, Sample 720000, Loss: 0.2473\n",
      "Epoch 3, Sample 730000, Loss: 0.2820\n",
      "Epoch 3, Sample 740000, Loss: 0.2363\n",
      "Epoch 3, Sample 750000, Loss: 0.0228\n",
      "Epoch 3, Sample 760000, Loss: 0.2543\n",
      "Epoch 3, Sample 770000, Loss: 0.2366\n",
      "Epoch 3, Sample 780000, Loss: 0.3254\n",
      "Epoch 3, Sample 790000, Loss: 0.2718\n",
      "Epoch 3, Sample 800000, Loss: 0.2257\n",
      "Epoch 3, Sample 810000, Loss: 0.2213\n",
      "Epoch 3, Sample 820000, Loss: 0.1955\n",
      "Epoch 3, Sample 830000, Loss: 0.1304\n",
      "Epoch 3, Sample 840000, Loss: 0.3349\n",
      "Epoch 3, Sample 850000, Loss: 0.1291\n",
      "Epoch 3, Sample 860000, Loss: 0.1479\n",
      "Epoch 3, Sample 870000, Loss: 0.2505\n",
      "Epoch 3, Sample 880000, Loss: 0.0756\n",
      "Epoch 3, Sample 890000, Loss: 0.2705\n",
      "Epoch 3, Sample 900000, Loss: 0.2097\n",
      "Epoch 3, Sample 910000, Loss: 0.2519\n",
      "Epoch 3, Sample 920000, Loss: 0.1481\n",
      "Epoch 3, Sample 930000, Loss: 0.2320\n",
      "Epoch 3, Sample 940000, Loss: 0.4209\n",
      "Epoch 3, Sample 950000, Loss: 0.1917\n",
      "Epoch 3, Sample 960000, Loss: 0.1569\n",
      "Epoch 3, Sample 970000, Loss: 0.0705\n",
      "Epoch 3, Sample 980000, Loss: 0.2505\n",
      "Epoch 3, Sample 990000, Loss: 0.5653\n",
      "Epoch 3, Sample 1000000, Loss: 0.1979\n",
      "Epoch 3, Sample 1010000, Loss: 0.0496\n",
      "Epoch 3, Sample 1020000, Loss: 0.2409\n",
      "Epoch 3, Sample 1030000, Loss: 0.1901\n",
      "Epoch 3, Sample 1040000, Loss: 0.1629\n",
      "Epoch 3, Sample 1050000, Loss: 0.1615\n",
      "Epoch 3, Sample 1060000, Loss: 0.2525\n",
      "Epoch 3, Sample 1070000, Loss: 0.1412\n",
      "Epoch 3, Sample 1080000, Loss: 0.1312\n",
      "Epoch 3, Sample 1090000, Loss: 0.2194\n",
      "Epoch 3, Sample 1100000, Loss: 0.2505\n",
      "Epoch 3, Sample 1110000, Loss: 0.1523\n",
      "Epoch 3, Sample 1120000, Loss: 0.1476\n",
      "Epoch 3, Sample 1130000, Loss: 0.1254\n",
      "Epoch 3, Sample 1140000, Loss: 0.0219\n",
      "Epoch 3, Sample 1150000, Loss: 0.0758\n",
      "Epoch 3, Sample 1160000, Loss: 0.2835\n",
      "Epoch 3, Sample 1170000, Loss: 0.2167\n",
      "Epoch 3, Sample 1180000, Loss: 0.2411\n",
      "Epoch 3, Sample 1190000, Loss: 0.1443\n",
      "Epoch 3, Sample 1200000, Loss: 0.5532\n",
      "Epoch 3, Sample 1210000, Loss: 0.1893\n",
      "Epoch 3, Sample 1220000, Loss: 0.2805\n",
      "Epoch 3, Sample 1230000, Loss: 0.2099\n",
      "Epoch 3, Sample 1240000, Loss: 0.2814\n",
      "Epoch 3, Sample 1250000, Loss: 0.2114\n",
      "Epoch 3, Sample 1260000, Loss: 0.2431\n",
      "Epoch 3, Sample 1270000, Loss: 0.2450\n",
      "Epoch 3, Sample 1280000, Loss: 0.0094\n",
      "Epoch 3, Sample 1290000, Loss: 0.0308\n",
      "Epoch 3, Sample 1300000, Loss: 0.2066\n",
      "Epoch 3, Sample 1310000, Loss: 0.2123\n",
      "Epoch 3, Sample 1320000, Loss: 0.3261\n",
      "Epoch 3, Sample 1330000, Loss: 0.0204\n",
      "Epoch 3, Sample 1340000, Loss: 0.2424\n",
      "Epoch 3, Sample 1350000, Loss: 0.2170\n",
      "Epoch 3, Sample 1360000, Loss: 0.1083\n",
      "Epoch 3, Sample 1370000, Loss: 0.2035\n",
      "Epoch 3, Sample 1380000, Loss: 0.1786\n",
      "Epoch 3, Sample 1390000, Loss: 0.0167\n",
      "Epoch 3, Sample 1400000, Loss: 0.2545\n",
      "Epoch 3, Sample 1410000, Loss: 0.1860\n",
      "Epoch 3, Sample 1420000, Loss: 0.3792\n",
      "Epoch 3, Sample 1430000, Loss: 0.0383\n",
      "Epoch 3, Sample 1440000, Loss: 0.0200\n",
      "Epoch 3, Sample 1450000, Loss: 0.2390\n",
      "Epoch 3, Sample 1460000, Loss: 0.2540\n",
      "Epoch 3, Sample 1470000, Loss: 0.1723\n",
      "Epoch 3, Sample 1480000, Loss: 0.2744\n",
      "Epoch 3, Sample 1490000, Loss: 0.1825\n",
      "Epoch 3, Sample 1500000, Loss: 0.1921\n",
      "Epoch 3, Sample 1510000, Loss: 0.1797\n",
      "Epoch 3, Sample 1520000, Loss: 0.1401\n",
      "Epoch 3, Sample 1530000, Loss: 0.1407\n",
      "Epoch 3, Sample 1540000, Loss: 0.2255\n",
      "Epoch 3, Sample 1550000, Loss: 0.2422\n",
      "Epoch 3, Sample 1560000, Loss: 0.2919\n",
      "Epoch 3, Sample 1570000, Loss: 0.1807\n",
      "Epoch 3, Sample 1580000, Loss: 0.0067\n",
      "Epoch 3, Sample 1590000, Loss: 0.0586\n",
      "Epoch 3, Sample 1600000, Loss: 0.2395\n",
      "Epoch 3, Sample 1610000, Loss: 0.0032\n",
      "Epoch 3, Sample 1620000, Loss: 0.2652\n",
      "Epoch 3, Sample 1630000, Loss: 0.0801\n",
      "Epoch 3, Sample 1640000, Loss: 0.2340\n",
      "Epoch 3, Sample 1650000, Loss: 0.1725\n",
      "Epoch 3, Sample 1660000, Loss: 0.0958\n",
      "Epoch 3, Sample 1670000, Loss: 0.0664\n",
      "Epoch 3, Sample 1680000, Loss: 0.2232\n",
      "Epoch 3, Sample 1690000, Loss: 0.1168\n",
      "Epoch 3, Sample 1700000, Loss: 0.0174\n",
      "Epoch 3, Sample 1710000, Loss: 0.1007\n",
      "Epoch 3, Sample 1720000, Loss: 0.1942\n",
      "Epoch 3, Sample 1730000, Loss: 0.0409\n",
      "Epoch 3, Sample 1740000, Loss: 0.0174\n",
      "Epoch 3, Sample 1750000, Loss: 0.2930\n",
      "Epoch 3, Sample 1760000, Loss: 0.2853\n",
      "Epoch 3, Sample 1770000, Loss: 0.0189\n",
      "Epoch 3, Sample 1780000, Loss: 0.2307\n",
      "Epoch 3, Sample 1790000, Loss: 0.2266\n",
      "Epoch 3, Sample 1800000, Loss: 0.2265\n",
      "Epoch 3, Sample 1810000, Loss: 0.0401\n",
      "Epoch 3, Sample 1820000, Loss: 0.0889\n",
      "Epoch 3, Sample 1830000, Loss: 0.1827\n",
      "Epoch 3, Sample 1840000, Loss: 0.1735\n",
      "Epoch 3, Sample 1850000, Loss: 0.2402\n",
      "Epoch 3, Sample 1860000, Loss: 0.2321\n",
      "Epoch 3, Sample 1870000, Loss: 0.2662\n",
      "Epoch 3, Sample 1880000, Loss: 0.2381\n",
      "Epoch 3, Sample 1890000, Loss: 0.2735\n",
      "Epoch 3, Sample 1900000, Loss: 0.2073\n",
      "Epoch 3, Sample 1910000, Loss: 0.2113\n",
      "Epoch 3, Sample 1920000, Loss: 0.2934\n",
      "Epoch 3, Sample 1930000, Loss: 0.2118\n",
      "Epoch 3, Sample 1940000, Loss: 0.1531\n",
      "Epoch 3, Sample 1950000, Loss: 0.2056\n",
      "Epoch 3, Sample 1960000, Loss: 0.1891\n",
      "Epoch 3, Sample 1970000, Loss: 0.1497\n",
      "Epoch 3, Sample 1980000, Loss: 0.2948\n",
      "Epoch 3, Sample 1990000, Loss: 0.2728\n",
      "Epoch 3, Sample 2000000, Loss: 0.2296\n",
      "Epoch 3, Sample 2010000, Loss: 0.3970\n",
      "Epoch 3, Sample 2020000, Loss: 0.2717\n",
      "Epoch 3, Sample 2030000, Loss: 0.2757\n",
      "Epoch 3, Sample 2040000, Loss: 0.2417\n",
      "Epoch 3, Sample 2050000, Loss: 0.2280\n",
      "Epoch 3, Sample 2060000, Loss: 0.1682\n",
      "Epoch 3, Sample 2070000, Loss: 0.1427\n",
      "Epoch 3, Sample 2080000, Loss: 0.0425\n",
      "Epoch 3, Sample 2090000, Loss: 0.2015\n",
      "Epoch 3, Sample 2100000, Loss: 0.2219\n",
      "Epoch 3, Sample 2110000, Loss: 0.0122\n",
      "Epoch 3, Sample 2120000, Loss: 0.2076\n",
      "Epoch 3, Sample 2130000, Loss: 0.2903\n",
      "Epoch 3, Sample 2140000, Loss: 0.2803\n",
      "Epoch 3, Sample 2150000, Loss: 0.1601\n",
      "Epoch 3, Sample 2160000, Loss: 0.2408\n",
      "Epoch 3, Sample 2170000, Loss: 0.1888\n",
      "Epoch 3, Sample 2180000, Loss: 0.1536\n",
      "Epoch 3, Sample 2190000, Loss: 0.3017\n",
      "Epoch 3, Sample 2200000, Loss: 0.2660\n",
      "Epoch 3, Sample 2210000, Loss: 0.2216\n",
      "Epoch 3, Sample 2220000, Loss: 0.0227\n",
      "Epoch 3, Sample 2230000, Loss: 0.2455\n",
      "Epoch 3, Sample 2240000, Loss: 0.1436\n",
      "Epoch 3, Sample 2250000, Loss: 0.1695\n",
      "Epoch 3, Sample 2260000, Loss: 0.1780\n",
      "Epoch 3, Sample 2270000, Loss: 0.0668\n",
      "Epoch 3, Sample 2280000, Loss: 0.0125\n",
      "Epoch 3, Sample 2290000, Loss: 0.0236\n",
      "Epoch 3, Sample 2300000, Loss: 0.1365\n",
      "Epoch 3, Sample 2310000, Loss: 0.1574\n",
      "Epoch 3, Sample 2320000, Loss: 0.1401\n",
      "Epoch 3, Sample 2330000, Loss: 0.0561\n",
      "Epoch 3, Sample 2340000, Loss: 0.2593\n",
      "Epoch 3, Sample 2350000, Loss: 0.2510\n",
      "Epoch 3, Sample 2360000, Loss: 0.3476\n",
      "Epoch 3, Sample 2370000, Loss: 0.1458\n",
      "Epoch 3, Sample 2380000, Loss: 0.2412\n",
      "Epoch 3, Sample 2390000, Loss: 0.1466\n",
      "Epoch 3, Sample 2400000, Loss: 0.2447\n",
      "Epoch 3, Sample 2410000, Loss: 0.0141\n",
      "Epoch 3, Sample 2420000, Loss: 0.2352\n",
      "Epoch 3, Sample 2430000, Loss: 0.3454\n",
      "Epoch 3, Sample 2440000, Loss: 0.1884\n",
      "Epoch 3, Sample 2450000, Loss: 0.1911\n",
      "Epoch 3, Sample 2460000, Loss: 0.1783\n",
      "Epoch 3, Sample 2470000, Loss: 0.1356\n",
      "Epoch 3, Sample 2480000, Loss: 0.2426\n",
      "Epoch 3, Sample 2490000, Loss: 0.1142\n",
      "Epoch 3, Sample 2500000, Loss: 0.2790\n",
      "Epoch 3, Sample 2510000, Loss: 0.0061\n",
      "Epoch 3, Sample 2520000, Loss: 0.2337\n",
      "Epoch 3, Sample 2530000, Loss: 0.6048\n",
      "Epoch 3, Sample 2540000, Loss: 0.4367\n",
      "Epoch 3, Sample 2550000, Loss: 0.2802\n",
      "Epoch 3, Sample 2560000, Loss: 0.0399\n",
      "Epoch 3, Sample 2570000, Loss: 0.0124\n",
      "Epoch 3, Sample 2580000, Loss: 0.1596\n",
      "Epoch 3, Sample 2590000, Loss: 0.1822\n",
      "Epoch 3, Sample 2600000, Loss: 0.1788\n",
      "Epoch 3, Sample 2610000, Loss: 0.1949\n",
      "Epoch 3, Sample 2620000, Loss: 0.2322\n",
      "Epoch 3, Sample 2630000, Loss: 0.2607\n",
      "Epoch 3, Sample 2640000, Loss: 0.1677\n",
      "Epoch 3, Sample 2650000, Loss: 0.1954\n",
      "Epoch 3, Sample 2660000, Loss: 0.1419\n",
      "Epoch 3, Sample 2670000, Loss: 0.3176\n",
      "Epoch 3, Sample 2680000, Loss: 0.2017\n",
      "Epoch 3, Sample 2690000, Loss: 0.1554\n",
      "Epoch 3, Sample 2700000, Loss: 0.1831\n",
      "Epoch 3, Sample 2710000, Loss: 0.0094\n",
      "Epoch 3, Sample 2720000, Loss: 0.3193\n",
      "Epoch 3, Sample 2730000, Loss: 0.2187\n",
      "Epoch 3, Sample 2740000, Loss: 0.1911\n",
      "Epoch 3, Sample 2750000, Loss: 0.3512\n",
      "Epoch 3, Sample 2760000, Loss: 0.2014\n",
      "Epoch 3, Sample 2770000, Loss: 0.0405\n",
      "Epoch 3, Sample 2780000, Loss: 0.0303\n",
      "Epoch 3, Sample 2790000, Loss: 0.1672\n",
      "Epoch 3, Sample 2800000, Loss: 0.1679\n",
      "Epoch 3, Sample 2810000, Loss: 0.1414\n",
      "Epoch 3, Sample 2820000, Loss: 0.1831\n",
      "Epoch 3, Sample 2830000, Loss: 0.0783\n",
      "Epoch 3, Sample 2840000, Loss: 0.2420\n",
      "Epoch 3, Sample 2850000, Loss: 0.2382\n",
      "Epoch 3, Sample 2860000, Loss: 0.0094\n",
      "Epoch 3, Sample 2870000, Loss: 0.1546\n",
      "Epoch 3, Sample 2880000, Loss: 0.1984\n",
      "Epoch 3, Sample 2890000, Loss: 0.0372\n",
      "Epoch 3, Sample 2900000, Loss: 0.0033\n",
      "Epoch 3, Sample 2910000, Loss: 0.0635\n",
      "Epoch 3, Sample 2920000, Loss: 0.3309\n",
      "Epoch 3, Sample 2930000, Loss: 0.2635\n",
      "Epoch 3, Sample 2940000, Loss: 0.1696\n",
      "Epoch 3, Sample 2950000, Loss: 0.0207\n",
      "Epoch 3, Sample 2960000, Loss: 0.2166\n",
      "Epoch 3, Sample 2970000, Loss: 0.2139\n",
      "Epoch 3, Sample 2980000, Loss: 0.2714\n",
      "Epoch 3, Sample 2990000, Loss: 0.1722\n",
      "Epoch 3, Sample 3000000, Loss: 0.3249\n",
      "Epoch 3, Sample 3010000, Loss: 0.1695\n",
      "Epoch 3, Sample 3020000, Loss: 0.2693\n",
      "Epoch 3, Sample 3030000, Loss: 0.0048\n",
      "Epoch 3, Sample 3040000, Loss: 0.1770\n",
      "Epoch 3, Sample 3050000, Loss: 0.1461\n",
      "Epoch 3, Sample 3060000, Loss: 0.0040\n",
      "Epoch 3, Sample 3070000, Loss: 0.1904\n",
      "Epoch 3/10, Average Loss: 0.1866\n",
      "Epoch 4, Sample 0, Loss: 0.0179\n",
      "Epoch 4, Sample 10000, Loss: 0.1500\n",
      "Epoch 4, Sample 20000, Loss: 0.2487\n",
      "Epoch 4, Sample 30000, Loss: 0.2137\n",
      "Epoch 4, Sample 40000, Loss: 0.1569\n",
      "Epoch 4, Sample 50000, Loss: 0.0313\n",
      "Epoch 4, Sample 60000, Loss: 0.1782\n",
      "Epoch 4, Sample 70000, Loss: 0.2695\n",
      "Epoch 4, Sample 80000, Loss: 0.2279\n",
      "Epoch 4, Sample 90000, Loss: 0.0190\n",
      "Epoch 4, Sample 100000, Loss: 0.1606\n",
      "Epoch 4, Sample 110000, Loss: 0.1760\n",
      "Epoch 4, Sample 120000, Loss: 0.0277\n",
      "Epoch 4, Sample 130000, Loss: 0.1959\n",
      "Epoch 4, Sample 140000, Loss: 0.2513\n",
      "Epoch 4, Sample 150000, Loss: 0.3830\n",
      "Epoch 4, Sample 160000, Loss: 0.2010\n",
      "Epoch 4, Sample 170000, Loss: 0.1526\n",
      "Epoch 4, Sample 180000, Loss: 0.2227\n",
      "Epoch 4, Sample 190000, Loss: 0.2329\n",
      "Epoch 4, Sample 200000, Loss: 0.1939\n",
      "Epoch 4, Sample 210000, Loss: 0.3400\n",
      "Epoch 4, Sample 220000, Loss: 0.3144\n",
      "Epoch 4, Sample 230000, Loss: 0.1802\n",
      "Epoch 4, Sample 240000, Loss: 0.2518\n",
      "Epoch 4, Sample 250000, Loss: 0.2589\n",
      "Epoch 4, Sample 260000, Loss: 0.2147\n",
      "Epoch 4, Sample 270000, Loss: 0.2316\n",
      "Epoch 4, Sample 280000, Loss: 0.3318\n",
      "Epoch 4, Sample 290000, Loss: 0.1561\n",
      "Epoch 4, Sample 300000, Loss: 0.2697\n",
      "Epoch 4, Sample 310000, Loss: 0.0041\n",
      "Epoch 4, Sample 320000, Loss: 0.2373\n",
      "Epoch 4, Sample 330000, Loss: 0.0207\n",
      "Epoch 4, Sample 340000, Loss: 0.1496\n",
      "Epoch 4, Sample 350000, Loss: 0.2246\n",
      "Epoch 4, Sample 360000, Loss: 0.7262\n",
      "Epoch 4, Sample 370000, Loss: 0.1214\n",
      "Epoch 4, Sample 380000, Loss: 0.1935\n",
      "Epoch 4, Sample 390000, Loss: 0.3072\n",
      "Epoch 4, Sample 400000, Loss: 0.1659\n",
      "Epoch 4, Sample 410000, Loss: 0.0880\n",
      "Epoch 4, Sample 420000, Loss: 0.5188\n",
      "Epoch 4, Sample 430000, Loss: 0.3073\n",
      "Epoch 4, Sample 440000, Loss: 0.2370\n",
      "Epoch 4, Sample 450000, Loss: 0.2360\n",
      "Epoch 4, Sample 460000, Loss: 0.2650\n",
      "Epoch 4, Sample 470000, Loss: 0.2246\n",
      "Epoch 4, Sample 480000, Loss: 0.1869\n",
      "Epoch 4, Sample 490000, Loss: 0.1743\n",
      "Epoch 4, Sample 500000, Loss: 0.0845\n",
      "Epoch 4, Sample 510000, Loss: 0.1945\n",
      "Epoch 4, Sample 520000, Loss: 0.1231\n",
      "Epoch 4, Sample 530000, Loss: 0.2205\n",
      "Epoch 4, Sample 540000, Loss: 0.3434\n",
      "Epoch 4, Sample 550000, Loss: 0.2560\n",
      "Epoch 4, Sample 560000, Loss: 0.3015\n",
      "Epoch 4, Sample 570000, Loss: 0.0921\n",
      "Epoch 4, Sample 580000, Loss: 0.0049\n",
      "Epoch 4, Sample 590000, Loss: 0.0464\n",
      "Epoch 4, Sample 600000, Loss: 0.1549\n",
      "Epoch 4, Sample 610000, Loss: 0.1226\n",
      "Epoch 4, Sample 620000, Loss: 0.2418\n",
      "Epoch 4, Sample 630000, Loss: 0.1842\n",
      "Epoch 4, Sample 640000, Loss: 0.3117\n",
      "Epoch 4, Sample 650000, Loss: 0.2653\n",
      "Epoch 4, Sample 660000, Loss: 0.3087\n",
      "Epoch 4, Sample 670000, Loss: 0.1609\n",
      "Epoch 4, Sample 680000, Loss: 0.0417\n",
      "Epoch 4, Sample 690000, Loss: 0.0775\n",
      "Epoch 4, Sample 700000, Loss: 0.3349\n",
      "Epoch 4, Sample 710000, Loss: 0.2889\n",
      "Epoch 4, Sample 720000, Loss: 0.2444\n",
      "Epoch 4, Sample 730000, Loss: 0.2811\n",
      "Epoch 4, Sample 740000, Loss: 0.2302\n",
      "Epoch 4, Sample 750000, Loss: 0.0138\n",
      "Epoch 4, Sample 760000, Loss: 0.2507\n",
      "Epoch 4, Sample 770000, Loss: 0.2205\n",
      "Epoch 4, Sample 780000, Loss: 0.3536\n",
      "Epoch 4, Sample 790000, Loss: 0.2911\n",
      "Epoch 4, Sample 800000, Loss: 0.2226\n",
      "Epoch 4, Sample 810000, Loss: 0.2098\n",
      "Epoch 4, Sample 820000, Loss: 0.1702\n",
      "Epoch 4, Sample 830000, Loss: 0.1008\n",
      "Epoch 4, Sample 840000, Loss: 0.3626\n",
      "Epoch 4, Sample 850000, Loss: 0.1049\n",
      "Epoch 4, Sample 860000, Loss: 0.1231\n",
      "Epoch 4, Sample 870000, Loss: 0.2495\n",
      "Epoch 4, Sample 880000, Loss: 0.0531\n",
      "Epoch 4, Sample 890000, Loss: 0.2619\n",
      "Epoch 4, Sample 900000, Loss: 0.1974\n",
      "Epoch 4, Sample 910000, Loss: 0.2271\n",
      "Epoch 4, Sample 920000, Loss: 0.1248\n",
      "Epoch 4, Sample 930000, Loss: 0.2298\n",
      "Epoch 4, Sample 940000, Loss: 0.4689\n",
      "Epoch 4, Sample 950000, Loss: 0.1729\n",
      "Epoch 4, Sample 960000, Loss: 0.1306\n",
      "Epoch 4, Sample 970000, Loss: 0.0508\n",
      "Epoch 4, Sample 980000, Loss: 0.2570\n",
      "Epoch 4, Sample 990000, Loss: 0.6168\n",
      "Epoch 4, Sample 1000000, Loss: 0.1782\n",
      "Epoch 4, Sample 1010000, Loss: 0.0373\n",
      "Epoch 4, Sample 1020000, Loss: 0.2474\n",
      "Epoch 4, Sample 1030000, Loss: 0.1721\n",
      "Epoch 4, Sample 1040000, Loss: 0.1408\n",
      "Epoch 4, Sample 1050000, Loss: 0.1334\n",
      "Epoch 4, Sample 1060000, Loss: 0.2504\n",
      "Epoch 4, Sample 1070000, Loss: 0.1174\n",
      "Epoch 4, Sample 1080000, Loss: 0.1075\n",
      "Epoch 4, Sample 1090000, Loss: 0.2113\n",
      "Epoch 4, Sample 1100000, Loss: 0.2614\n",
      "Epoch 4, Sample 1110000, Loss: 0.1274\n",
      "Epoch 4, Sample 1120000, Loss: 0.1309\n",
      "Epoch 4, Sample 1130000, Loss: 0.1042\n",
      "Epoch 4, Sample 1140000, Loss: 0.0148\n",
      "Epoch 4, Sample 1150000, Loss: 0.0612\n",
      "Epoch 4, Sample 1160000, Loss: 0.2943\n",
      "Epoch 4, Sample 1170000, Loss: 0.2028\n",
      "Epoch 4, Sample 1180000, Loss: 0.2452\n",
      "Epoch 4, Sample 1190000, Loss: 0.1285\n",
      "Epoch 4, Sample 1200000, Loss: 0.5904\n",
      "Epoch 4, Sample 1210000, Loss: 0.1666\n",
      "Epoch 4, Sample 1220000, Loss: 0.2901\n",
      "Epoch 4, Sample 1230000, Loss: 0.2032\n",
      "Epoch 4, Sample 1240000, Loss: 0.2766\n",
      "Epoch 4, Sample 1250000, Loss: 0.1995\n",
      "Epoch 4, Sample 1260000, Loss: 0.2326\n",
      "Epoch 4, Sample 1270000, Loss: 0.2453\n",
      "Epoch 4, Sample 1280000, Loss: 0.0070\n",
      "Epoch 4, Sample 1290000, Loss: 0.0215\n",
      "Epoch 4, Sample 1300000, Loss: 0.2012\n",
      "Epoch 4, Sample 1310000, Loss: 0.2095\n",
      "Epoch 4, Sample 1320000, Loss: 0.3390\n",
      "Epoch 4, Sample 1330000, Loss: 0.0152\n",
      "Epoch 4, Sample 1340000, Loss: 0.2425\n",
      "Epoch 4, Sample 1350000, Loss: 0.1880\n",
      "Epoch 4, Sample 1360000, Loss: 0.0819\n",
      "Epoch 4, Sample 1370000, Loss: 0.1905\n",
      "Epoch 4, Sample 1380000, Loss: 0.1554\n",
      "Epoch 4, Sample 1390000, Loss: 0.0117\n",
      "Epoch 4, Sample 1400000, Loss: 0.2505\n",
      "Epoch 4, Sample 1410000, Loss: 0.1631\n",
      "Epoch 4, Sample 1420000, Loss: 0.4159\n",
      "Epoch 4, Sample 1430000, Loss: 0.0281\n",
      "Epoch 4, Sample 1440000, Loss: 0.0121\n",
      "Epoch 4, Sample 1450000, Loss: 0.2300\n",
      "Epoch 4, Sample 1460000, Loss: 0.2406\n",
      "Epoch 4, Sample 1470000, Loss: 0.1544\n",
      "Epoch 4, Sample 1480000, Loss: 0.2887\n",
      "Epoch 4, Sample 1490000, Loss: 0.1605\n",
      "Epoch 4, Sample 1500000, Loss: 0.1684\n",
      "Epoch 4, Sample 1510000, Loss: 0.1662\n",
      "Epoch 4, Sample 1520000, Loss: 0.1186\n",
      "Epoch 4, Sample 1530000, Loss: 0.1160\n",
      "Epoch 4, Sample 1540000, Loss: 0.2074\n",
      "Epoch 4, Sample 1550000, Loss: 0.2391\n",
      "Epoch 4, Sample 1560000, Loss: 0.3158\n",
      "Epoch 4, Sample 1570000, Loss: 0.1675\n",
      "Epoch 4, Sample 1580000, Loss: 0.0045\n",
      "Epoch 4, Sample 1590000, Loss: 0.0415\n",
      "Epoch 4, Sample 1600000, Loss: 0.2347\n",
      "Epoch 4, Sample 1610000, Loss: 0.0018\n",
      "Epoch 4, Sample 1620000, Loss: 0.2549\n",
      "Epoch 4, Sample 1630000, Loss: 0.0654\n",
      "Epoch 4, Sample 1640000, Loss: 0.2367\n",
      "Epoch 4, Sample 1650000, Loss: 0.1489\n",
      "Epoch 4, Sample 1660000, Loss: 0.0812\n",
      "Epoch 4, Sample 1670000, Loss: 0.0482\n",
      "Epoch 4, Sample 1680000, Loss: 0.2095\n",
      "Epoch 4, Sample 1690000, Loss: 0.0915\n",
      "Epoch 4, Sample 1700000, Loss: 0.0121\n",
      "Epoch 4, Sample 1710000, Loss: 0.0821\n",
      "Epoch 4, Sample 1720000, Loss: 0.1819\n",
      "Epoch 4, Sample 1730000, Loss: 0.0272\n",
      "Epoch 4, Sample 1740000, Loss: 0.0126\n",
      "Epoch 4, Sample 1750000, Loss: 0.3097\n",
      "Epoch 4, Sample 1760000, Loss: 0.2968\n",
      "Epoch 4, Sample 1770000, Loss: 0.0137\n",
      "Epoch 4, Sample 1780000, Loss: 0.2203\n",
      "Epoch 4, Sample 1790000, Loss: 0.2122\n",
      "Epoch 4, Sample 1800000, Loss: 0.2220\n",
      "Epoch 4, Sample 1810000, Loss: 0.0280\n",
      "Epoch 4, Sample 1820000, Loss: 0.0672\n",
      "Epoch 4, Sample 1830000, Loss: 0.1674\n",
      "Epoch 4, Sample 1840000, Loss: 0.1579\n",
      "Epoch 4, Sample 1850000, Loss: 0.2219\n",
      "Epoch 4, Sample 1860000, Loss: 0.2249\n",
      "Epoch 4, Sample 1870000, Loss: 0.2690\n",
      "Epoch 4, Sample 1880000, Loss: 0.2380\n",
      "Epoch 4, Sample 1890000, Loss: 0.2761\n",
      "Epoch 4, Sample 1900000, Loss: 0.1977\n",
      "Epoch 4, Sample 1910000, Loss: 0.1935\n",
      "Epoch 4, Sample 1920000, Loss: 0.3037\n",
      "Epoch 4, Sample 1930000, Loss: 0.2104\n",
      "Epoch 4, Sample 1940000, Loss: 0.1298\n",
      "Epoch 4, Sample 1950000, Loss: 0.2011\n",
      "Epoch 4, Sample 1960000, Loss: 0.1714\n",
      "Epoch 4, Sample 1970000, Loss: 0.1301\n",
      "Epoch 4, Sample 1980000, Loss: 0.3111\n",
      "Epoch 4, Sample 1990000, Loss: 0.2818\n",
      "Epoch 4, Sample 2000000, Loss: 0.2128\n",
      "Epoch 4, Sample 2010000, Loss: 0.4297\n",
      "Epoch 4, Sample 2020000, Loss: 0.2621\n",
      "Epoch 4, Sample 2030000, Loss: 0.2782\n",
      "Epoch 4, Sample 2040000, Loss: 0.2394\n",
      "Epoch 4, Sample 2050000, Loss: 0.2175\n",
      "Epoch 4, Sample 2060000, Loss: 0.1477\n",
      "Epoch 4, Sample 2070000, Loss: 0.1174\n",
      "Epoch 4, Sample 2080000, Loss: 0.0319\n",
      "Epoch 4, Sample 2090000, Loss: 0.1926\n",
      "Epoch 4, Sample 2100000, Loss: 0.2179\n",
      "Epoch 4, Sample 2110000, Loss: 0.0082\n",
      "Epoch 4, Sample 2120000, Loss: 0.1848\n",
      "Epoch 4, Sample 2130000, Loss: 0.2903\n",
      "Epoch 4, Sample 2140000, Loss: 0.2866\n",
      "Epoch 4, Sample 2150000, Loss: 0.1445\n",
      "Epoch 4, Sample 2160000, Loss: 0.2364\n",
      "Epoch 4, Sample 2170000, Loss: 0.1636\n",
      "Epoch 4, Sample 2180000, Loss: 0.1379\n",
      "Epoch 4, Sample 2190000, Loss: 0.3133\n",
      "Epoch 4, Sample 2200000, Loss: 0.2716\n",
      "Epoch 4, Sample 2210000, Loss: 0.2061\n",
      "Epoch 4, Sample 2220000, Loss: 0.0143\n",
      "Epoch 4, Sample 2230000, Loss: 0.2314\n",
      "Epoch 4, Sample 2240000, Loss: 0.1218\n",
      "Epoch 4, Sample 2250000, Loss: 0.1462\n",
      "Epoch 4, Sample 2260000, Loss: 0.1635\n",
      "Epoch 4, Sample 2270000, Loss: 0.0499\n",
      "Epoch 4, Sample 2280000, Loss: 0.0100\n",
      "Epoch 4, Sample 2290000, Loss: 0.0174\n",
      "Epoch 4, Sample 2300000, Loss: 0.1155\n",
      "Epoch 4, Sample 2310000, Loss: 0.1332\n",
      "Epoch 4, Sample 2320000, Loss: 0.1195\n",
      "Epoch 4, Sample 2330000, Loss: 0.0392\n",
      "Epoch 4, Sample 2340000, Loss: 0.2613\n",
      "Epoch 4, Sample 2350000, Loss: 0.2472\n",
      "Epoch 4, Sample 2360000, Loss: 0.3616\n",
      "Epoch 4, Sample 2370000, Loss: 0.1228\n",
      "Epoch 4, Sample 2380000, Loss: 0.2367\n",
      "Epoch 4, Sample 2390000, Loss: 0.1237\n",
      "Epoch 4, Sample 2400000, Loss: 0.2387\n",
      "Epoch 4, Sample 2410000, Loss: 0.0096\n",
      "Epoch 4, Sample 2420000, Loss: 0.2377\n",
      "Epoch 4, Sample 2430000, Loss: 0.3705\n",
      "Epoch 4, Sample 2440000, Loss: 0.1710\n",
      "Epoch 4, Sample 2450000, Loss: 0.1655\n",
      "Epoch 4, Sample 2460000, Loss: 0.1514\n",
      "Epoch 4, Sample 2470000, Loss: 0.1108\n",
      "Epoch 4, Sample 2480000, Loss: 0.2367\n",
      "Epoch 4, Sample 2490000, Loss: 0.0960\n",
      "Epoch 4, Sample 2500000, Loss: 0.2933\n",
      "Epoch 4, Sample 2510000, Loss: 0.0057\n",
      "Epoch 4, Sample 2520000, Loss: 0.2176\n",
      "Epoch 4, Sample 2530000, Loss: 0.6507\n",
      "Epoch 4, Sample 2540000, Loss: 0.4674\n",
      "Epoch 4, Sample 2550000, Loss: 0.2878\n",
      "Epoch 4, Sample 2560000, Loss: 0.0282\n",
      "Epoch 4, Sample 2570000, Loss: 0.0092\n",
      "Epoch 4, Sample 2580000, Loss: 0.1480\n",
      "Epoch 4, Sample 2590000, Loss: 0.1645\n",
      "Epoch 4, Sample 2600000, Loss: 0.1632\n",
      "Epoch 4, Sample 2610000, Loss: 0.1798\n",
      "Epoch 4, Sample 2620000, Loss: 0.2268\n",
      "Epoch 4, Sample 2630000, Loss: 0.2591\n",
      "Epoch 4, Sample 2640000, Loss: 0.1503\n",
      "Epoch 4, Sample 2650000, Loss: 0.1767\n",
      "Epoch 4, Sample 2660000, Loss: 0.1224\n",
      "Epoch 4, Sample 2670000, Loss: 0.3442\n",
      "Epoch 4, Sample 2680000, Loss: 0.1804\n",
      "Epoch 4, Sample 2690000, Loss: 0.1356\n",
      "Epoch 4, Sample 2700000, Loss: 0.1623\n",
      "Epoch 4, Sample 2710000, Loss: 0.0075\n",
      "Epoch 4, Sample 2720000, Loss: 0.3341\n",
      "Epoch 4, Sample 2730000, Loss: 0.2093\n",
      "Epoch 4, Sample 2740000, Loss: 0.1837\n",
      "Epoch 4, Sample 2750000, Loss: 0.3799\n",
      "Epoch 4, Sample 2760000, Loss: 0.1916\n",
      "Epoch 4, Sample 2770000, Loss: 0.0264\n",
      "Epoch 4, Sample 2780000, Loss: 0.0245\n",
      "Epoch 4, Sample 2790000, Loss: 0.1540\n",
      "Epoch 4, Sample 2800000, Loss: 0.1468\n",
      "Epoch 4, Sample 2810000, Loss: 0.1214\n",
      "Epoch 4, Sample 2820000, Loss: 0.1688\n",
      "Epoch 4, Sample 2830000, Loss: 0.0605\n",
      "Epoch 4, Sample 2840000, Loss: 0.2436\n",
      "Epoch 4, Sample 2850000, Loss: 0.2216\n",
      "Epoch 4, Sample 2860000, Loss: 0.0066\n",
      "Epoch 4, Sample 2870000, Loss: 0.1320\n",
      "Epoch 4, Sample 2880000, Loss: 0.1839\n",
      "Epoch 4, Sample 2890000, Loss: 0.0282\n",
      "Epoch 4, Sample 2900000, Loss: 0.0028\n",
      "Epoch 4, Sample 2910000, Loss: 0.0471\n",
      "Epoch 4, Sample 2920000, Loss: 0.3485\n",
      "Epoch 4, Sample 2930000, Loss: 0.2564\n",
      "Epoch 4, Sample 2940000, Loss: 0.1510\n",
      "Epoch 4, Sample 2950000, Loss: 0.0134\n",
      "Epoch 4, Sample 2960000, Loss: 0.2055\n",
      "Epoch 4, Sample 2970000, Loss: 0.1932\n",
      "Epoch 4, Sample 2980000, Loss: 0.2798\n",
      "Epoch 4, Sample 2990000, Loss: 0.1485\n",
      "Epoch 4, Sample 3000000, Loss: 0.3408\n",
      "Epoch 4, Sample 3010000, Loss: 0.1472\n",
      "Epoch 4, Sample 3020000, Loss: 0.2610\n",
      "Epoch 4, Sample 3030000, Loss: 0.0033\n",
      "Epoch 4, Sample 3040000, Loss: 0.1554\n",
      "Epoch 4, Sample 3050000, Loss: 0.1247\n",
      "Epoch 4, Sample 3060000, Loss: 0.0027\n",
      "Epoch 4, Sample 3070000, Loss: 0.1753\n",
      "Epoch 4/10, Average Loss: 0.1776\n",
      "Epoch 5, Sample 0, Loss: 0.0138\n",
      "Epoch 5, Sample 10000, Loss: 0.1276\n",
      "Epoch 5, Sample 20000, Loss: 0.2473\n",
      "Epoch 5, Sample 30000, Loss: 0.2127\n",
      "Epoch 5, Sample 40000, Loss: 0.1350\n",
      "Epoch 5, Sample 50000, Loss: 0.0241\n",
      "Epoch 5, Sample 60000, Loss: 0.1610\n",
      "Epoch 5, Sample 70000, Loss: 0.2798\n",
      "Epoch 5, Sample 80000, Loss: 0.2111\n",
      "Epoch 5, Sample 90000, Loss: 0.0148\n",
      "Epoch 5, Sample 100000, Loss: 0.1394\n",
      "Epoch 5, Sample 110000, Loss: 0.1654\n",
      "Epoch 5, Sample 120000, Loss: 0.0199\n",
      "Epoch 5, Sample 130000, Loss: 0.1862\n",
      "Epoch 5, Sample 140000, Loss: 0.2533\n",
      "Epoch 5, Sample 150000, Loss: 0.4179\n",
      "Epoch 5, Sample 160000, Loss: 0.1947\n",
      "Epoch 5, Sample 170000, Loss: 0.1304\n",
      "Epoch 5, Sample 180000, Loss: 0.2119\n",
      "Epoch 5, Sample 190000, Loss: 0.2230\n",
      "Epoch 5, Sample 200000, Loss: 0.1847\n",
      "Epoch 5, Sample 210000, Loss: 0.3609\n",
      "Epoch 5, Sample 220000, Loss: 0.3221\n",
      "Epoch 5, Sample 230000, Loss: 0.1667\n",
      "Epoch 5, Sample 240000, Loss: 0.2485\n",
      "Epoch 5, Sample 250000, Loss: 0.2542\n",
      "Epoch 5, Sample 260000, Loss: 0.2004\n",
      "Epoch 5, Sample 270000, Loss: 0.2322\n",
      "Epoch 5, Sample 280000, Loss: 0.3522\n",
      "Epoch 5, Sample 290000, Loss: 0.1364\n",
      "Epoch 5, Sample 300000, Loss: 0.2876\n",
      "Epoch 5, Sample 310000, Loss: 0.0031\n",
      "Epoch 5, Sample 320000, Loss: 0.2305\n",
      "Epoch 5, Sample 330000, Loss: 0.0139\n",
      "Epoch 5, Sample 340000, Loss: 0.1255\n",
      "Epoch 5, Sample 350000, Loss: 0.2184\n",
      "Epoch 5, Sample 360000, Loss: 0.7646\n",
      "Epoch 5, Sample 370000, Loss: 0.1023\n",
      "Epoch 5, Sample 380000, Loss: 0.1711\n",
      "Epoch 5, Sample 390000, Loss: 0.3208\n",
      "Epoch 5, Sample 400000, Loss: 0.1425\n",
      "Epoch 5, Sample 410000, Loss: 0.0734\n",
      "Epoch 5, Sample 420000, Loss: 0.5565\n",
      "Epoch 5, Sample 430000, Loss: 0.3186\n",
      "Epoch 5, Sample 440000, Loss: 0.2324\n",
      "Epoch 5, Sample 450000, Loss: 0.2307\n",
      "Epoch 5, Sample 460000, Loss: 0.2615\n",
      "Epoch 5, Sample 470000, Loss: 0.2154\n",
      "Epoch 5, Sample 480000, Loss: 0.1720\n",
      "Epoch 5, Sample 490000, Loss: 0.1589\n",
      "Epoch 5, Sample 500000, Loss: 0.0727\n",
      "Epoch 5, Sample 510000, Loss: 0.1815\n",
      "Epoch 5, Sample 520000, Loss: 0.1090\n",
      "Epoch 5, Sample 530000, Loss: 0.2075\n",
      "Epoch 5, Sample 540000, Loss: 0.3571\n",
      "Epoch 5, Sample 550000, Loss: 0.2504\n",
      "Epoch 5, Sample 560000, Loss: 0.3121\n",
      "Epoch 5, Sample 570000, Loss: 0.0817\n",
      "Epoch 5, Sample 580000, Loss: 0.0034\n",
      "Epoch 5, Sample 590000, Loss: 0.0388\n",
      "Epoch 5, Sample 600000, Loss: 0.1333\n",
      "Epoch 5, Sample 610000, Loss: 0.0984\n",
      "Epoch 5, Sample 620000, Loss: 0.2375\n",
      "Epoch 5, Sample 630000, Loss: 0.1645\n",
      "Epoch 5, Sample 640000, Loss: 0.3168\n",
      "Epoch 5, Sample 650000, Loss: 0.2613\n",
      "Epoch 5, Sample 660000, Loss: 0.3224\n",
      "Epoch 5, Sample 670000, Loss: 0.1442\n",
      "Epoch 5, Sample 680000, Loss: 0.0322\n",
      "Epoch 5, Sample 690000, Loss: 0.0665\n",
      "Epoch 5, Sample 700000, Loss: 0.3443\n",
      "Epoch 5, Sample 710000, Loss: 0.3006\n",
      "Epoch 5, Sample 720000, Loss: 0.2411\n",
      "Epoch 5, Sample 730000, Loss: 0.2796\n",
      "Epoch 5, Sample 740000, Loss: 0.2263\n",
      "Epoch 5, Sample 750000, Loss: 0.0097\n",
      "Epoch 5, Sample 760000, Loss: 0.2459\n",
      "Epoch 5, Sample 770000, Loss: 0.2047\n",
      "Epoch 5, Sample 780000, Loss: 0.3799\n",
      "Epoch 5, Sample 790000, Loss: 0.3116\n",
      "Epoch 5, Sample 800000, Loss: 0.2184\n",
      "Epoch 5, Sample 810000, Loss: 0.1981\n",
      "Epoch 5, Sample 820000, Loss: 0.1490\n",
      "Epoch 5, Sample 830000, Loss: 0.0815\n",
      "Epoch 5, Sample 840000, Loss: 0.3870\n",
      "Epoch 5, Sample 850000, Loss: 0.0888\n",
      "Epoch 5, Sample 860000, Loss: 0.1034\n",
      "Epoch 5, Sample 870000, Loss: 0.2489\n",
      "Epoch 5, Sample 880000, Loss: 0.0400\n",
      "Epoch 5, Sample 890000, Loss: 0.2516\n",
      "Epoch 5, Sample 900000, Loss: 0.1864\n",
      "Epoch 5, Sample 910000, Loss: 0.2033\n",
      "Epoch 5, Sample 920000, Loss: 0.1071\n",
      "Epoch 5, Sample 930000, Loss: 0.2273\n",
      "Epoch 5, Sample 940000, Loss: 0.5086\n",
      "Epoch 5, Sample 950000, Loss: 0.1575\n",
      "Epoch 5, Sample 960000, Loss: 0.1107\n",
      "Epoch 5, Sample 970000, Loss: 0.0393\n",
      "Epoch 5, Sample 980000, Loss: 0.2625\n",
      "Epoch 5, Sample 990000, Loss: 0.6465\n",
      "Epoch 5, Sample 1000000, Loss: 0.1600\n",
      "Epoch 5, Sample 1010000, Loss: 0.0310\n",
      "Epoch 5, Sample 1020000, Loss: 0.2528\n",
      "Epoch 5, Sample 1030000, Loss: 0.1567\n",
      "Epoch 5, Sample 1040000, Loss: 0.1230\n",
      "Epoch 5, Sample 1050000, Loss: 0.1115\n",
      "Epoch 5, Sample 1060000, Loss: 0.2487\n",
      "Epoch 5, Sample 1070000, Loss: 0.1010\n",
      "Epoch 5, Sample 1080000, Loss: 0.0911\n",
      "Epoch 5, Sample 1090000, Loss: 0.2040\n",
      "Epoch 5, Sample 1100000, Loss: 0.2712\n",
      "Epoch 5, Sample 1110000, Loss: 0.1088\n",
      "Epoch 5, Sample 1120000, Loss: 0.1202\n",
      "Epoch 5, Sample 1130000, Loss: 0.0894\n",
      "Epoch 5, Sample 1140000, Loss: 0.0114\n",
      "Epoch 5, Sample 1150000, Loss: 0.0537\n",
      "Epoch 5, Sample 1160000, Loss: 0.3034\n",
      "Epoch 5, Sample 1170000, Loss: 0.1891\n",
      "Epoch 5, Sample 1180000, Loss: 0.2498\n",
      "Epoch 5, Sample 1190000, Loss: 0.1182\n",
      "Epoch 5, Sample 1200000, Loss: 0.6100\n",
      "Epoch 5, Sample 1210000, Loss: 0.1481\n",
      "Epoch 5, Sample 1220000, Loss: 0.2986\n",
      "Epoch 5, Sample 1230000, Loss: 0.1982\n",
      "Epoch 5, Sample 1240000, Loss: 0.2702\n",
      "Epoch 5, Sample 1250000, Loss: 0.1880\n",
      "Epoch 5, Sample 1260000, Loss: 0.2219\n",
      "Epoch 5, Sample 1270000, Loss: 0.2447\n",
      "Epoch 5, Sample 1280000, Loss: 0.0060\n",
      "Epoch 5, Sample 1290000, Loss: 0.0170\n",
      "Epoch 5, Sample 1300000, Loss: 0.1972\n",
      "Epoch 5, Sample 1310000, Loss: 0.2079\n",
      "Epoch 5, Sample 1320000, Loss: 0.3496\n",
      "Epoch 5, Sample 1330000, Loss: 0.0130\n",
      "Epoch 5, Sample 1340000, Loss: 0.2422\n",
      "Epoch 5, Sample 1350000, Loss: 0.1640\n",
      "Epoch 5, Sample 1360000, Loss: 0.0644\n",
      "Epoch 5, Sample 1370000, Loss: 0.1798\n",
      "Epoch 5, Sample 1380000, Loss: 0.1353\n",
      "Epoch 5, Sample 1390000, Loss: 0.0094\n",
      "Epoch 5, Sample 1400000, Loss: 0.2447\n",
      "Epoch 5, Sample 1410000, Loss: 0.1445\n",
      "Epoch 5, Sample 1420000, Loss: 0.4482\n",
      "Epoch 5, Sample 1430000, Loss: 0.0228\n",
      "Epoch 5, Sample 1440000, Loss: 0.0085\n",
      "Epoch 5, Sample 1450000, Loss: 0.2220\n",
      "Epoch 5, Sample 1460000, Loss: 0.2275\n",
      "Epoch 5, Sample 1470000, Loss: 0.1406\n",
      "Epoch 5, Sample 1480000, Loss: 0.3011\n",
      "Epoch 5, Sample 1490000, Loss: 0.1422\n",
      "Epoch 5, Sample 1500000, Loss: 0.1481\n",
      "Epoch 5, Sample 1510000, Loss: 0.1546\n",
      "Epoch 5, Sample 1520000, Loss: 0.1030\n",
      "Epoch 5, Sample 1530000, Loss: 0.0963\n",
      "Epoch 5, Sample 1540000, Loss: 0.1900\n",
      "Epoch 5, Sample 1550000, Loss: 0.2364\n",
      "Epoch 5, Sample 1560000, Loss: 0.3385\n",
      "Epoch 5, Sample 1570000, Loss: 0.1565\n",
      "Epoch 5, Sample 1580000, Loss: 0.0035\n",
      "Epoch 5, Sample 1590000, Loss: 0.0318\n",
      "Epoch 5, Sample 1600000, Loss: 0.2298\n",
      "Epoch 5, Sample 1610000, Loss: 0.0013\n",
      "Epoch 5, Sample 1620000, Loss: 0.2455\n",
      "Epoch 5, Sample 1630000, Loss: 0.0570\n",
      "Epoch 5, Sample 1640000, Loss: 0.2387\n",
      "Epoch 5, Sample 1650000, Loss: 0.1298\n",
      "Epoch 5, Sample 1660000, Loss: 0.0722\n",
      "Epoch 5, Sample 1670000, Loss: 0.0376\n",
      "Epoch 5, Sample 1680000, Loss: 0.1969\n",
      "Epoch 5, Sample 1690000, Loss: 0.0745\n",
      "Epoch 5, Sample 1700000, Loss: 0.0096\n",
      "Epoch 5, Sample 1710000, Loss: 0.0694\n",
      "Epoch 5, Sample 1720000, Loss: 0.1718\n",
      "Epoch 5, Sample 1730000, Loss: 0.0201\n",
      "Epoch 5, Sample 1740000, Loss: 0.0104\n",
      "Epoch 5, Sample 1750000, Loss: 0.3230\n",
      "Epoch 5, Sample 1760000, Loss: 0.3067\n",
      "Epoch 5, Sample 1770000, Loss: 0.0112\n",
      "Epoch 5, Sample 1780000, Loss: 0.2093\n",
      "Epoch 5, Sample 1790000, Loss: 0.1999\n",
      "Epoch 5, Sample 1800000, Loss: 0.2195\n",
      "Epoch 5, Sample 1810000, Loss: 0.0215\n",
      "Epoch 5, Sample 1820000, Loss: 0.0530\n",
      "Epoch 5, Sample 1830000, Loss: 0.1551\n",
      "Epoch 5, Sample 1840000, Loss: 0.1454\n",
      "Epoch 5, Sample 1850000, Loss: 0.2058\n",
      "Epoch 5, Sample 1860000, Loss: 0.2190\n",
      "Epoch 5, Sample 1870000, Loss: 0.2702\n",
      "Epoch 5, Sample 1880000, Loss: 0.2387\n",
      "Epoch 5, Sample 1890000, Loss: 0.2777\n",
      "Epoch 5, Sample 1900000, Loss: 0.1890\n",
      "Epoch 5, Sample 1910000, Loss: 0.1775\n",
      "Epoch 5, Sample 1920000, Loss: 0.3118\n",
      "Epoch 5, Sample 1930000, Loss: 0.2098\n",
      "Epoch 5, Sample 1940000, Loss: 0.1118\n",
      "Epoch 5, Sample 1950000, Loss: 0.1985\n",
      "Epoch 5, Sample 1960000, Loss: 0.1570\n",
      "Epoch 5, Sample 1970000, Loss: 0.1145\n",
      "Epoch 5, Sample 1980000, Loss: 0.3267\n",
      "Epoch 5, Sample 1990000, Loss: 0.2885\n",
      "Epoch 5, Sample 2000000, Loss: 0.1975\n",
      "Epoch 5, Sample 2010000, Loss: 0.4551\n",
      "Epoch 5, Sample 2020000, Loss: 0.2525\n",
      "Epoch 5, Sample 2030000, Loss: 0.2810\n",
      "Epoch 5, Sample 2040000, Loss: 0.2390\n",
      "Epoch 5, Sample 2050000, Loss: 0.2091\n",
      "Epoch 5, Sample 2060000, Loss: 0.1309\n",
      "Epoch 5, Sample 2070000, Loss: 0.0980\n",
      "Epoch 5, Sample 2080000, Loss: 0.0262\n",
      "Epoch 5, Sample 2090000, Loss: 0.1856\n",
      "Epoch 5, Sample 2100000, Loss: 0.2138\n",
      "Epoch 5, Sample 2110000, Loss: 0.0063\n",
      "Epoch 5, Sample 2120000, Loss: 0.1649\n",
      "Epoch 5, Sample 2130000, Loss: 0.2895\n",
      "Epoch 5, Sample 2140000, Loss: 0.2901\n",
      "Epoch 5, Sample 2150000, Loss: 0.1314\n",
      "Epoch 5, Sample 2160000, Loss: 0.2329\n",
      "Epoch 5, Sample 2170000, Loss: 0.1428\n",
      "Epoch 5, Sample 2180000, Loss: 0.1254\n",
      "Epoch 5, Sample 2190000, Loss: 0.3233\n",
      "Epoch 5, Sample 2200000, Loss: 0.2743\n",
      "Epoch 5, Sample 2210000, Loss: 0.1912\n",
      "Epoch 5, Sample 2220000, Loss: 0.0102\n",
      "Epoch 5, Sample 2230000, Loss: 0.2177\n",
      "Epoch 5, Sample 2240000, Loss: 0.1052\n",
      "Epoch 5, Sample 2250000, Loss: 0.1275\n",
      "Epoch 5, Sample 2260000, Loss: 0.1512\n",
      "Epoch 5, Sample 2270000, Loss: 0.0395\n",
      "Epoch 5, Sample 2280000, Loss: 0.0090\n",
      "Epoch 5, Sample 2290000, Loss: 0.0143\n",
      "Epoch 5, Sample 2300000, Loss: 0.1003\n",
      "Epoch 5, Sample 2310000, Loss: 0.1142\n",
      "Epoch 5, Sample 2320000, Loss: 0.1032\n",
      "Epoch 5, Sample 2330000, Loss: 0.0292\n",
      "Epoch 5, Sample 2340000, Loss: 0.2638\n",
      "Epoch 5, Sample 2350000, Loss: 0.2436\n",
      "Epoch 5, Sample 2360000, Loss: 0.3730\n",
      "Epoch 5, Sample 2370000, Loss: 0.1057\n",
      "Epoch 5, Sample 2380000, Loss: 0.2336\n",
      "Epoch 5, Sample 2390000, Loss: 0.1058\n",
      "Epoch 5, Sample 2400000, Loss: 0.2319\n",
      "Epoch 5, Sample 2410000, Loss: 0.0074\n",
      "Epoch 5, Sample 2420000, Loss: 0.2404\n",
      "Epoch 5, Sample 2430000, Loss: 0.3924\n",
      "Epoch 5, Sample 2440000, Loss: 0.1561\n",
      "Epoch 5, Sample 2450000, Loss: 0.1436\n",
      "Epoch 5, Sample 2460000, Loss: 0.1284\n",
      "Epoch 5, Sample 2470000, Loss: 0.0916\n",
      "Epoch 5, Sample 2480000, Loss: 0.2314\n",
      "Epoch 5, Sample 2490000, Loss: 0.0835\n",
      "Epoch 5, Sample 2500000, Loss: 0.3064\n",
      "Epoch 5, Sample 2510000, Loss: 0.0058\n",
      "Epoch 5, Sample 2520000, Loss: 0.2022\n",
      "Epoch 5, Sample 2530000, Loss: 0.6816\n",
      "Epoch 5, Sample 2540000, Loss: 0.4893\n",
      "Epoch 5, Sample 2550000, Loss: 0.2937\n",
      "Epoch 5, Sample 2560000, Loss: 0.0217\n",
      "Epoch 5, Sample 2570000, Loss: 0.0076\n",
      "Epoch 5, Sample 2580000, Loss: 0.1390\n",
      "Epoch 5, Sample 2590000, Loss: 0.1493\n",
      "Epoch 5, Sample 2600000, Loss: 0.1504\n",
      "Epoch 5, Sample 2610000, Loss: 0.1667\n",
      "Epoch 5, Sample 2620000, Loss: 0.2223\n",
      "Epoch 5, Sample 2630000, Loss: 0.2578\n",
      "Epoch 5, Sample 2640000, Loss: 0.1365\n",
      "Epoch 5, Sample 2650000, Loss: 0.1604\n",
      "Epoch 5, Sample 2660000, Loss: 0.1083\n",
      "Epoch 5, Sample 2670000, Loss: 0.3683\n",
      "Epoch 5, Sample 2680000, Loss: 0.1626\n",
      "Epoch 5, Sample 2690000, Loss: 0.1193\n",
      "Epoch 5, Sample 2700000, Loss: 0.1446\n",
      "Epoch 5, Sample 2710000, Loss: 0.0065\n",
      "Epoch 5, Sample 2720000, Loss: 0.3470\n",
      "Epoch 5, Sample 2730000, Loss: 0.2014\n",
      "Epoch 5, Sample 2740000, Loss: 0.1778\n",
      "Epoch 5, Sample 2750000, Loss: 0.4058\n",
      "Epoch 5, Sample 2760000, Loss: 0.1830\n",
      "Epoch 5, Sample 2770000, Loss: 0.0187\n",
      "Epoch 5, Sample 2780000, Loss: 0.0216\n",
      "Epoch 5, Sample 2790000, Loss: 0.1436\n",
      "Epoch 5, Sample 2800000, Loss: 0.1294\n",
      "Epoch 5, Sample 2810000, Loss: 0.1053\n",
      "Epoch 5, Sample 2820000, Loss: 0.1560\n",
      "Epoch 5, Sample 2830000, Loss: 0.0487\n",
      "Epoch 5, Sample 2840000, Loss: 0.2449\n",
      "Epoch 5, Sample 2850000, Loss: 0.2058\n",
      "Epoch 5, Sample 2860000, Loss: 0.0053\n",
      "Epoch 5, Sample 2870000, Loss: 0.1137\n",
      "Epoch 5, Sample 2880000, Loss: 0.1710\n",
      "Epoch 5, Sample 2890000, Loss: 0.0231\n",
      "Epoch 5, Sample 2900000, Loss: 0.0027\n",
      "Epoch 5, Sample 2910000, Loss: 0.0370\n",
      "Epoch 5, Sample 2920000, Loss: 0.3648\n",
      "Epoch 5, Sample 2930000, Loss: 0.2486\n",
      "Epoch 5, Sample 2940000, Loss: 0.1353\n",
      "Epoch 5, Sample 2950000, Loss: 0.0095\n",
      "Epoch 5, Sample 2960000, Loss: 0.1957\n",
      "Epoch 5, Sample 2970000, Loss: 0.1760\n",
      "Epoch 5, Sample 2980000, Loss: 0.2877\n",
      "Epoch 5, Sample 2990000, Loss: 0.1294\n",
      "Epoch 5, Sample 3000000, Loss: 0.3541\n",
      "Epoch 5, Sample 3010000, Loss: 0.1287\n",
      "Epoch 5, Sample 3020000, Loss: 0.2522\n",
      "Epoch 5, Sample 3030000, Loss: 0.0026\n",
      "Epoch 5, Sample 3040000, Loss: 0.1376\n",
      "Epoch 5, Sample 3050000, Loss: 0.1073\n",
      "Epoch 5, Sample 3060000, Loss: 0.0021\n",
      "Epoch 5, Sample 3070000, Loss: 0.1627\n",
      "Epoch 5/10, Average Loss: 0.1708\n",
      "Epoch 6, Sample 0, Loss: 0.0117\n",
      "Epoch 6, Sample 10000, Loss: 0.1092\n",
      "Epoch 6, Sample 20000, Loss: 0.2464\n",
      "Epoch 6, Sample 30000, Loss: 0.2119\n",
      "Epoch 6, Sample 40000, Loss: 0.1171\n",
      "Epoch 6, Sample 50000, Loss: 0.0200\n",
      "Epoch 6, Sample 60000, Loss: 0.1468\n",
      "Epoch 6, Sample 70000, Loss: 0.2905\n",
      "Epoch 6, Sample 80000, Loss: 0.1959\n",
      "Epoch 6, Sample 90000, Loss: 0.0127\n",
      "Epoch 6, Sample 100000, Loss: 0.1220\n",
      "Epoch 6, Sample 110000, Loss: 0.1558\n",
      "Epoch 6, Sample 120000, Loss: 0.0157\n",
      "Epoch 6, Sample 130000, Loss: 0.1788\n",
      "Epoch 6, Sample 140000, Loss: 0.2553\n",
      "Epoch 6, Sample 150000, Loss: 0.4478\n",
      "Epoch 6, Sample 160000, Loss: 0.1896\n",
      "Epoch 6, Sample 170000, Loss: 0.1128\n",
      "Epoch 6, Sample 180000, Loss: 0.2018\n",
      "Epoch 6, Sample 190000, Loss: 0.2138\n",
      "Epoch 6, Sample 200000, Loss: 0.1768\n",
      "Epoch 6, Sample 210000, Loss: 0.3799\n",
      "Epoch 6, Sample 220000, Loss: 0.3272\n",
      "Epoch 6, Sample 230000, Loss: 0.1555\n",
      "Epoch 6, Sample 240000, Loss: 0.2449\n",
      "Epoch 6, Sample 250000, Loss: 0.2487\n",
      "Epoch 6, Sample 260000, Loss: 0.1874\n",
      "Epoch 6, Sample 270000, Loss: 0.2322\n",
      "Epoch 6, Sample 280000, Loss: 0.3706\n",
      "Epoch 6, Sample 290000, Loss: 0.1200\n",
      "Epoch 6, Sample 300000, Loss: 0.3054\n",
      "Epoch 6, Sample 310000, Loss: 0.0026\n",
      "Epoch 6, Sample 320000, Loss: 0.2230\n",
      "Epoch 6, Sample 330000, Loss: 0.0102\n",
      "Epoch 6, Sample 340000, Loss: 0.1060\n",
      "Epoch 6, Sample 350000, Loss: 0.2129\n",
      "Epoch 6, Sample 360000, Loss: 0.7897\n",
      "Epoch 6, Sample 370000, Loss: 0.0881\n",
      "Epoch 6, Sample 380000, Loss: 0.1517\n",
      "Epoch 6, Sample 390000, Loss: 0.3323\n",
      "Epoch 6, Sample 400000, Loss: 0.1230\n",
      "Epoch 6, Sample 410000, Loss: 0.0634\n",
      "Epoch 6, Sample 420000, Loss: 0.5839\n",
      "Epoch 6, Sample 430000, Loss: 0.3278\n",
      "Epoch 6, Sample 440000, Loss: 0.2278\n",
      "Epoch 6, Sample 450000, Loss: 0.2269\n",
      "Epoch 6, Sample 460000, Loss: 0.2576\n",
      "Epoch 6, Sample 470000, Loss: 0.2078\n",
      "Epoch 6, Sample 480000, Loss: 0.1595\n",
      "Epoch 6, Sample 490000, Loss: 0.1463\n",
      "Epoch 6, Sample 500000, Loss: 0.0653\n",
      "Epoch 6, Sample 510000, Loss: 0.1702\n",
      "Epoch 6, Sample 520000, Loss: 0.0991\n",
      "Epoch 6, Sample 530000, Loss: 0.1949\n",
      "Epoch 6, Sample 540000, Loss: 0.3677\n",
      "Epoch 6, Sample 550000, Loss: 0.2452\n",
      "Epoch 6, Sample 560000, Loss: 0.3210\n",
      "Epoch 6, Sample 570000, Loss: 0.0754\n",
      "Epoch 6, Sample 580000, Loss: 0.0027\n",
      "Epoch 6, Sample 590000, Loss: 0.0345\n",
      "Epoch 6, Sample 600000, Loss: 0.1156\n",
      "Epoch 6, Sample 610000, Loss: 0.0799\n",
      "Epoch 6, Sample 620000, Loss: 0.2325\n",
      "Epoch 6, Sample 630000, Loss: 0.1478\n",
      "Epoch 6, Sample 640000, Loss: 0.3207\n",
      "Epoch 6, Sample 650000, Loss: 0.2568\n",
      "Epoch 6, Sample 660000, Loss: 0.3345\n",
      "Epoch 6, Sample 670000, Loss: 0.1301\n",
      "Epoch 6, Sample 680000, Loss: 0.0265\n",
      "Epoch 6, Sample 690000, Loss: 0.0598\n",
      "Epoch 6, Sample 700000, Loss: 0.3521\n",
      "Epoch 6, Sample 710000, Loss: 0.3122\n",
      "Epoch 6, Sample 720000, Loss: 0.2376\n",
      "Epoch 6, Sample 730000, Loss: 0.2780\n",
      "Epoch 6, Sample 740000, Loss: 0.2237\n",
      "Epoch 6, Sample 750000, Loss: 0.0075\n",
      "Epoch 6, Sample 760000, Loss: 0.2405\n",
      "Epoch 6, Sample 770000, Loss: 0.1899\n",
      "Epoch 6, Sample 780000, Loss: 0.4041\n",
      "Epoch 6, Sample 790000, Loss: 0.3323\n",
      "Epoch 6, Sample 800000, Loss: 0.2132\n",
      "Epoch 6, Sample 810000, Loss: 0.1866\n",
      "Epoch 6, Sample 820000, Loss: 0.1312\n",
      "Epoch 6, Sample 830000, Loss: 0.0684\n",
      "Epoch 6, Sample 840000, Loss: 0.4082\n",
      "Epoch 6, Sample 850000, Loss: 0.0776\n",
      "Epoch 6, Sample 860000, Loss: 0.0880\n",
      "Epoch 6, Sample 870000, Loss: 0.2485\n",
      "Epoch 6, Sample 880000, Loss: 0.0318\n",
      "Epoch 6, Sample 890000, Loss: 0.2410\n",
      "Epoch 6, Sample 900000, Loss: 0.1767\n",
      "Epoch 6, Sample 910000, Loss: 0.1817\n",
      "Epoch 6, Sample 920000, Loss: 0.0933\n",
      "Epoch 6, Sample 930000, Loss: 0.2244\n",
      "Epoch 6, Sample 940000, Loss: 0.5417\n",
      "Epoch 6, Sample 950000, Loss: 0.1448\n",
      "Epoch 6, Sample 960000, Loss: 0.0952\n",
      "Epoch 6, Sample 970000, Loss: 0.0321\n",
      "Epoch 6, Sample 980000, Loss: 0.2670\n",
      "Epoch 6, Sample 990000, Loss: 0.6642\n",
      "Epoch 6, Sample 1000000, Loss: 0.1438\n",
      "Epoch 6, Sample 1010000, Loss: 0.0274\n",
      "Epoch 6, Sample 1020000, Loss: 0.2575\n",
      "Epoch 6, Sample 1030000, Loss: 0.1434\n",
      "Epoch 6, Sample 1040000, Loss: 0.1087\n",
      "Epoch 6, Sample 1050000, Loss: 0.0943\n",
      "Epoch 6, Sample 1060000, Loss: 0.2471\n",
      "Epoch 6, Sample 1070000, Loss: 0.0893\n",
      "Epoch 6, Sample 1080000, Loss: 0.0792\n",
      "Epoch 6, Sample 1090000, Loss: 0.1973\n",
      "Epoch 6, Sample 1100000, Loss: 0.2804\n",
      "Epoch 6, Sample 1110000, Loss: 0.0946\n",
      "Epoch 6, Sample 1120000, Loss: 0.1130\n",
      "Epoch 6, Sample 1130000, Loss: 0.0785\n",
      "Epoch 6, Sample 1140000, Loss: 0.0096\n",
      "Epoch 6, Sample 1150000, Loss: 0.0496\n",
      "Epoch 6, Sample 1160000, Loss: 0.3112\n",
      "Epoch 6, Sample 1170000, Loss: 0.1765\n",
      "Epoch 6, Sample 1180000, Loss: 0.2543\n",
      "Epoch 6, Sample 1190000, Loss: 0.1111\n",
      "Epoch 6, Sample 1200000, Loss: 0.6198\n",
      "Epoch 6, Sample 1210000, Loss: 0.1330\n",
      "Epoch 6, Sample 1220000, Loss: 0.3061\n",
      "Epoch 6, Sample 1230000, Loss: 0.1943\n",
      "Epoch 6, Sample 1240000, Loss: 0.2631\n",
      "Epoch 6, Sample 1250000, Loss: 0.1775\n",
      "Epoch 6, Sample 1260000, Loss: 0.2115\n",
      "Epoch 6, Sample 1270000, Loss: 0.2435\n",
      "Epoch 6, Sample 1280000, Loss: 0.0056\n",
      "Epoch 6, Sample 1290000, Loss: 0.0144\n",
      "Epoch 6, Sample 1300000, Loss: 0.1943\n",
      "Epoch 6, Sample 1310000, Loss: 0.2070\n",
      "Epoch 6, Sample 1320000, Loss: 0.3583\n",
      "Epoch 6, Sample 1330000, Loss: 0.0119\n",
      "Epoch 6, Sample 1340000, Loss: 0.2413\n",
      "Epoch 6, Sample 1350000, Loss: 0.1441\n",
      "Epoch 6, Sample 1360000, Loss: 0.0522\n",
      "Epoch 6, Sample 1370000, Loss: 0.1709\n",
      "Epoch 6, Sample 1380000, Loss: 0.1181\n",
      "Epoch 6, Sample 1390000, Loss: 0.0083\n",
      "Epoch 6, Sample 1400000, Loss: 0.2381\n",
      "Epoch 6, Sample 1410000, Loss: 0.1291\n",
      "Epoch 6, Sample 1420000, Loss: 0.4766\n",
      "Epoch 6, Sample 1430000, Loss: 0.0197\n",
      "Epoch 6, Sample 1440000, Loss: 0.0066\n",
      "Epoch 6, Sample 1450000, Loss: 0.2150\n",
      "Epoch 6, Sample 1460000, Loss: 0.2151\n",
      "Epoch 6, Sample 1470000, Loss: 0.1295\n",
      "Epoch 6, Sample 1480000, Loss: 0.3119\n",
      "Epoch 6, Sample 1490000, Loss: 0.1270\n",
      "Epoch 6, Sample 1500000, Loss: 0.1308\n",
      "Epoch 6, Sample 1510000, Loss: 0.1445\n",
      "Epoch 6, Sample 1520000, Loss: 0.0911\n",
      "Epoch 6, Sample 1530000, Loss: 0.0807\n",
      "Epoch 6, Sample 1540000, Loss: 0.1740\n",
      "Epoch 6, Sample 1550000, Loss: 0.2339\n",
      "Epoch 6, Sample 1560000, Loss: 0.3596\n",
      "Epoch 6, Sample 1570000, Loss: 0.1472\n",
      "Epoch 6, Sample 1580000, Loss: 0.0030\n",
      "Epoch 6, Sample 1590000, Loss: 0.0257\n",
      "Epoch 6, Sample 1600000, Loss: 0.2251\n",
      "Epoch 6, Sample 1610000, Loss: 0.0010\n",
      "Epoch 6, Sample 1620000, Loss: 0.2369\n",
      "Epoch 6, Sample 1630000, Loss: 0.0520\n",
      "Epoch 6, Sample 1640000, Loss: 0.2403\n",
      "Epoch 6, Sample 1650000, Loss: 0.1143\n",
      "Epoch 6, Sample 1660000, Loss: 0.0662\n",
      "Epoch 6, Sample 1670000, Loss: 0.0310\n",
      "Epoch 6, Sample 1680000, Loss: 0.1856\n",
      "Epoch 6, Sample 1690000, Loss: 0.0625\n",
      "Epoch 6, Sample 1700000, Loss: 0.0081\n",
      "Epoch 6, Sample 1710000, Loss: 0.0604\n",
      "Epoch 6, Sample 1720000, Loss: 0.1634\n",
      "Epoch 6, Sample 1730000, Loss: 0.0160\n",
      "Epoch 6, Sample 1740000, Loss: 0.0092\n",
      "Epoch 6, Sample 1750000, Loss: 0.3334\n",
      "Epoch 6, Sample 1760000, Loss: 0.3149\n",
      "Epoch 6, Sample 1770000, Loss: 0.0099\n",
      "Epoch 6, Sample 1780000, Loss: 0.1987\n",
      "Epoch 6, Sample 1790000, Loss: 0.1890\n",
      "Epoch 6, Sample 1800000, Loss: 0.2182\n",
      "Epoch 6, Sample 1810000, Loss: 0.0176\n",
      "Epoch 6, Sample 1820000, Loss: 0.0432\n",
      "Epoch 6, Sample 1830000, Loss: 0.1450\n",
      "Epoch 6, Sample 1840000, Loss: 0.1354\n",
      "Epoch 6, Sample 1850000, Loss: 0.1919\n",
      "Epoch 6, Sample 1860000, Loss: 0.2138\n",
      "Epoch 6, Sample 1870000, Loss: 0.2702\n",
      "Epoch 6, Sample 1880000, Loss: 0.2398\n",
      "Epoch 6, Sample 1890000, Loss: 0.2784\n",
      "Epoch 6, Sample 1900000, Loss: 0.1808\n",
      "Epoch 6, Sample 1910000, Loss: 0.1634\n",
      "Epoch 6, Sample 1920000, Loss: 0.3179\n",
      "Epoch 6, Sample 1930000, Loss: 0.2099\n",
      "Epoch 6, Sample 1940000, Loss: 0.0976\n",
      "Epoch 6, Sample 1950000, Loss: 0.1968\n",
      "Epoch 6, Sample 1960000, Loss: 0.1450\n",
      "Epoch 6, Sample 1970000, Loss: 0.1020\n",
      "Epoch 6, Sample 1980000, Loss: 0.3413\n",
      "Epoch 6, Sample 1990000, Loss: 0.2933\n",
      "Epoch 6, Sample 2000000, Loss: 0.1837\n",
      "Epoch 6, Sample 2010000, Loss: 0.4752\n",
      "Epoch 6, Sample 2020000, Loss: 0.2437\n",
      "Epoch 6, Sample 2030000, Loss: 0.2842\n",
      "Epoch 6, Sample 2040000, Loss: 0.2398\n",
      "Epoch 6, Sample 2050000, Loss: 0.2023\n",
      "Epoch 6, Sample 2060000, Loss: 0.1171\n",
      "Epoch 6, Sample 2070000, Loss: 0.0831\n",
      "Epoch 6, Sample 2080000, Loss: 0.0228\n",
      "Epoch 6, Sample 2090000, Loss: 0.1800\n",
      "Epoch 6, Sample 2100000, Loss: 0.2099\n",
      "Epoch 6, Sample 2110000, Loss: 0.0052\n",
      "Epoch 6, Sample 2120000, Loss: 0.1479\n",
      "Epoch 6, Sample 2130000, Loss: 0.2884\n",
      "Epoch 6, Sample 2140000, Loss: 0.2914\n",
      "Epoch 6, Sample 2150000, Loss: 0.1203\n",
      "Epoch 6, Sample 2160000, Loss: 0.2301\n",
      "Epoch 6, Sample 2170000, Loss: 0.1258\n",
      "Epoch 6, Sample 2180000, Loss: 0.1154\n",
      "Epoch 6, Sample 2190000, Loss: 0.3319\n",
      "Epoch 6, Sample 2200000, Loss: 0.2747\n",
      "Epoch 6, Sample 2210000, Loss: 0.1772\n",
      "Epoch 6, Sample 2220000, Loss: 0.0079\n",
      "Epoch 6, Sample 2230000, Loss: 0.2050\n",
      "Epoch 6, Sample 2240000, Loss: 0.0924\n",
      "Epoch 6, Sample 2250000, Loss: 0.1124\n",
      "Epoch 6, Sample 2260000, Loss: 0.1408\n",
      "Epoch 6, Sample 2270000, Loss: 0.0327\n",
      "Epoch 6, Sample 2280000, Loss: 0.0086\n",
      "Epoch 6, Sample 2290000, Loss: 0.0125\n",
      "Epoch 6, Sample 2300000, Loss: 0.0890\n",
      "Epoch 6, Sample 2310000, Loss: 0.0991\n",
      "Epoch 6, Sample 2320000, Loss: 0.0901\n",
      "Epoch 6, Sample 2330000, Loss: 0.0229\n",
      "Epoch 6, Sample 2340000, Loss: 0.2666\n",
      "Epoch 6, Sample 2350000, Loss: 0.2404\n",
      "Epoch 6, Sample 2360000, Loss: 0.3824\n",
      "Epoch 6, Sample 2370000, Loss: 0.0926\n",
      "Epoch 6, Sample 2380000, Loss: 0.2317\n",
      "Epoch 6, Sample 2390000, Loss: 0.0917\n",
      "Epoch 6, Sample 2400000, Loss: 0.2250\n",
      "Epoch 6, Sample 2410000, Loss: 0.0061\n",
      "Epoch 6, Sample 2420000, Loss: 0.2430\n",
      "Epoch 6, Sample 2430000, Loss: 0.4115\n",
      "Epoch 6, Sample 2440000, Loss: 0.1433\n",
      "Epoch 6, Sample 2450000, Loss: 0.1251\n",
      "Epoch 6, Sample 2460000, Loss: 0.1093\n",
      "Epoch 6, Sample 2470000, Loss: 0.0766\n",
      "Epoch 6, Sample 2480000, Loss: 0.2264\n",
      "Epoch 6, Sample 2490000, Loss: 0.0745\n",
      "Epoch 6, Sample 2500000, Loss: 0.3184\n",
      "Epoch 6, Sample 2510000, Loss: 0.0060\n",
      "Epoch 6, Sample 2520000, Loss: 0.1880\n",
      "Epoch 6, Sample 2530000, Loss: 0.7035\n",
      "Epoch 6, Sample 2540000, Loss: 0.5053\n",
      "Epoch 6, Sample 2550000, Loss: 0.2983\n",
      "Epoch 6, Sample 2560000, Loss: 0.0177\n",
      "Epoch 6, Sample 2570000, Loss: 0.0066\n",
      "Epoch 6, Sample 2580000, Loss: 0.1315\n",
      "Epoch 6, Sample 2590000, Loss: 0.1361\n",
      "Epoch 6, Sample 2600000, Loss: 0.1399\n",
      "Epoch 6, Sample 2610000, Loss: 0.1551\n",
      "Epoch 6, Sample 2620000, Loss: 0.2185\n",
      "Epoch 6, Sample 2630000, Loss: 0.2566\n",
      "Epoch 6, Sample 2640000, Loss: 0.1253\n",
      "Epoch 6, Sample 2650000, Loss: 0.1464\n",
      "Epoch 6, Sample 2660000, Loss: 0.0977\n",
      "Epoch 6, Sample 2670000, Loss: 0.3899\n",
      "Epoch 6, Sample 2680000, Loss: 0.1476\n",
      "Epoch 6, Sample 2690000, Loss: 0.1058\n",
      "Epoch 6, Sample 2700000, Loss: 0.1295\n",
      "Epoch 6, Sample 2710000, Loss: 0.0059\n",
      "Epoch 6, Sample 2720000, Loss: 0.3582\n",
      "Epoch 6, Sample 2730000, Loss: 0.1946\n",
      "Epoch 6, Sample 2740000, Loss: 0.1728\n",
      "Epoch 6, Sample 2750000, Loss: 0.4289\n",
      "Epoch 6, Sample 2760000, Loss: 0.1755\n",
      "Epoch 6, Sample 2770000, Loss: 0.0141\n",
      "Epoch 6, Sample 2780000, Loss: 0.0199\n",
      "Epoch 6, Sample 2790000, Loss: 0.1351\n",
      "Epoch 6, Sample 2800000, Loss: 0.1150\n",
      "Epoch 6, Sample 2810000, Loss: 0.0923\n",
      "Epoch 6, Sample 2820000, Loss: 0.1445\n",
      "Epoch 6, Sample 2830000, Loss: 0.0404\n",
      "Epoch 6, Sample 2840000, Loss: 0.2456\n",
      "Epoch 6, Sample 2850000, Loss: 0.1911\n",
      "Epoch 6, Sample 2860000, Loss: 0.0045\n",
      "Epoch 6, Sample 2870000, Loss: 0.0986\n",
      "Epoch 6, Sample 2880000, Loss: 0.1597\n",
      "Epoch 6, Sample 2890000, Loss: 0.0199\n",
      "Epoch 6, Sample 2900000, Loss: 0.0026\n",
      "Epoch 6, Sample 2910000, Loss: 0.0304\n",
      "Epoch 6, Sample 2920000, Loss: 0.3795\n",
      "Epoch 6, Sample 2930000, Loss: 0.2408\n",
      "Epoch 6, Sample 2940000, Loss: 0.1221\n",
      "Epoch 6, Sample 2950000, Loss: 0.0073\n",
      "Epoch 6, Sample 2960000, Loss: 0.1871\n",
      "Epoch 6, Sample 2970000, Loss: 0.1616\n",
      "Epoch 6, Sample 2980000, Loss: 0.2950\n",
      "Epoch 6, Sample 2990000, Loss: 0.1139\n",
      "Epoch 6, Sample 3000000, Loss: 0.3650\n",
      "Epoch 6, Sample 3010000, Loss: 0.1132\n",
      "Epoch 6, Sample 3020000, Loss: 0.2436\n",
      "Epoch 6, Sample 3030000, Loss: 0.0021\n",
      "Epoch 6, Sample 3040000, Loss: 0.1229\n",
      "Epoch 6, Sample 3050000, Loss: 0.0932\n",
      "Epoch 6, Sample 3060000, Loss: 0.0018\n",
      "Epoch 6, Sample 3070000, Loss: 0.1522\n",
      "Epoch 6/10, Average Loss: 0.1653\n",
      "Epoch 7, Sample 0, Loss: 0.0105\n",
      "Epoch 7, Sample 10000, Loss: 0.0942\n",
      "Epoch 7, Sample 20000, Loss: 0.2458\n",
      "Epoch 7, Sample 30000, Loss: 0.2110\n",
      "Epoch 7, Sample 40000, Loss: 0.1026\n",
      "Epoch 7, Sample 50000, Loss: 0.0176\n",
      "Epoch 7, Sample 60000, Loss: 0.1350\n",
      "Epoch 7, Sample 70000, Loss: 0.3010\n",
      "Epoch 7, Sample 80000, Loss: 0.1825\n",
      "Epoch 7, Sample 90000, Loss: 0.0115\n",
      "Epoch 7, Sample 100000, Loss: 0.1076\n",
      "Epoch 7, Sample 110000, Loss: 0.1469\n",
      "Epoch 7, Sample 120000, Loss: 0.0131\n",
      "Epoch 7, Sample 130000, Loss: 0.1731\n",
      "Epoch 7, Sample 140000, Loss: 0.2569\n",
      "Epoch 7, Sample 150000, Loss: 0.4735\n",
      "Epoch 7, Sample 160000, Loss: 0.1854\n",
      "Epoch 7, Sample 170000, Loss: 0.0986\n",
      "Epoch 7, Sample 180000, Loss: 0.1925\n",
      "Epoch 7, Sample 190000, Loss: 0.2052\n",
      "Epoch 7, Sample 200000, Loss: 0.1698\n",
      "Epoch 7, Sample 210000, Loss: 0.3969\n",
      "Epoch 7, Sample 220000, Loss: 0.3307\n",
      "Epoch 7, Sample 230000, Loss: 0.1461\n",
      "Epoch 7, Sample 240000, Loss: 0.2412\n",
      "Epoch 7, Sample 250000, Loss: 0.2429\n",
      "Epoch 7, Sample 260000, Loss: 0.1757\n",
      "Epoch 7, Sample 270000, Loss: 0.2316\n",
      "Epoch 7, Sample 280000, Loss: 0.3869\n",
      "Epoch 7, Sample 290000, Loss: 0.1064\n",
      "Epoch 7, Sample 300000, Loss: 0.3225\n",
      "Epoch 7, Sample 310000, Loss: 0.0023\n",
      "Epoch 7, Sample 320000, Loss: 0.2151\n",
      "Epoch 7, Sample 330000, Loss: 0.0080\n",
      "Epoch 7, Sample 340000, Loss: 0.0902\n",
      "Epoch 7, Sample 350000, Loss: 0.2080\n",
      "Epoch 7, Sample 360000, Loss: 0.8072\n",
      "Epoch 7, Sample 370000, Loss: 0.0773\n",
      "Epoch 7, Sample 380000, Loss: 0.1353\n",
      "Epoch 7, Sample 390000, Loss: 0.3421\n",
      "Epoch 7, Sample 400000, Loss: 0.1069\n",
      "Epoch 7, Sample 410000, Loss: 0.0562\n",
      "Epoch 7, Sample 420000, Loss: 0.6043\n",
      "Epoch 7, Sample 430000, Loss: 0.3353\n",
      "Epoch 7, Sample 440000, Loss: 0.2231\n",
      "Epoch 7, Sample 450000, Loss: 0.2239\n",
      "Epoch 7, Sample 460000, Loss: 0.2537\n",
      "Epoch 7, Sample 470000, Loss: 0.2014\n",
      "Epoch 7, Sample 480000, Loss: 0.1491\n",
      "Epoch 7, Sample 490000, Loss: 0.1360\n",
      "Epoch 7, Sample 500000, Loss: 0.0602\n",
      "Epoch 7, Sample 510000, Loss: 0.1604\n",
      "Epoch 7, Sample 520000, Loss: 0.0918\n",
      "Epoch 7, Sample 530000, Loss: 0.1833\n",
      "Epoch 7, Sample 540000, Loss: 0.3759\n",
      "Epoch 7, Sample 550000, Loss: 0.2404\n",
      "Epoch 7, Sample 560000, Loss: 0.3284\n",
      "Epoch 7, Sample 570000, Loss: 0.0714\n",
      "Epoch 7, Sample 580000, Loss: 0.0022\n",
      "Epoch 7, Sample 590000, Loss: 0.0320\n",
      "Epoch 7, Sample 600000, Loss: 0.1010\n",
      "Epoch 7, Sample 610000, Loss: 0.0656\n",
      "Epoch 7, Sample 620000, Loss: 0.2273\n",
      "Epoch 7, Sample 630000, Loss: 0.1339\n",
      "Epoch 7, Sample 640000, Loss: 0.3239\n",
      "Epoch 7, Sample 650000, Loss: 0.2523\n",
      "Epoch 7, Sample 660000, Loss: 0.3449\n",
      "Epoch 7, Sample 670000, Loss: 0.1182\n",
      "Epoch 7, Sample 680000, Loss: 0.0228\n",
      "Epoch 7, Sample 690000, Loss: 0.0555\n",
      "Epoch 7, Sample 700000, Loss: 0.3588\n",
      "Epoch 7, Sample 710000, Loss: 0.3236\n",
      "Epoch 7, Sample 720000, Loss: 0.2342\n",
      "Epoch 7, Sample 730000, Loss: 0.2766\n",
      "Epoch 7, Sample 740000, Loss: 0.2219\n",
      "Epoch 7, Sample 750000, Loss: 0.0061\n",
      "Epoch 7, Sample 760000, Loss: 0.2350\n",
      "Epoch 7, Sample 770000, Loss: 0.1766\n",
      "Epoch 7, Sample 780000, Loss: 0.4262\n",
      "Epoch 7, Sample 790000, Loss: 0.3526\n",
      "Epoch 7, Sample 800000, Loss: 0.2075\n",
      "Epoch 7, Sample 810000, Loss: 0.1755\n",
      "Epoch 7, Sample 820000, Loss: 0.1164\n",
      "Epoch 7, Sample 830000, Loss: 0.0590\n",
      "Epoch 7, Sample 840000, Loss: 0.4267\n",
      "Epoch 7, Sample 850000, Loss: 0.0694\n",
      "Epoch 7, Sample 860000, Loss: 0.0759\n",
      "Epoch 7, Sample 870000, Loss: 0.2481\n",
      "Epoch 7, Sample 880000, Loss: 0.0262\n",
      "Epoch 7, Sample 890000, Loss: 0.2306\n",
      "Epoch 7, Sample 900000, Loss: 0.1680\n",
      "Epoch 7, Sample 910000, Loss: 0.1625\n",
      "Epoch 7, Sample 920000, Loss: 0.0823\n",
      "Epoch 7, Sample 930000, Loss: 0.2212\n",
      "Epoch 7, Sample 940000, Loss: 0.5698\n",
      "Epoch 7, Sample 950000, Loss: 0.1343\n",
      "Epoch 7, Sample 960000, Loss: 0.0830\n",
      "Epoch 7, Sample 970000, Loss: 0.0273\n",
      "Epoch 7, Sample 980000, Loss: 0.2704\n",
      "Epoch 7, Sample 990000, Loss: 0.6746\n",
      "Epoch 7, Sample 1000000, Loss: 0.1297\n",
      "Epoch 7, Sample 1010000, Loss: 0.0253\n",
      "Epoch 7, Sample 1020000, Loss: 0.2617\n",
      "Epoch 7, Sample 1030000, Loss: 0.1320\n",
      "Epoch 7, Sample 1040000, Loss: 0.0971\n",
      "Epoch 7, Sample 1050000, Loss: 0.0807\n",
      "Epoch 7, Sample 1060000, Loss: 0.2454\n",
      "Epoch 7, Sample 1070000, Loss: 0.0805\n",
      "Epoch 7, Sample 1080000, Loss: 0.0702\n",
      "Epoch 7, Sample 1090000, Loss: 0.1910\n",
      "Epoch 7, Sample 1100000, Loss: 0.2889\n",
      "Epoch 7, Sample 1110000, Loss: 0.0834\n",
      "Epoch 7, Sample 1120000, Loss: 0.1080\n",
      "Epoch 7, Sample 1130000, Loss: 0.0703\n",
      "Epoch 7, Sample 1140000, Loss: 0.0085\n",
      "Epoch 7, Sample 1150000, Loss: 0.0473\n",
      "Epoch 7, Sample 1160000, Loss: 0.3180\n",
      "Epoch 7, Sample 1170000, Loss: 0.1650\n",
      "Epoch 7, Sample 1180000, Loss: 0.2585\n",
      "Epoch 7, Sample 1190000, Loss: 0.1061\n",
      "Epoch 7, Sample 1200000, Loss: 0.6238\n",
      "Epoch 7, Sample 1210000, Loss: 0.1204\n",
      "Epoch 7, Sample 1220000, Loss: 0.3125\n",
      "Epoch 7, Sample 1230000, Loss: 0.1910\n",
      "Epoch 7, Sample 1240000, Loss: 0.2559\n",
      "Epoch 7, Sample 1250000, Loss: 0.1679\n",
      "Epoch 7, Sample 1260000, Loss: 0.2015\n",
      "Epoch 7, Sample 1270000, Loss: 0.2421\n",
      "Epoch 7, Sample 1280000, Loss: 0.0054\n",
      "Epoch 7, Sample 1290000, Loss: 0.0128\n",
      "Epoch 7, Sample 1300000, Loss: 0.1923\n",
      "Epoch 7, Sample 1310000, Loss: 0.2064\n",
      "Epoch 7, Sample 1320000, Loss: 0.3653\n",
      "Epoch 7, Sample 1330000, Loss: 0.0114\n",
      "Epoch 7, Sample 1340000, Loss: 0.2398\n",
      "Epoch 7, Sample 1350000, Loss: 0.1276\n",
      "Epoch 7, Sample 1360000, Loss: 0.0434\n",
      "Epoch 7, Sample 1370000, Loss: 0.1634\n",
      "Epoch 7, Sample 1380000, Loss: 0.1034\n",
      "Epoch 7, Sample 1390000, Loss: 0.0076\n",
      "Epoch 7, Sample 1400000, Loss: 0.2314\n",
      "Epoch 7, Sample 1410000, Loss: 0.1163\n",
      "Epoch 7, Sample 1420000, Loss: 0.5013\n",
      "Epoch 7, Sample 1430000, Loss: 0.0177\n",
      "Epoch 7, Sample 1440000, Loss: 0.0055\n",
      "Epoch 7, Sample 1450000, Loss: 0.2090\n",
      "Epoch 7, Sample 1460000, Loss: 0.2036\n",
      "Epoch 7, Sample 1470000, Loss: 0.1203\n",
      "Epoch 7, Sample 1480000, Loss: 0.3211\n",
      "Epoch 7, Sample 1490000, Loss: 0.1145\n",
      "Epoch 7, Sample 1500000, Loss: 0.1162\n",
      "Epoch 7, Sample 1510000, Loss: 0.1355\n",
      "Epoch 7, Sample 1520000, Loss: 0.0818\n",
      "Epoch 7, Sample 1530000, Loss: 0.0682\n",
      "Epoch 7, Sample 1540000, Loss: 0.1595\n",
      "Epoch 7, Sample 1550000, Loss: 0.2314\n",
      "Epoch 7, Sample 1560000, Loss: 0.3789\n",
      "Epoch 7, Sample 1570000, Loss: 0.1393\n",
      "Epoch 7, Sample 1580000, Loss: 0.0027\n",
      "Epoch 7, Sample 1590000, Loss: 0.0216\n",
      "Epoch 7, Sample 1600000, Loss: 0.2206\n",
      "Epoch 7, Sample 1610000, Loss: 0.0008\n",
      "Epoch 7, Sample 1620000, Loss: 0.2291\n",
      "Epoch 7, Sample 1630000, Loss: 0.0489\n",
      "Epoch 7, Sample 1640000, Loss: 0.2416\n",
      "Epoch 7, Sample 1650000, Loss: 0.1017\n",
      "Epoch 7, Sample 1660000, Loss: 0.0619\n",
      "Epoch 7, Sample 1670000, Loss: 0.0265\n",
      "Epoch 7, Sample 1680000, Loss: 0.1753\n",
      "Epoch 7, Sample 1690000, Loss: 0.0537\n",
      "Epoch 7, Sample 1700000, Loss: 0.0072\n",
      "Epoch 7, Sample 1710000, Loss: 0.0536\n",
      "Epoch 7, Sample 1720000, Loss: 0.1561\n",
      "Epoch 7, Sample 1730000, Loss: 0.0134\n",
      "Epoch 7, Sample 1740000, Loss: 0.0086\n",
      "Epoch 7, Sample 1750000, Loss: 0.3413\n",
      "Epoch 7, Sample 1760000, Loss: 0.3215\n",
      "Epoch 7, Sample 1770000, Loss: 0.0091\n",
      "Epoch 7, Sample 1780000, Loss: 0.1887\n",
      "Epoch 7, Sample 1790000, Loss: 0.1792\n",
      "Epoch 7, Sample 1800000, Loss: 0.2174\n",
      "Epoch 7, Sample 1810000, Loss: 0.0151\n",
      "Epoch 7, Sample 1820000, Loss: 0.0362\n",
      "Epoch 7, Sample 1830000, Loss: 0.1363\n",
      "Epoch 7, Sample 1840000, Loss: 0.1275\n",
      "Epoch 7, Sample 1850000, Loss: 0.1801\n",
      "Epoch 7, Sample 1860000, Loss: 0.2089\n",
      "Epoch 7, Sample 1870000, Loss: 0.2689\n",
      "Epoch 7, Sample 1880000, Loss: 0.2412\n",
      "Epoch 7, Sample 1890000, Loss: 0.2783\n",
      "Epoch 7, Sample 1900000, Loss: 0.1731\n",
      "Epoch 7, Sample 1910000, Loss: 0.1510\n",
      "Epoch 7, Sample 1920000, Loss: 0.3223\n",
      "Epoch 7, Sample 1930000, Loss: 0.2102\n",
      "Epoch 7, Sample 1940000, Loss: 0.0863\n",
      "Epoch 7, Sample 1950000, Loss: 0.1957\n",
      "Epoch 7, Sample 1960000, Loss: 0.1349\n",
      "Epoch 7, Sample 1970000, Loss: 0.0919\n",
      "Epoch 7, Sample 1980000, Loss: 0.3548\n",
      "Epoch 7, Sample 1990000, Loss: 0.2963\n",
      "Epoch 7, Sample 2000000, Loss: 0.1715\n",
      "Epoch 7, Sample 2010000, Loss: 0.4912\n",
      "Epoch 7, Sample 2020000, Loss: 0.2360\n",
      "Epoch 7, Sample 2030000, Loss: 0.2876\n",
      "Epoch 7, Sample 2040000, Loss: 0.2412\n",
      "Epoch 7, Sample 2050000, Loss: 0.1970\n",
      "Epoch 7, Sample 2060000, Loss: 0.1058\n",
      "Epoch 7, Sample 2070000, Loss: 0.0714\n",
      "Epoch 7, Sample 2080000, Loss: 0.0205\n",
      "Epoch 7, Sample 2090000, Loss: 0.1757\n",
      "Epoch 7, Sample 2100000, Loss: 0.2063\n",
      "Epoch 7, Sample 2110000, Loss: 0.0046\n",
      "Epoch 7, Sample 2120000, Loss: 0.1334\n",
      "Epoch 7, Sample 2130000, Loss: 0.2874\n",
      "Epoch 7, Sample 2140000, Loss: 0.2908\n",
      "Epoch 7, Sample 2150000, Loss: 0.1109\n",
      "Epoch 7, Sample 2160000, Loss: 0.2281\n",
      "Epoch 7, Sample 2170000, Loss: 0.1119\n",
      "Epoch 7, Sample 2180000, Loss: 0.1074\n",
      "Epoch 7, Sample 2190000, Loss: 0.3391\n",
      "Epoch 7, Sample 2200000, Loss: 0.2732\n",
      "Epoch 7, Sample 2210000, Loss: 0.1645\n",
      "Epoch 7, Sample 2220000, Loss: 0.0064\n",
      "Epoch 7, Sample 2230000, Loss: 0.1934\n",
      "Epoch 7, Sample 2240000, Loss: 0.0824\n",
      "Epoch 7, Sample 2250000, Loss: 0.1002\n",
      "Epoch 7, Sample 2260000, Loss: 0.1321\n",
      "Epoch 7, Sample 2270000, Loss: 0.0279\n",
      "Epoch 7, Sample 2280000, Loss: 0.0084\n",
      "Epoch 7, Sample 2290000, Loss: 0.0114\n",
      "Epoch 7, Sample 2300000, Loss: 0.0802\n",
      "Epoch 7, Sample 2310000, Loss: 0.0871\n",
      "Epoch 7, Sample 2320000, Loss: 0.0796\n",
      "Epoch 7, Sample 2330000, Loss: 0.0187\n",
      "Epoch 7, Sample 2340000, Loss: 0.2696\n",
      "Epoch 7, Sample 2350000, Loss: 0.2375\n",
      "Epoch 7, Sample 2360000, Loss: 0.3906\n",
      "Epoch 7, Sample 2370000, Loss: 0.0825\n",
      "Epoch 7, Sample 2380000, Loss: 0.2308\n",
      "Epoch 7, Sample 2390000, Loss: 0.0804\n",
      "Epoch 7, Sample 2400000, Loss: 0.2184\n",
      "Epoch 7, Sample 2410000, Loss: 0.0053\n",
      "Epoch 7, Sample 2420000, Loss: 0.2453\n",
      "Epoch 7, Sample 2430000, Loss: 0.4279\n",
      "Epoch 7, Sample 2440000, Loss: 0.1323\n",
      "Epoch 7, Sample 2450000, Loss: 0.1096\n",
      "Epoch 7, Sample 2460000, Loss: 0.0935\n",
      "Epoch 7, Sample 2470000, Loss: 0.0648\n",
      "Epoch 7, Sample 2480000, Loss: 0.2215\n",
      "Epoch 7, Sample 2490000, Loss: 0.0676\n",
      "Epoch 7, Sample 2500000, Loss: 0.3290\n",
      "Epoch 7, Sample 2510000, Loss: 0.0064\n",
      "Epoch 7, Sample 2520000, Loss: 0.1751\n",
      "Epoch 7, Sample 2530000, Loss: 0.7197\n",
      "Epoch 7, Sample 2540000, Loss: 0.5172\n",
      "Epoch 7, Sample 2550000, Loss: 0.3017\n",
      "Epoch 7, Sample 2560000, Loss: 0.0151\n",
      "Epoch 7, Sample 2570000, Loss: 0.0061\n",
      "Epoch 7, Sample 2580000, Loss: 0.1252\n",
      "Epoch 7, Sample 2590000, Loss: 0.1249\n",
      "Epoch 7, Sample 2600000, Loss: 0.1312\n",
      "Epoch 7, Sample 2610000, Loss: 0.1448\n",
      "Epoch 7, Sample 2620000, Loss: 0.2155\n",
      "Epoch 7, Sample 2630000, Loss: 0.2556\n",
      "Epoch 7, Sample 2640000, Loss: 0.1160\n",
      "Epoch 7, Sample 2650000, Loss: 0.1344\n",
      "Epoch 7, Sample 2660000, Loss: 0.0896\n",
      "Epoch 7, Sample 2670000, Loss: 0.4091\n",
      "Epoch 7, Sample 2680000, Loss: 0.1349\n",
      "Epoch 7, Sample 2690000, Loss: 0.0946\n",
      "Epoch 7, Sample 2700000, Loss: 0.1167\n",
      "Epoch 7, Sample 2710000, Loss: 0.0056\n",
      "Epoch 7, Sample 2720000, Loss: 0.3678\n",
      "Epoch 7, Sample 2730000, Loss: 0.1887\n",
      "Epoch 7, Sample 2740000, Loss: 0.1685\n",
      "Epoch 7, Sample 2750000, Loss: 0.4495\n",
      "Epoch 7, Sample 2760000, Loss: 0.1689\n",
      "Epoch 7, Sample 2770000, Loss: 0.0111\n",
      "Epoch 7, Sample 2780000, Loss: 0.0189\n",
      "Epoch 7, Sample 2790000, Loss: 0.1281\n",
      "Epoch 7, Sample 2800000, Loss: 0.1030\n",
      "Epoch 7, Sample 2810000, Loss: 0.0817\n",
      "Epoch 7, Sample 2820000, Loss: 0.1342\n",
      "Epoch 7, Sample 2830000, Loss: 0.0343\n",
      "Epoch 7, Sample 2840000, Loss: 0.2460\n",
      "Epoch 7, Sample 2850000, Loss: 0.1776\n",
      "Epoch 7, Sample 2860000, Loss: 0.0041\n",
      "Epoch 7, Sample 2870000, Loss: 0.0862\n",
      "Epoch 7, Sample 2880000, Loss: 0.1499\n",
      "Epoch 7, Sample 2890000, Loss: 0.0178\n",
      "Epoch 7, Sample 2900000, Loss: 0.0026\n",
      "Epoch 7, Sample 2910000, Loss: 0.0259\n",
      "Epoch 7, Sample 2920000, Loss: 0.3926\n",
      "Epoch 7, Sample 2930000, Loss: 0.2334\n",
      "Epoch 7, Sample 2940000, Loss: 0.1110\n",
      "Epoch 7, Sample 2950000, Loss: 0.0058\n",
      "Epoch 7, Sample 2960000, Loss: 0.1795\n",
      "Epoch 7, Sample 2970000, Loss: 0.1495\n",
      "Epoch 7, Sample 2980000, Loss: 0.3016\n",
      "Epoch 7, Sample 2990000, Loss: 0.1014\n",
      "Epoch 7, Sample 3000000, Loss: 0.3739\n",
      "Epoch 7, Sample 3010000, Loss: 0.1003\n",
      "Epoch 7, Sample 3020000, Loss: 0.2355\n",
      "Epoch 7, Sample 3030000, Loss: 0.0019\n",
      "Epoch 7, Sample 3040000, Loss: 0.1108\n",
      "Epoch 7, Sample 3050000, Loss: 0.0817\n",
      "Epoch 7, Sample 3060000, Loss: 0.0015\n",
      "Epoch 7, Sample 3070000, Loss: 0.1434\n",
      "Epoch 7/10, Average Loss: 0.1609\n",
      "Epoch 8, Sample 0, Loss: 0.0097\n",
      "Epoch 8, Sample 10000, Loss: 0.0818\n",
      "Epoch 8, Sample 20000, Loss: 0.2453\n",
      "Epoch 8, Sample 30000, Loss: 0.2098\n",
      "Epoch 8, Sample 40000, Loss: 0.0907\n",
      "Epoch 8, Sample 50000, Loss: 0.0159\n",
      "Epoch 8, Sample 60000, Loss: 0.1253\n",
      "Epoch 8, Sample 70000, Loss: 0.3112\n",
      "Epoch 8, Sample 80000, Loss: 0.1708\n",
      "Epoch 8, Sample 90000, Loss: 0.0108\n",
      "Epoch 8, Sample 100000, Loss: 0.0958\n",
      "Epoch 8, Sample 110000, Loss: 0.1386\n",
      "Epoch 8, Sample 120000, Loss: 0.0114\n",
      "Epoch 8, Sample 130000, Loss: 0.1689\n",
      "Epoch 8, Sample 140000, Loss: 0.2581\n",
      "Epoch 8, Sample 150000, Loss: 0.4954\n",
      "Epoch 8, Sample 160000, Loss: 0.1820\n",
      "Epoch 8, Sample 170000, Loss: 0.0871\n",
      "Epoch 8, Sample 180000, Loss: 0.1839\n",
      "Epoch 8, Sample 190000, Loss: 0.1976\n",
      "Epoch 8, Sample 200000, Loss: 0.1638\n",
      "Epoch 8, Sample 210000, Loss: 0.4122\n",
      "Epoch 8, Sample 220000, Loss: 0.3331\n",
      "Epoch 8, Sample 230000, Loss: 0.1380\n",
      "Epoch 8, Sample 240000, Loss: 0.2377\n",
      "Epoch 8, Sample 250000, Loss: 0.2372\n",
      "Epoch 8, Sample 260000, Loss: 0.1654\n",
      "Epoch 8, Sample 270000, Loss: 0.2306\n",
      "Epoch 8, Sample 280000, Loss: 0.4015\n",
      "Epoch 8, Sample 290000, Loss: 0.0951\n",
      "Epoch 8, Sample 300000, Loss: 0.3386\n",
      "Epoch 8, Sample 310000, Loss: 0.0021\n",
      "Epoch 8, Sample 320000, Loss: 0.2072\n",
      "Epoch 8, Sample 330000, Loss: 0.0066\n",
      "Epoch 8, Sample 340000, Loss: 0.0773\n",
      "Epoch 8, Sample 350000, Loss: 0.2036\n",
      "Epoch 8, Sample 360000, Loss: 0.8201\n",
      "Epoch 8, Sample 370000, Loss: 0.0689\n",
      "Epoch 8, Sample 380000, Loss: 0.1213\n",
      "Epoch 8, Sample 390000, Loss: 0.3505\n",
      "Epoch 8, Sample 400000, Loss: 0.0936\n",
      "Epoch 8, Sample 410000, Loss: 0.0507\n",
      "Epoch 8, Sample 420000, Loss: 0.6199\n",
      "Epoch 8, Sample 430000, Loss: 0.3416\n",
      "Epoch 8, Sample 440000, Loss: 0.2184\n",
      "Epoch 8, Sample 450000, Loss: 0.2215\n",
      "Epoch 8, Sample 460000, Loss: 0.2498\n",
      "Epoch 8, Sample 470000, Loss: 0.1962\n",
      "Epoch 8, Sample 480000, Loss: 0.1405\n",
      "Epoch 8, Sample 490000, Loss: 0.1274\n",
      "Epoch 8, Sample 500000, Loss: 0.0567\n",
      "Epoch 8, Sample 510000, Loss: 0.1519\n",
      "Epoch 8, Sample 520000, Loss: 0.0863\n",
      "Epoch 8, Sample 530000, Loss: 0.1726\n",
      "Epoch 8, Sample 540000, Loss: 0.3822\n",
      "Epoch 8, Sample 550000, Loss: 0.2360\n",
      "Epoch 8, Sample 560000, Loss: 0.3343\n",
      "Epoch 8, Sample 570000, Loss: 0.0689\n",
      "Epoch 8, Sample 580000, Loss: 0.0020\n",
      "Epoch 8, Sample 590000, Loss: 0.0304\n",
      "Epoch 8, Sample 600000, Loss: 0.0890\n",
      "Epoch 8, Sample 610000, Loss: 0.0546\n",
      "Epoch 8, Sample 620000, Loss: 0.2224\n",
      "Epoch 8, Sample 630000, Loss: 0.1221\n",
      "Epoch 8, Sample 640000, Loss: 0.3263\n",
      "Epoch 8, Sample 650000, Loss: 0.2479\n",
      "Epoch 8, Sample 660000, Loss: 0.3537\n",
      "Epoch 8, Sample 670000, Loss: 0.1079\n",
      "Epoch 8, Sample 680000, Loss: 0.0202\n",
      "Epoch 8, Sample 690000, Loss: 0.0527\n",
      "Epoch 8, Sample 700000, Loss: 0.3648\n",
      "Epoch 8, Sample 710000, Loss: 0.3348\n",
      "Epoch 8, Sample 720000, Loss: 0.2309\n",
      "Epoch 8, Sample 730000, Loss: 0.2755\n",
      "Epoch 8, Sample 740000, Loss: 0.2206\n",
      "Epoch 8, Sample 750000, Loss: 0.0052\n",
      "Epoch 8, Sample 760000, Loss: 0.2295\n",
      "Epoch 8, Sample 770000, Loss: 0.1647\n",
      "Epoch 8, Sample 780000, Loss: 0.4461\n",
      "Epoch 8, Sample 790000, Loss: 0.3722\n",
      "Epoch 8, Sample 800000, Loss: 0.2014\n",
      "Epoch 8, Sample 810000, Loss: 0.1649\n",
      "Epoch 8, Sample 820000, Loss: 0.1040\n",
      "Epoch 8, Sample 830000, Loss: 0.0521\n",
      "Epoch 8, Sample 840000, Loss: 0.4428\n",
      "Epoch 8, Sample 850000, Loss: 0.0633\n",
      "Epoch 8, Sample 860000, Loss: 0.0663\n",
      "Epoch 8, Sample 870000, Loss: 0.2477\n",
      "Epoch 8, Sample 880000, Loss: 0.0223\n",
      "Epoch 8, Sample 890000, Loss: 0.2208\n",
      "Epoch 8, Sample 900000, Loss: 0.1603\n",
      "Epoch 8, Sample 910000, Loss: 0.1458\n",
      "Epoch 8, Sample 920000, Loss: 0.0734\n",
      "Epoch 8, Sample 930000, Loss: 0.2178\n",
      "Epoch 8, Sample 940000, Loss: 0.5939\n",
      "Epoch 8, Sample 950000, Loss: 0.1256\n",
      "Epoch 8, Sample 960000, Loss: 0.0731\n",
      "Epoch 8, Sample 970000, Loss: 0.0239\n",
      "Epoch 8, Sample 980000, Loss: 0.2728\n",
      "Epoch 8, Sample 990000, Loss: 0.6804\n",
      "Epoch 8, Sample 1000000, Loss: 0.1174\n",
      "Epoch 8, Sample 1010000, Loss: 0.0239\n",
      "Epoch 8, Sample 1020000, Loss: 0.2655\n",
      "Epoch 8, Sample 1030000, Loss: 0.1221\n",
      "Epoch 8, Sample 1040000, Loss: 0.0875\n",
      "Epoch 8, Sample 1050000, Loss: 0.0699\n",
      "Epoch 8, Sample 1060000, Loss: 0.2436\n",
      "Epoch 8, Sample 1070000, Loss: 0.0738\n",
      "Epoch 8, Sample 1080000, Loss: 0.0632\n",
      "Epoch 8, Sample 1090000, Loss: 0.1850\n",
      "Epoch 8, Sample 1100000, Loss: 0.2969\n",
      "Epoch 8, Sample 1110000, Loss: 0.0746\n",
      "Epoch 8, Sample 1120000, Loss: 0.1045\n",
      "Epoch 8, Sample 1130000, Loss: 0.0639\n",
      "Epoch 8, Sample 1140000, Loss: 0.0077\n",
      "Epoch 8, Sample 1150000, Loss: 0.0461\n",
      "Epoch 8, Sample 1160000, Loss: 0.3242\n",
      "Epoch 8, Sample 1170000, Loss: 0.1548\n",
      "Epoch 8, Sample 1180000, Loss: 0.2624\n",
      "Epoch 8, Sample 1190000, Loss: 0.1023\n",
      "Epoch 8, Sample 1200000, Loss: 0.6243\n",
      "Epoch 8, Sample 1210000, Loss: 0.1100\n",
      "Epoch 8, Sample 1220000, Loss: 0.3180\n",
      "Epoch 8, Sample 1230000, Loss: 0.1880\n",
      "Epoch 8, Sample 1240000, Loss: 0.2488\n",
      "Epoch 8, Sample 1250000, Loss: 0.1593\n",
      "Epoch 8, Sample 1260000, Loss: 0.1922\n",
      "Epoch 8, Sample 1270000, Loss: 0.2406\n",
      "Epoch 8, Sample 1280000, Loss: 0.0054\n",
      "Epoch 8, Sample 1290000, Loss: 0.0118\n",
      "Epoch 8, Sample 1300000, Loss: 0.1911\n",
      "Epoch 8, Sample 1310000, Loss: 0.2059\n",
      "Epoch 8, Sample 1320000, Loss: 0.3710\n",
      "Epoch 8, Sample 1330000, Loss: 0.0112\n",
      "Epoch 8, Sample 1340000, Loss: 0.2378\n",
      "Epoch 8, Sample 1350000, Loss: 0.1138\n",
      "Epoch 8, Sample 1360000, Loss: 0.0369\n",
      "Epoch 8, Sample 1370000, Loss: 0.1572\n",
      "Epoch 8, Sample 1380000, Loss: 0.0910\n",
      "Epoch 8, Sample 1390000, Loss: 0.0072\n",
      "Epoch 8, Sample 1400000, Loss: 0.2249\n",
      "Epoch 8, Sample 1410000, Loss: 0.1056\n",
      "Epoch 8, Sample 1420000, Loss: 0.5229\n",
      "Epoch 8, Sample 1430000, Loss: 0.0164\n",
      "Epoch 8, Sample 1440000, Loss: 0.0048\n",
      "Epoch 8, Sample 1450000, Loss: 0.2038\n",
      "Epoch 8, Sample 1460000, Loss: 0.1932\n",
      "Epoch 8, Sample 1470000, Loss: 0.1125\n",
      "Epoch 8, Sample 1480000, Loss: 0.3289\n",
      "Epoch 8, Sample 1490000, Loss: 0.1040\n",
      "Epoch 8, Sample 1500000, Loss: 0.1037\n",
      "Epoch 8, Sample 1510000, Loss: 0.1275\n",
      "Epoch 8, Sample 1520000, Loss: 0.0743\n",
      "Epoch 8, Sample 1530000, Loss: 0.0583\n",
      "Epoch 8, Sample 1540000, Loss: 0.1465\n",
      "Epoch 8, Sample 1550000, Loss: 0.2290\n",
      "Epoch 8, Sample 1560000, Loss: 0.3964\n",
      "Epoch 8, Sample 1570000, Loss: 0.1327\n",
      "Epoch 8, Sample 1580000, Loss: 0.0025\n",
      "Epoch 8, Sample 1590000, Loss: 0.0187\n",
      "Epoch 8, Sample 1600000, Loss: 0.2165\n",
      "Epoch 8, Sample 1610000, Loss: 0.0007\n",
      "Epoch 8, Sample 1620000, Loss: 0.2219\n",
      "Epoch 8, Sample 1630000, Loss: 0.0469\n",
      "Epoch 8, Sample 1640000, Loss: 0.2427\n",
      "Epoch 8, Sample 1650000, Loss: 0.0913\n",
      "Epoch 8, Sample 1660000, Loss: 0.0588\n",
      "Epoch 8, Sample 1670000, Loss: 0.0234\n",
      "Epoch 8, Sample 1680000, Loss: 0.1661\n",
      "Epoch 8, Sample 1690000, Loss: 0.0471\n",
      "Epoch 8, Sample 1700000, Loss: 0.0066\n",
      "Epoch 8, Sample 1710000, Loss: 0.0485\n",
      "Epoch 8, Sample 1720000, Loss: 0.1498\n",
      "Epoch 8, Sample 1730000, Loss: 0.0116\n",
      "Epoch 8, Sample 1740000, Loss: 0.0082\n",
      "Epoch 8, Sample 1750000, Loss: 0.3471\n",
      "Epoch 8, Sample 1760000, Loss: 0.3268\n",
      "Epoch 8, Sample 1770000, Loss: 0.0087\n",
      "Epoch 8, Sample 1780000, Loss: 0.1797\n",
      "Epoch 8, Sample 1790000, Loss: 0.1704\n",
      "Epoch 8, Sample 1800000, Loss: 0.2169\n",
      "Epoch 8, Sample 1810000, Loss: 0.0133\n",
      "Epoch 8, Sample 1820000, Loss: 0.0310\n",
      "Epoch 8, Sample 1830000, Loss: 0.1287\n",
      "Epoch 8, Sample 1840000, Loss: 0.1211\n",
      "Epoch 8, Sample 1850000, Loss: 0.1702\n",
      "Epoch 8, Sample 1860000, Loss: 0.2043\n",
      "Epoch 8, Sample 1870000, Loss: 0.2667\n",
      "Epoch 8, Sample 1880000, Loss: 0.2428\n",
      "Epoch 8, Sample 1890000, Loss: 0.2778\n",
      "Epoch 8, Sample 1900000, Loss: 0.1657\n",
      "Epoch 8, Sample 1910000, Loss: 0.1402\n",
      "Epoch 8, Sample 1920000, Loss: 0.3252\n",
      "Epoch 8, Sample 1930000, Loss: 0.2108\n",
      "Epoch 8, Sample 1940000, Loss: 0.0773\n",
      "Epoch 8, Sample 1950000, Loss: 0.1948\n",
      "Epoch 8, Sample 1960000, Loss: 0.1261\n",
      "Epoch 8, Sample 1970000, Loss: 0.0835\n",
      "Epoch 8, Sample 1980000, Loss: 0.3672\n",
      "Epoch 8, Sample 1990000, Loss: 0.2980\n",
      "Epoch 8, Sample 2000000, Loss: 0.1608\n",
      "Epoch 8, Sample 2010000, Loss: 0.5042\n",
      "Epoch 8, Sample 2020000, Loss: 0.2294\n",
      "Epoch 8, Sample 2030000, Loss: 0.2910\n",
      "Epoch 8, Sample 2040000, Loss: 0.2431\n",
      "Epoch 8, Sample 2050000, Loss: 0.1929\n",
      "Epoch 8, Sample 2060000, Loss: 0.0965\n",
      "Epoch 8, Sample 2070000, Loss: 0.0622\n",
      "Epoch 8, Sample 2080000, Loss: 0.0190\n",
      "Epoch 8, Sample 2090000, Loss: 0.1724\n",
      "Epoch 8, Sample 2100000, Loss: 0.2030\n",
      "Epoch 8, Sample 2110000, Loss: 0.0042\n",
      "Epoch 8, Sample 2120000, Loss: 0.1211\n",
      "Epoch 8, Sample 2130000, Loss: 0.2864\n",
      "Epoch 8, Sample 2140000, Loss: 0.2888\n",
      "Epoch 8, Sample 2150000, Loss: 0.1029\n",
      "Epoch 8, Sample 2160000, Loss: 0.2266\n",
      "Epoch 8, Sample 2170000, Loss: 0.1004\n",
      "Epoch 8, Sample 2180000, Loss: 0.1009\n",
      "Epoch 8, Sample 2190000, Loss: 0.3451\n",
      "Epoch 8, Sample 2200000, Loss: 0.2701\n",
      "Epoch 8, Sample 2210000, Loss: 0.1529\n",
      "Epoch 8, Sample 2220000, Loss: 0.0054\n",
      "Epoch 8, Sample 2230000, Loss: 0.1830\n",
      "Epoch 8, Sample 2240000, Loss: 0.0744\n",
      "Epoch 8, Sample 2250000, Loss: 0.0901\n",
      "Epoch 8, Sample 2260000, Loss: 0.1247\n",
      "Epoch 8, Sample 2270000, Loss: 0.0245\n",
      "Epoch 8, Sample 2280000, Loss: 0.0084\n",
      "Epoch 8, Sample 2290000, Loss: 0.0107\n",
      "Epoch 8, Sample 2300000, Loss: 0.0733\n",
      "Epoch 8, Sample 2310000, Loss: 0.0774\n",
      "Epoch 8, Sample 2320000, Loss: 0.0709\n",
      "Epoch 8, Sample 2330000, Loss: 0.0157\n",
      "Epoch 8, Sample 2340000, Loss: 0.2725\n",
      "Epoch 8, Sample 2350000, Loss: 0.2347\n",
      "Epoch 8, Sample 2360000, Loss: 0.3976\n",
      "Epoch 8, Sample 2370000, Loss: 0.0745\n",
      "Epoch 8, Sample 2380000, Loss: 0.2308\n",
      "Epoch 8, Sample 2390000, Loss: 0.0714\n",
      "Epoch 8, Sample 2400000, Loss: 0.2121\n",
      "Epoch 8, Sample 2410000, Loss: 0.0048\n",
      "Epoch 8, Sample 2420000, Loss: 0.2472\n",
      "Epoch 8, Sample 2430000, Loss: 0.4421\n",
      "Epoch 8, Sample 2440000, Loss: 0.1227\n",
      "Epoch 8, Sample 2450000, Loss: 0.0966\n",
      "Epoch 8, Sample 2460000, Loss: 0.0805\n",
      "Epoch 8, Sample 2470000, Loss: 0.0554\n",
      "Epoch 8, Sample 2480000, Loss: 0.2167\n",
      "Epoch 8, Sample 2490000, Loss: 0.0623\n",
      "Epoch 8, Sample 2500000, Loss: 0.3384\n",
      "Epoch 8, Sample 2510000, Loss: 0.0067\n",
      "Epoch 8, Sample 2520000, Loss: 0.1636\n",
      "Epoch 8, Sample 2530000, Loss: 0.7320\n",
      "Epoch 8, Sample 2540000, Loss: 0.5261\n",
      "Epoch 8, Sample 2550000, Loss: 0.3040\n",
      "Epoch 8, Sample 2560000, Loss: 0.0132\n",
      "Epoch 8, Sample 2570000, Loss: 0.0056\n",
      "Epoch 8, Sample 2580000, Loss: 0.1197\n",
      "Epoch 8, Sample 2590000, Loss: 0.1152\n",
      "Epoch 8, Sample 2600000, Loss: 0.1239\n",
      "Epoch 8, Sample 2610000, Loss: 0.1355\n",
      "Epoch 8, Sample 2620000, Loss: 0.2131\n",
      "Epoch 8, Sample 2630000, Loss: 0.2547\n",
      "Epoch 8, Sample 2640000, Loss: 0.1081\n",
      "Epoch 8, Sample 2650000, Loss: 0.1242\n",
      "Epoch 8, Sample 2660000, Loss: 0.0832\n",
      "Epoch 8, Sample 2670000, Loss: 0.4263\n",
      "Epoch 8, Sample 2680000, Loss: 0.1242\n",
      "Epoch 8, Sample 2690000, Loss: 0.0852\n",
      "Epoch 8, Sample 2700000, Loss: 0.1057\n",
      "Epoch 8, Sample 2710000, Loss: 0.0053\n",
      "Epoch 8, Sample 2720000, Loss: 0.3761\n",
      "Epoch 8, Sample 2730000, Loss: 0.1834\n",
      "Epoch 8, Sample 2740000, Loss: 0.1647\n",
      "Epoch 8, Sample 2750000, Loss: 0.4678\n",
      "Epoch 8, Sample 2760000, Loss: 0.1630\n",
      "Epoch 8, Sample 2770000, Loss: 0.0091\n",
      "Epoch 8, Sample 2780000, Loss: 0.0182\n",
      "Epoch 8, Sample 2790000, Loss: 0.1222\n",
      "Epoch 8, Sample 2800000, Loss: 0.0930\n",
      "Epoch 8, Sample 2810000, Loss: 0.0731\n",
      "Epoch 8, Sample 2820000, Loss: 0.1252\n",
      "Epoch 8, Sample 2830000, Loss: 0.0297\n",
      "Epoch 8, Sample 2840000, Loss: 0.2459\n",
      "Epoch 8, Sample 2850000, Loss: 0.1654\n",
      "Epoch 8, Sample 2860000, Loss: 0.0037\n",
      "Epoch 8, Sample 2870000, Loss: 0.0760\n",
      "Epoch 8, Sample 2880000, Loss: 0.1414\n",
      "Epoch 8, Sample 2890000, Loss: 0.0163\n",
      "Epoch 8, Sample 2900000, Loss: 0.0026\n",
      "Epoch 8, Sample 2910000, Loss: 0.0226\n",
      "Epoch 8, Sample 2920000, Loss: 0.4040\n",
      "Epoch 8, Sample 2930000, Loss: 0.2264\n",
      "Epoch 8, Sample 2940000, Loss: 0.1015\n",
      "Epoch 8, Sample 2950000, Loss: 0.0048\n",
      "Epoch 8, Sample 2960000, Loss: 0.1728\n",
      "Epoch 8, Sample 2970000, Loss: 0.1393\n",
      "Epoch 8, Sample 2980000, Loss: 0.3075\n",
      "Epoch 8, Sample 2990000, Loss: 0.0911\n",
      "Epoch 8, Sample 3000000, Loss: 0.3811\n",
      "Epoch 8, Sample 3010000, Loss: 0.0895\n",
      "Epoch 8, Sample 3020000, Loss: 0.2280\n",
      "Epoch 8, Sample 3030000, Loss: 0.0017\n",
      "Epoch 8, Sample 3040000, Loss: 0.1007\n",
      "Epoch 8, Sample 3050000, Loss: 0.0723\n",
      "Epoch 8, Sample 3060000, Loss: 0.0014\n",
      "Epoch 8, Sample 3070000, Loss: 0.1360\n",
      "Epoch 8/10, Average Loss: 0.1573\n",
      "Epoch 9, Sample 0, Loss: 0.0093\n",
      "Epoch 9, Sample 10000, Loss: 0.0715\n",
      "Epoch 9, Sample 20000, Loss: 0.2448\n",
      "Epoch 9, Sample 30000, Loss: 0.2083\n",
      "Epoch 9, Sample 40000, Loss: 0.0809\n",
      "Epoch 9, Sample 50000, Loss: 0.0148\n",
      "Epoch 9, Sample 60000, Loss: 0.1171\n",
      "Epoch 9, Sample 70000, Loss: 0.3208\n",
      "Epoch 9, Sample 80000, Loss: 0.1607\n",
      "Epoch 9, Sample 90000, Loss: 0.0103\n",
      "Epoch 9, Sample 100000, Loss: 0.0860\n",
      "Epoch 9, Sample 110000, Loss: 0.1309\n",
      "Epoch 9, Sample 120000, Loss: 0.0102\n",
      "Epoch 9, Sample 130000, Loss: 0.1658\n",
      "Epoch 9, Sample 140000, Loss: 0.2589\n",
      "Epoch 9, Sample 150000, Loss: 0.5142\n",
      "Epoch 9, Sample 160000, Loss: 0.1792\n",
      "Epoch 9, Sample 170000, Loss: 0.0776\n",
      "Epoch 9, Sample 180000, Loss: 0.1761\n",
      "Epoch 9, Sample 190000, Loss: 0.1907\n",
      "Epoch 9, Sample 200000, Loss: 0.1584\n",
      "Epoch 9, Sample 210000, Loss: 0.4258\n",
      "Epoch 9, Sample 220000, Loss: 0.3346\n",
      "Epoch 9, Sample 230000, Loss: 0.1310\n",
      "Epoch 9, Sample 240000, Loss: 0.2343\n",
      "Epoch 9, Sample 250000, Loss: 0.2318\n",
      "Epoch 9, Sample 260000, Loss: 0.1562\n",
      "Epoch 9, Sample 270000, Loss: 0.2292\n",
      "Epoch 9, Sample 280000, Loss: 0.4146\n",
      "Epoch 9, Sample 290000, Loss: 0.0857\n",
      "Epoch 9, Sample 300000, Loss: 0.3535\n",
      "Epoch 9, Sample 310000, Loss: 0.0020\n",
      "Epoch 9, Sample 320000, Loss: 0.1995\n",
      "Epoch 9, Sample 330000, Loss: 0.0056\n",
      "Epoch 9, Sample 340000, Loss: 0.0668\n",
      "Epoch 9, Sample 350000, Loss: 0.1997\n",
      "Epoch 9, Sample 360000, Loss: 0.8298\n",
      "Epoch 9, Sample 370000, Loss: 0.0623\n",
      "Epoch 9, Sample 380000, Loss: 0.1093\n",
      "Epoch 9, Sample 390000, Loss: 0.3577\n",
      "Epoch 9, Sample 400000, Loss: 0.0826\n",
      "Epoch 9, Sample 410000, Loss: 0.0465\n",
      "Epoch 9, Sample 420000, Loss: 0.6321\n",
      "Epoch 9, Sample 430000, Loss: 0.3468\n",
      "Epoch 9, Sample 440000, Loss: 0.2136\n",
      "Epoch 9, Sample 450000, Loss: 0.2194\n",
      "Epoch 9, Sample 460000, Loss: 0.2461\n",
      "Epoch 9, Sample 470000, Loss: 0.1917\n",
      "Epoch 9, Sample 480000, Loss: 0.1333\n",
      "Epoch 9, Sample 490000, Loss: 0.1204\n",
      "Epoch 9, Sample 500000, Loss: 0.0541\n",
      "Epoch 9, Sample 510000, Loss: 0.1444\n",
      "Epoch 9, Sample 520000, Loss: 0.0821\n",
      "Epoch 9, Sample 530000, Loss: 0.1628\n",
      "Epoch 9, Sample 540000, Loss: 0.3870\n",
      "Epoch 9, Sample 550000, Loss: 0.2319\n",
      "Epoch 9, Sample 560000, Loss: 0.3390\n",
      "Epoch 9, Sample 570000, Loss: 0.0673\n",
      "Epoch 9, Sample 580000, Loss: 0.0018\n",
      "Epoch 9, Sample 590000, Loss: 0.0294\n",
      "Epoch 9, Sample 600000, Loss: 0.0791\n",
      "Epoch 9, Sample 610000, Loss: 0.0460\n",
      "Epoch 9, Sample 620000, Loss: 0.2178\n",
      "Epoch 9, Sample 630000, Loss: 0.1121\n",
      "Epoch 9, Sample 640000, Loss: 0.3282\n",
      "Epoch 9, Sample 650000, Loss: 0.2438\n",
      "Epoch 9, Sample 660000, Loss: 0.3611\n",
      "Epoch 9, Sample 670000, Loss: 0.0992\n",
      "Epoch 9, Sample 680000, Loss: 0.0184\n",
      "Epoch 9, Sample 690000, Loss: 0.0508\n",
      "Epoch 9, Sample 700000, Loss: 0.3702\n",
      "Epoch 9, Sample 710000, Loss: 0.3456\n",
      "Epoch 9, Sample 720000, Loss: 0.2278\n",
      "Epoch 9, Sample 730000, Loss: 0.2747\n",
      "Epoch 9, Sample 740000, Loss: 0.2196\n",
      "Epoch 9, Sample 750000, Loss: 0.0046\n",
      "Epoch 9, Sample 760000, Loss: 0.2241\n",
      "Epoch 9, Sample 770000, Loss: 0.1541\n",
      "Epoch 9, Sample 780000, Loss: 0.4640\n",
      "Epoch 9, Sample 790000, Loss: 0.3907\n",
      "Epoch 9, Sample 800000, Loss: 0.1952\n",
      "Epoch 9, Sample 810000, Loss: 0.1550\n",
      "Epoch 9, Sample 820000, Loss: 0.0935\n",
      "Epoch 9, Sample 830000, Loss: 0.0468\n",
      "Epoch 9, Sample 840000, Loss: 0.4567\n",
      "Epoch 9, Sample 850000, Loss: 0.0586\n",
      "Epoch 9, Sample 860000, Loss: 0.0587\n",
      "Epoch 9, Sample 870000, Loss: 0.2473\n",
      "Epoch 9, Sample 880000, Loss: 0.0193\n",
      "Epoch 9, Sample 890000, Loss: 0.2116\n",
      "Epoch 9, Sample 900000, Loss: 0.1533\n",
      "Epoch 9, Sample 910000, Loss: 0.1314\n",
      "Epoch 9, Sample 920000, Loss: 0.0661\n",
      "Epoch 9, Sample 930000, Loss: 0.2141\n",
      "Epoch 9, Sample 940000, Loss: 0.6146\n",
      "Epoch 9, Sample 950000, Loss: 0.1183\n",
      "Epoch 9, Sample 960000, Loss: 0.0651\n",
      "Epoch 9, Sample 970000, Loss: 0.0214\n",
      "Epoch 9, Sample 980000, Loss: 0.2743\n",
      "Epoch 9, Sample 990000, Loss: 0.6830\n",
      "Epoch 9, Sample 1000000, Loss: 0.1068\n",
      "Epoch 9, Sample 1010000, Loss: 0.0230\n",
      "Epoch 9, Sample 1020000, Loss: 0.2691\n",
      "Epoch 9, Sample 1030000, Loss: 0.1135\n",
      "Epoch 9, Sample 1040000, Loss: 0.0796\n",
      "Epoch 9, Sample 1050000, Loss: 0.0612\n",
      "Epoch 9, Sample 1060000, Loss: 0.2416\n",
      "Epoch 9, Sample 1070000, Loss: 0.0685\n",
      "Epoch 9, Sample 1080000, Loss: 0.0577\n",
      "Epoch 9, Sample 1090000, Loss: 0.1792\n",
      "Epoch 9, Sample 1100000, Loss: 0.3043\n",
      "Epoch 9, Sample 1110000, Loss: 0.0674\n",
      "Epoch 9, Sample 1120000, Loss: 0.1020\n",
      "Epoch 9, Sample 1130000, Loss: 0.0588\n",
      "Epoch 9, Sample 1140000, Loss: 0.0072\n",
      "Epoch 9, Sample 1150000, Loss: 0.0456\n",
      "Epoch 9, Sample 1160000, Loss: 0.3297\n",
      "Epoch 9, Sample 1170000, Loss: 0.1457\n",
      "Epoch 9, Sample 1180000, Loss: 0.2658\n",
      "Epoch 9, Sample 1190000, Loss: 0.0995\n",
      "Epoch 9, Sample 1200000, Loss: 0.6225\n",
      "Epoch 9, Sample 1210000, Loss: 0.1013\n",
      "Epoch 9, Sample 1220000, Loss: 0.3228\n",
      "Epoch 9, Sample 1230000, Loss: 0.1853\n",
      "Epoch 9, Sample 1240000, Loss: 0.2421\n",
      "Epoch 9, Sample 1250000, Loss: 0.1516\n",
      "Epoch 9, Sample 1260000, Loss: 0.1834\n",
      "Epoch 9, Sample 1270000, Loss: 0.2390\n",
      "Epoch 9, Sample 1280000, Loss: 0.0054\n",
      "Epoch 9, Sample 1290000, Loss: 0.0111\n",
      "Epoch 9, Sample 1300000, Loss: 0.1903\n",
      "Epoch 9, Sample 1310000, Loss: 0.2054\n",
      "Epoch 9, Sample 1320000, Loss: 0.3754\n",
      "Epoch 9, Sample 1330000, Loss: 0.0111\n",
      "Epoch 9, Sample 1340000, Loss: 0.2355\n",
      "Epoch 9, Sample 1350000, Loss: 0.1022\n",
      "Epoch 9, Sample 1360000, Loss: 0.0319\n",
      "Epoch 9, Sample 1370000, Loss: 0.1520\n",
      "Epoch 9, Sample 1380000, Loss: 0.0805\n",
      "Epoch 9, Sample 1390000, Loss: 0.0069\n",
      "Epoch 9, Sample 1400000, Loss: 0.2185\n",
      "Epoch 9, Sample 1410000, Loss: 0.0965\n",
      "Epoch 9, Sample 1420000, Loss: 0.5418\n",
      "Epoch 9, Sample 1430000, Loss: 0.0155\n",
      "Epoch 9, Sample 1440000, Loss: 0.0042\n",
      "Epoch 9, Sample 1450000, Loss: 0.1994\n",
      "Epoch 9, Sample 1460000, Loss: 0.1837\n",
      "Epoch 9, Sample 1470000, Loss: 0.1058\n",
      "Epoch 9, Sample 1480000, Loss: 0.3355\n",
      "Epoch 9, Sample 1490000, Loss: 0.0952\n",
      "Epoch 9, Sample 1500000, Loss: 0.0932\n",
      "Epoch 9, Sample 1510000, Loss: 0.1203\n",
      "Epoch 9, Sample 1520000, Loss: 0.0682\n",
      "Epoch 9, Sample 1530000, Loss: 0.0502\n",
      "Epoch 9, Sample 1540000, Loss: 0.1350\n",
      "Epoch 9, Sample 1550000, Loss: 0.2266\n",
      "Epoch 9, Sample 1560000, Loss: 0.4123\n",
      "Epoch 9, Sample 1570000, Loss: 0.1269\n",
      "Epoch 9, Sample 1580000, Loss: 0.0023\n",
      "Epoch 9, Sample 1590000, Loss: 0.0166\n",
      "Epoch 9, Sample 1600000, Loss: 0.2126\n",
      "Epoch 9, Sample 1610000, Loss: 0.0006\n",
      "Epoch 9, Sample 1620000, Loss: 0.2154\n",
      "Epoch 9, Sample 1630000, Loss: 0.0457\n",
      "Epoch 9, Sample 1640000, Loss: 0.2436\n",
      "Epoch 9, Sample 1650000, Loss: 0.0827\n",
      "Epoch 9, Sample 1660000, Loss: 0.0563\n",
      "Epoch 9, Sample 1670000, Loss: 0.0210\n",
      "Epoch 9, Sample 1680000, Loss: 0.1577\n",
      "Epoch 9, Sample 1690000, Loss: 0.0420\n",
      "Epoch 9, Sample 1700000, Loss: 0.0061\n",
      "Epoch 9, Sample 1710000, Loss: 0.0443\n",
      "Epoch 9, Sample 1720000, Loss: 0.1441\n",
      "Epoch 9, Sample 1730000, Loss: 0.0104\n",
      "Epoch 9, Sample 1740000, Loss: 0.0079\n",
      "Epoch 9, Sample 1750000, Loss: 0.3511\n",
      "Epoch 9, Sample 1760000, Loss: 0.3307\n",
      "Epoch 9, Sample 1770000, Loss: 0.0085\n",
      "Epoch 9, Sample 1780000, Loss: 0.1715\n",
      "Epoch 9, Sample 1790000, Loss: 0.1624\n",
      "Epoch 9, Sample 1800000, Loss: 0.2166\n",
      "Epoch 9, Sample 1810000, Loss: 0.0120\n",
      "Epoch 9, Sample 1820000, Loss: 0.0270\n",
      "Epoch 9, Sample 1830000, Loss: 0.1220\n",
      "Epoch 9, Sample 1840000, Loss: 0.1159\n",
      "Epoch 9, Sample 1850000, Loss: 0.1619\n",
      "Epoch 9, Sample 1860000, Loss: 0.1998\n",
      "Epoch 9, Sample 1870000, Loss: 0.2637\n",
      "Epoch 9, Sample 1880000, Loss: 0.2446\n",
      "Epoch 9, Sample 1890000, Loss: 0.2769\n",
      "Epoch 9, Sample 1900000, Loss: 0.1586\n",
      "Epoch 9, Sample 1910000, Loss: 0.1307\n",
      "Epoch 9, Sample 1920000, Loss: 0.3270\n",
      "Epoch 9, Sample 1930000, Loss: 0.2115\n",
      "Epoch 9, Sample 1940000, Loss: 0.0700\n",
      "Epoch 9, Sample 1950000, Loss: 0.1941\n",
      "Epoch 9, Sample 1960000, Loss: 0.1185\n",
      "Epoch 9, Sample 1970000, Loss: 0.0766\n",
      "Epoch 9, Sample 1980000, Loss: 0.3786\n",
      "Epoch 9, Sample 1990000, Loss: 0.2985\n",
      "Epoch 9, Sample 2000000, Loss: 0.1514\n",
      "Epoch 9, Sample 2010000, Loss: 0.5148\n",
      "Epoch 9, Sample 2020000, Loss: 0.2238\n",
      "Epoch 9, Sample 2030000, Loss: 0.2944\n",
      "Epoch 9, Sample 2040000, Loss: 0.2452\n",
      "Epoch 9, Sample 2050000, Loss: 0.1897\n",
      "Epoch 9, Sample 2060000, Loss: 0.0888\n",
      "Epoch 9, Sample 2070000, Loss: 0.0548\n",
      "Epoch 9, Sample 2080000, Loss: 0.0179\n",
      "Epoch 9, Sample 2090000, Loss: 0.1698\n",
      "Epoch 9, Sample 2100000, Loss: 0.1999\n",
      "Epoch 9, Sample 2110000, Loss: 0.0039\n",
      "Epoch 9, Sample 2120000, Loss: 0.1106\n",
      "Epoch 9, Sample 2130000, Loss: 0.2857\n",
      "Epoch 9, Sample 2140000, Loss: 0.2856\n",
      "Epoch 9, Sample 2150000, Loss: 0.0960\n",
      "Epoch 9, Sample 2160000, Loss: 0.2256\n",
      "Epoch 9, Sample 2170000, Loss: 0.0908\n",
      "Epoch 9, Sample 2180000, Loss: 0.0955\n",
      "Epoch 9, Sample 2190000, Loss: 0.3501\n",
      "Epoch 9, Sample 2200000, Loss: 0.2659\n",
      "Epoch 9, Sample 2210000, Loss: 0.1424\n",
      "Epoch 9, Sample 2220000, Loss: 0.0048\n",
      "Epoch 9, Sample 2230000, Loss: 0.1737\n",
      "Epoch 9, Sample 2240000, Loss: 0.0679\n",
      "Epoch 9, Sample 2250000, Loss: 0.0818\n",
      "Epoch 9, Sample 2260000, Loss: 0.1184\n",
      "Epoch 9, Sample 2270000, Loss: 0.0220\n",
      "Epoch 9, Sample 2280000, Loss: 0.0085\n",
      "Epoch 9, Sample 2290000, Loss: 0.0102\n",
      "Epoch 9, Sample 2300000, Loss: 0.0677\n",
      "Epoch 9, Sample 2310000, Loss: 0.0695\n",
      "Epoch 9, Sample 2320000, Loss: 0.0638\n",
      "Epoch 9, Sample 2330000, Loss: 0.0136\n",
      "Epoch 9, Sample 2340000, Loss: 0.2754\n",
      "Epoch 9, Sample 2350000, Loss: 0.2322\n",
      "Epoch 9, Sample 2360000, Loss: 0.4039\n",
      "Epoch 9, Sample 2370000, Loss: 0.0680\n",
      "Epoch 9, Sample 2380000, Loss: 0.2314\n",
      "Epoch 9, Sample 2390000, Loss: 0.0640\n",
      "Epoch 9, Sample 2400000, Loss: 0.2063\n",
      "Epoch 9, Sample 2410000, Loss: 0.0044\n",
      "Epoch 9, Sample 2420000, Loss: 0.2488\n",
      "Epoch 9, Sample 2430000, Loss: 0.4543\n",
      "Epoch 9, Sample 2440000, Loss: 0.1144\n",
      "Epoch 9, Sample 2450000, Loss: 0.0858\n",
      "Epoch 9, Sample 2460000, Loss: 0.0698\n",
      "Epoch 9, Sample 2470000, Loss: 0.0478\n",
      "Epoch 9, Sample 2480000, Loss: 0.2119\n",
      "Epoch 9, Sample 2490000, Loss: 0.0580\n",
      "Epoch 9, Sample 2500000, Loss: 0.3467\n",
      "Epoch 9, Sample 2510000, Loss: 0.0071\n",
      "Epoch 9, Sample 2520000, Loss: 0.1533\n",
      "Epoch 9, Sample 2530000, Loss: 0.7416\n",
      "Epoch 9, Sample 2540000, Loss: 0.5328\n",
      "Epoch 9, Sample 2550000, Loss: 0.3055\n",
      "Epoch 9, Sample 2560000, Loss: 0.0118\n",
      "Epoch 9, Sample 2570000, Loss: 0.0053\n",
      "Epoch 9, Sample 2580000, Loss: 0.1148\n",
      "Epoch 9, Sample 2590000, Loss: 0.1068\n",
      "Epoch 9, Sample 2600000, Loss: 0.1178\n",
      "Epoch 9, Sample 2610000, Loss: 0.1272\n",
      "Epoch 9, Sample 2620000, Loss: 0.2113\n",
      "Epoch 9, Sample 2630000, Loss: 0.2539\n",
      "Epoch 9, Sample 2640000, Loss: 0.1013\n",
      "Epoch 9, Sample 2650000, Loss: 0.1155\n",
      "Epoch 9, Sample 2660000, Loss: 0.0781\n",
      "Epoch 9, Sample 2670000, Loss: 0.4414\n",
      "Epoch 9, Sample 2680000, Loss: 0.1150\n",
      "Epoch 9, Sample 2690000, Loss: 0.0773\n",
      "Epoch 9, Sample 2700000, Loss: 0.0963\n",
      "Epoch 9, Sample 2710000, Loss: 0.0051\n",
      "Epoch 9, Sample 2720000, Loss: 0.3833\n",
      "Epoch 9, Sample 2730000, Loss: 0.1786\n",
      "Epoch 9, Sample 2740000, Loss: 0.1613\n",
      "Epoch 9, Sample 2750000, Loss: 0.4841\n",
      "Epoch 9, Sample 2760000, Loss: 0.1577\n",
      "Epoch 9, Sample 2770000, Loss: 0.0077\n",
      "Epoch 9, Sample 2780000, Loss: 0.0178\n",
      "Epoch 9, Sample 2790000, Loss: 0.1171\n",
      "Epoch 9, Sample 2800000, Loss: 0.0844\n",
      "Epoch 9, Sample 2810000, Loss: 0.0659\n",
      "Epoch 9, Sample 2820000, Loss: 0.1172\n",
      "Epoch 9, Sample 2830000, Loss: 0.0262\n",
      "Epoch 9, Sample 2840000, Loss: 0.2454\n",
      "Epoch 9, Sample 2850000, Loss: 0.1543\n",
      "Epoch 9, Sample 2860000, Loss: 0.0035\n",
      "Epoch 9, Sample 2870000, Loss: 0.0675\n",
      "Epoch 9, Sample 2880000, Loss: 0.1339\n",
      "Epoch 9, Sample 2890000, Loss: 0.0152\n",
      "Epoch 9, Sample 2900000, Loss: 0.0027\n",
      "Epoch 9, Sample 2910000, Loss: 0.0201\n",
      "Epoch 9, Sample 2920000, Loss: 0.4138\n",
      "Epoch 9, Sample 2930000, Loss: 0.2199\n",
      "Epoch 9, Sample 2940000, Loss: 0.0934\n",
      "Epoch 9, Sample 2950000, Loss: 0.0041\n",
      "Epoch 9, Sample 2960000, Loss: 0.1668\n",
      "Epoch 9, Sample 2970000, Loss: 0.1305\n",
      "Epoch 9, Sample 2980000, Loss: 0.3126\n",
      "Epoch 9, Sample 2990000, Loss: 0.0827\n",
      "Epoch 9, Sample 3000000, Loss: 0.3869\n",
      "Epoch 9, Sample 3010000, Loss: 0.0804\n",
      "Epoch 9, Sample 3020000, Loss: 0.2211\n",
      "Epoch 9, Sample 3030000, Loss: 0.0015\n",
      "Epoch 9, Sample 3040000, Loss: 0.0922\n",
      "Epoch 9, Sample 3050000, Loss: 0.0645\n",
      "Epoch 9, Sample 3060000, Loss: 0.0013\n",
      "Epoch 9, Sample 3070000, Loss: 0.1297\n",
      "Epoch 9/10, Average Loss: 0.1542\n",
      "Epoch 10, Sample 0, Loss: 0.0090\n",
      "Epoch 10, Sample 10000, Loss: 0.0630\n",
      "Epoch 10, Sample 20000, Loss: 0.2443\n",
      "Epoch 10, Sample 30000, Loss: 0.2064\n",
      "Epoch 10, Sample 40000, Loss: 0.0729\n",
      "Epoch 10, Sample 50000, Loss: 0.0140\n",
      "Epoch 10, Sample 60000, Loss: 0.1101\n",
      "Epoch 10, Sample 70000, Loss: 0.3298\n",
      "Epoch 10, Sample 80000, Loss: 0.1520\n",
      "Epoch 10, Sample 90000, Loss: 0.0100\n",
      "Epoch 10, Sample 100000, Loss: 0.0779\n",
      "Epoch 10, Sample 110000, Loss: 0.1238\n",
      "Epoch 10, Sample 120000, Loss: 0.0093\n",
      "Epoch 10, Sample 130000, Loss: 0.1636\n",
      "Epoch 10, Sample 140000, Loss: 0.2593\n",
      "Epoch 10, Sample 150000, Loss: 0.5302\n",
      "Epoch 10, Sample 160000, Loss: 0.1769\n",
      "Epoch 10, Sample 170000, Loss: 0.0698\n",
      "Epoch 10, Sample 180000, Loss: 0.1688\n",
      "Epoch 10, Sample 190000, Loss: 0.1846\n",
      "Epoch 10, Sample 200000, Loss: 0.1537\n",
      "Epoch 10, Sample 210000, Loss: 0.4380\n",
      "Epoch 10, Sample 220000, Loss: 0.3356\n",
      "Epoch 10, Sample 230000, Loss: 0.1249\n",
      "Epoch 10, Sample 240000, Loss: 0.2310\n",
      "Epoch 10, Sample 250000, Loss: 0.2268\n",
      "Epoch 10, Sample 260000, Loss: 0.1482\n",
      "Epoch 10, Sample 270000, Loss: 0.2275\n",
      "Epoch 10, Sample 280000, Loss: 0.4262\n",
      "Epoch 10, Sample 290000, Loss: 0.0777\n",
      "Epoch 10, Sample 300000, Loss: 0.3672\n",
      "Epoch 10, Sample 310000, Loss: 0.0019\n",
      "Epoch 10, Sample 320000, Loss: 0.1920\n",
      "Epoch 10, Sample 330000, Loss: 0.0049\n",
      "Epoch 10, Sample 340000, Loss: 0.0581\n",
      "Epoch 10, Sample 350000, Loss: 0.1961\n",
      "Epoch 10, Sample 360000, Loss: 0.8375\n",
      "Epoch 10, Sample 370000, Loss: 0.0569\n",
      "Epoch 10, Sample 380000, Loss: 0.0992\n",
      "Epoch 10, Sample 390000, Loss: 0.3639\n",
      "Epoch 10, Sample 400000, Loss: 0.0733\n",
      "Epoch 10, Sample 410000, Loss: 0.0431\n",
      "Epoch 10, Sample 420000, Loss: 0.6418\n",
      "Epoch 10, Sample 430000, Loss: 0.3513\n",
      "Epoch 10, Sample 440000, Loss: 0.2089\n",
      "Epoch 10, Sample 450000, Loss: 0.2176\n",
      "Epoch 10, Sample 460000, Loss: 0.2425\n",
      "Epoch 10, Sample 470000, Loss: 0.1879\n",
      "Epoch 10, Sample 480000, Loss: 0.1274\n",
      "Epoch 10, Sample 490000, Loss: 0.1144\n",
      "Epoch 10, Sample 500000, Loss: 0.0521\n",
      "Epoch 10, Sample 510000, Loss: 0.1378\n",
      "Epoch 10, Sample 520000, Loss: 0.0788\n",
      "Epoch 10, Sample 530000, Loss: 0.1540\n",
      "Epoch 10, Sample 540000, Loss: 0.3905\n",
      "Epoch 10, Sample 550000, Loss: 0.2282\n",
      "Epoch 10, Sample 560000, Loss: 0.3425\n",
      "Epoch 10, Sample 570000, Loss: 0.0663\n",
      "Epoch 10, Sample 580000, Loss: 0.0016\n",
      "Epoch 10, Sample 590000, Loss: 0.0288\n",
      "Epoch 10, Sample 600000, Loss: 0.0708\n",
      "Epoch 10, Sample 610000, Loss: 0.0391\n",
      "Epoch 10, Sample 620000, Loss: 0.2136\n",
      "Epoch 10, Sample 630000, Loss: 0.1036\n",
      "Epoch 10, Sample 640000, Loss: 0.3296\n",
      "Epoch 10, Sample 650000, Loss: 0.2399\n",
      "Epoch 10, Sample 660000, Loss: 0.3672\n",
      "Epoch 10, Sample 670000, Loss: 0.0916\n",
      "Epoch 10, Sample 680000, Loss: 0.0170\n",
      "Epoch 10, Sample 690000, Loss: 0.0494\n",
      "Epoch 10, Sample 700000, Loss: 0.3753\n",
      "Epoch 10, Sample 710000, Loss: 0.3561\n",
      "Epoch 10, Sample 720000, Loss: 0.2248\n",
      "Epoch 10, Sample 730000, Loss: 0.2741\n",
      "Epoch 10, Sample 740000, Loss: 0.2188\n",
      "Epoch 10, Sample 750000, Loss: 0.0041\n",
      "Epoch 10, Sample 760000, Loss: 0.2190\n",
      "Epoch 10, Sample 770000, Loss: 0.1448\n",
      "Epoch 10, Sample 780000, Loss: 0.4802\n",
      "Epoch 10, Sample 790000, Loss: 0.4083\n",
      "Epoch 10, Sample 800000, Loss: 0.1889\n",
      "Epoch 10, Sample 810000, Loss: 0.1457\n",
      "Epoch 10, Sample 820000, Loss: 0.0846\n",
      "Epoch 10, Sample 830000, Loss: 0.0427\n",
      "Epoch 10, Sample 840000, Loss: 0.4687\n",
      "Epoch 10, Sample 850000, Loss: 0.0549\n",
      "Epoch 10, Sample 860000, Loss: 0.0524\n",
      "Epoch 10, Sample 870000, Loss: 0.2468\n",
      "Epoch 10, Sample 880000, Loss: 0.0171\n",
      "Epoch 10, Sample 890000, Loss: 0.2032\n",
      "Epoch 10, Sample 900000, Loss: 0.1470\n",
      "Epoch 10, Sample 910000, Loss: 0.1189\n",
      "Epoch 10, Sample 920000, Loss: 0.0601\n",
      "Epoch 10, Sample 930000, Loss: 0.2104\n",
      "Epoch 10, Sample 940000, Loss: 0.6327\n",
      "Epoch 10, Sample 950000, Loss: 0.1122\n",
      "Epoch 10, Sample 960000, Loss: 0.0584\n",
      "Epoch 10, Sample 970000, Loss: 0.0195\n",
      "Epoch 10, Sample 980000, Loss: 0.2750\n",
      "Epoch 10, Sample 990000, Loss: 0.6836\n",
      "Epoch 10, Sample 1000000, Loss: 0.0977\n",
      "Epoch 10, Sample 1010000, Loss: 0.0223\n",
      "Epoch 10, Sample 1020000, Loss: 0.2724\n",
      "Epoch 10, Sample 1030000, Loss: 0.1059\n",
      "Epoch 10, Sample 1040000, Loss: 0.0730\n",
      "Epoch 10, Sample 1050000, Loss: 0.0542\n",
      "Epoch 10, Sample 1060000, Loss: 0.2395\n",
      "Epoch 10, Sample 1070000, Loss: 0.0642\n",
      "Epoch 10, Sample 1080000, Loss: 0.0531\n",
      "Epoch 10, Sample 1090000, Loss: 0.1736\n",
      "Epoch 10, Sample 1100000, Loss: 0.3113\n",
      "Epoch 10, Sample 1110000, Loss: 0.0615\n",
      "Epoch 10, Sample 1120000, Loss: 0.1002\n",
      "Epoch 10, Sample 1130000, Loss: 0.0547\n",
      "Epoch 10, Sample 1140000, Loss: 0.0068\n",
      "Epoch 10, Sample 1150000, Loss: 0.0455\n",
      "Epoch 10, Sample 1160000, Loss: 0.3347\n",
      "Epoch 10, Sample 1170000, Loss: 0.1376\n",
      "Epoch 10, Sample 1180000, Loss: 0.2688\n",
      "Epoch 10, Sample 1190000, Loss: 0.0973\n",
      "Epoch 10, Sample 1200000, Loss: 0.6192\n",
      "Epoch 10, Sample 1210000, Loss: 0.0939\n",
      "Epoch 10, Sample 1220000, Loss: 0.3267\n",
      "Epoch 10, Sample 1230000, Loss: 0.1826\n",
      "Epoch 10, Sample 1240000, Loss: 0.2358\n",
      "Epoch 10, Sample 1250000, Loss: 0.1446\n",
      "Epoch 10, Sample 1260000, Loss: 0.1752\n",
      "Epoch 10, Sample 1270000, Loss: 0.2374\n",
      "Epoch 10, Sample 1280000, Loss: 0.0054\n",
      "Epoch 10, Sample 1290000, Loss: 0.0106\n",
      "Epoch 10, Sample 1300000, Loss: 0.1899\n",
      "Epoch 10, Sample 1310000, Loss: 0.2048\n",
      "Epoch 10, Sample 1320000, Loss: 0.3789\n",
      "Epoch 10, Sample 1330000, Loss: 0.0111\n",
      "Epoch 10, Sample 1340000, Loss: 0.2327\n",
      "Epoch 10, Sample 1350000, Loss: 0.0925\n",
      "Epoch 10, Sample 1360000, Loss: 0.0281\n",
      "Epoch 10, Sample 1370000, Loss: 0.1476\n",
      "Epoch 10, Sample 1380000, Loss: 0.0716\n",
      "Epoch 10, Sample 1390000, Loss: 0.0067\n",
      "Epoch 10, Sample 1400000, Loss: 0.2125\n",
      "Epoch 10, Sample 1410000, Loss: 0.0888\n",
      "Epoch 10, Sample 1420000, Loss: 0.5582\n",
      "Epoch 10, Sample 1430000, Loss: 0.0148\n",
      "Epoch 10, Sample 1440000, Loss: 0.0039\n",
      "Epoch 10, Sample 1450000, Loss: 0.1956\n",
      "Epoch 10, Sample 1460000, Loss: 0.1752\n",
      "Epoch 10, Sample 1470000, Loss: 0.1001\n",
      "Epoch 10, Sample 1480000, Loss: 0.3410\n",
      "Epoch 10, Sample 1490000, Loss: 0.0877\n",
      "Epoch 10, Sample 1500000, Loss: 0.0842\n",
      "Epoch 10, Sample 1510000, Loss: 0.1138\n",
      "Epoch 10, Sample 1520000, Loss: 0.0630\n",
      "Epoch 10, Sample 1530000, Loss: 0.0437\n",
      "Epoch 10, Sample 1540000, Loss: 0.1249\n",
      "Epoch 10, Sample 1550000, Loss: 0.2241\n",
      "Epoch 10, Sample 1560000, Loss: 0.4266\n",
      "Epoch 10, Sample 1570000, Loss: 0.1219\n",
      "Epoch 10, Sample 1580000, Loss: 0.0022\n",
      "Epoch 10, Sample 1590000, Loss: 0.0149\n",
      "Epoch 10, Sample 1600000, Loss: 0.2091\n",
      "Epoch 10, Sample 1610000, Loss: 0.0005\n",
      "Epoch 10, Sample 1620000, Loss: 0.2094\n",
      "Epoch 10, Sample 1630000, Loss: 0.0449\n",
      "Epoch 10, Sample 1640000, Loss: 0.2443\n",
      "Epoch 10, Sample 1650000, Loss: 0.0755\n",
      "Epoch 10, Sample 1660000, Loss: 0.0543\n",
      "Epoch 10, Sample 1670000, Loss: 0.0193\n",
      "Epoch 10, Sample 1680000, Loss: 0.1501\n",
      "Epoch 10, Sample 1690000, Loss: 0.0379\n",
      "Epoch 10, Sample 1700000, Loss: 0.0058\n",
      "Epoch 10, Sample 1710000, Loss: 0.0410\n",
      "Epoch 10, Sample 1720000, Loss: 0.1391\n",
      "Epoch 10, Sample 1730000, Loss: 0.0094\n",
      "Epoch 10, Sample 1740000, Loss: 0.0078\n",
      "Epoch 10, Sample 1750000, Loss: 0.3538\n",
      "Epoch 10, Sample 1760000, Loss: 0.3334\n",
      "Epoch 10, Sample 1770000, Loss: 0.0084\n",
      "Epoch 10, Sample 1780000, Loss: 0.1642\n",
      "Epoch 10, Sample 1790000, Loss: 0.1551\n",
      "Epoch 10, Sample 1800000, Loss: 0.2163\n",
      "Epoch 10, Sample 1810000, Loss: 0.0110\n",
      "Epoch 10, Sample 1820000, Loss: 0.0239\n",
      "Epoch 10, Sample 1830000, Loss: 0.1160\n",
      "Epoch 10, Sample 1840000, Loss: 0.1118\n",
      "Epoch 10, Sample 1850000, Loss: 0.1548\n",
      "Epoch 10, Sample 1860000, Loss: 0.1954\n",
      "Epoch 10, Sample 1870000, Loss: 0.2600\n",
      "Epoch 10, Sample 1880000, Loss: 0.2464\n",
      "Epoch 10, Sample 1890000, Loss: 0.2757\n",
      "Epoch 10, Sample 1900000, Loss: 0.1519\n",
      "Epoch 10, Sample 1910000, Loss: 0.1223\n",
      "Epoch 10, Sample 1920000, Loss: 0.3276\n",
      "Epoch 10, Sample 1930000, Loss: 0.2123\n",
      "Epoch 10, Sample 1940000, Loss: 0.0640\n",
      "Epoch 10, Sample 1950000, Loss: 0.1934\n",
      "Epoch 10, Sample 1960000, Loss: 0.1117\n",
      "Epoch 10, Sample 1970000, Loss: 0.0707\n",
      "Epoch 10, Sample 1980000, Loss: 0.3889\n",
      "Epoch 10, Sample 1990000, Loss: 0.2979\n",
      "Epoch 10, Sample 2000000, Loss: 0.1431\n",
      "Epoch 10, Sample 2010000, Loss: 0.5235\n",
      "Epoch 10, Sample 2020000, Loss: 0.2192\n",
      "Epoch 10, Sample 2030000, Loss: 0.2976\n",
      "Epoch 10, Sample 2040000, Loss: 0.2474\n",
      "Epoch 10, Sample 2050000, Loss: 0.1873\n",
      "Epoch 10, Sample 2060000, Loss: 0.0824\n",
      "Epoch 10, Sample 2070000, Loss: 0.0489\n",
      "Epoch 10, Sample 2080000, Loss: 0.0170\n",
      "Epoch 10, Sample 2090000, Loss: 0.1678\n",
      "Epoch 10, Sample 2100000, Loss: 0.1971\n",
      "Epoch 10, Sample 2110000, Loss: 0.0036\n",
      "Epoch 10, Sample 2120000, Loss: 0.1016\n",
      "Epoch 10, Sample 2130000, Loss: 0.2852\n",
      "Epoch 10, Sample 2140000, Loss: 0.2816\n",
      "Epoch 10, Sample 2150000, Loss: 0.0899\n",
      "Epoch 10, Sample 2160000, Loss: 0.2250\n",
      "Epoch 10, Sample 2170000, Loss: 0.0828\n",
      "Epoch 10, Sample 2180000, Loss: 0.0909\n",
      "Epoch 10, Sample 2190000, Loss: 0.3543\n",
      "Epoch 10, Sample 2200000, Loss: 0.2608\n",
      "Epoch 10, Sample 2210000, Loss: 0.1329\n",
      "Epoch 10, Sample 2220000, Loss: 0.0043\n",
      "Epoch 10, Sample 2230000, Loss: 0.1653\n",
      "Epoch 10, Sample 2240000, Loss: 0.0627\n",
      "Epoch 10, Sample 2250000, Loss: 0.0748\n",
      "Epoch 10, Sample 2260000, Loss: 0.1130\n",
      "Epoch 10, Sample 2270000, Loss: 0.0200\n",
      "Epoch 10, Sample 2280000, Loss: 0.0086\n",
      "Epoch 10, Sample 2290000, Loss: 0.0098\n",
      "Epoch 10, Sample 2300000, Loss: 0.0631\n",
      "Epoch 10, Sample 2310000, Loss: 0.0630\n",
      "Epoch 10, Sample 2320000, Loss: 0.0579\n",
      "Epoch 10, Sample 2330000, Loss: 0.0119\n",
      "Epoch 10, Sample 2340000, Loss: 0.2782\n",
      "Epoch 10, Sample 2350000, Loss: 0.2299\n",
      "Epoch 10, Sample 2360000, Loss: 0.4095\n",
      "Epoch 10, Sample 2370000, Loss: 0.0628\n",
      "Epoch 10, Sample 2380000, Loss: 0.2324\n",
      "Epoch 10, Sample 2390000, Loss: 0.0579\n",
      "Epoch 10, Sample 2400000, Loss: 0.2009\n",
      "Epoch 10, Sample 2410000, Loss: 0.0041\n",
      "Epoch 10, Sample 2420000, Loss: 0.2500\n",
      "Epoch 10, Sample 2430000, Loss: 0.4647\n",
      "Epoch 10, Sample 2440000, Loss: 0.1071\n",
      "Epoch 10, Sample 2450000, Loss: 0.0767\n",
      "Epoch 10, Sample 2460000, Loss: 0.0610\n",
      "Epoch 10, Sample 2470000, Loss: 0.0417\n",
      "Epoch 10, Sample 2480000, Loss: 0.2073\n",
      "Epoch 10, Sample 2490000, Loss: 0.0545\n",
      "Epoch 10, Sample 2500000, Loss: 0.3540\n",
      "Epoch 10, Sample 2510000, Loss: 0.0075\n",
      "Epoch 10, Sample 2520000, Loss: 0.1442\n",
      "Epoch 10, Sample 2530000, Loss: 0.7492\n",
      "Epoch 10, Sample 2540000, Loss: 0.5378\n",
      "Epoch 10, Sample 2550000, Loss: 0.3062\n",
      "Epoch 10, Sample 2560000, Loss: 0.0108\n",
      "Epoch 10, Sample 2570000, Loss: 0.0051\n",
      "Epoch 10, Sample 2580000, Loss: 0.1105\n",
      "Epoch 10, Sample 2590000, Loss: 0.0996\n",
      "Epoch 10, Sample 2600000, Loss: 0.1126\n",
      "Epoch 10, Sample 2610000, Loss: 0.1197\n",
      "Epoch 10, Sample 2620000, Loss: 0.2099\n",
      "Epoch 10, Sample 2630000, Loss: 0.2531\n",
      "Epoch 10, Sample 2640000, Loss: 0.0954\n",
      "Epoch 10, Sample 2650000, Loss: 0.1081\n",
      "Epoch 10, Sample 2660000, Loss: 0.0740\n",
      "Epoch 10, Sample 2670000, Loss: 0.4547\n",
      "Epoch 10, Sample 2680000, Loss: 0.1072\n",
      "Epoch 10, Sample 2690000, Loss: 0.0706\n",
      "Epoch 10, Sample 2700000, Loss: 0.0883\n",
      "Epoch 10, Sample 2710000, Loss: 0.0049\n",
      "Epoch 10, Sample 2720000, Loss: 0.3894\n",
      "Epoch 10, Sample 2730000, Loss: 0.1743\n",
      "Epoch 10, Sample 2740000, Loss: 0.1581\n",
      "Epoch 10, Sample 2750000, Loss: 0.4987\n",
      "Epoch 10, Sample 2760000, Loss: 0.1529\n",
      "Epoch 10, Sample 2770000, Loss: 0.0067\n",
      "Epoch 10, Sample 2780000, Loss: 0.0176\n",
      "Epoch 10, Sample 2790000, Loss: 0.1126\n",
      "Epoch 10, Sample 2800000, Loss: 0.0772\n",
      "Epoch 10, Sample 2810000, Loss: 0.0599\n",
      "Epoch 10, Sample 2820000, Loss: 0.1101\n",
      "Epoch 10, Sample 2830000, Loss: 0.0233\n",
      "Epoch 10, Sample 2840000, Loss: 0.2447\n",
      "Epoch 10, Sample 2850000, Loss: 0.1443\n",
      "Epoch 10, Sample 2860000, Loss: 0.0034\n",
      "Epoch 10, Sample 2870000, Loss: 0.0603\n",
      "Epoch 10, Sample 2880000, Loss: 0.1274\n",
      "Epoch 10, Sample 2890000, Loss: 0.0144\n",
      "Epoch 10, Sample 2900000, Loss: 0.0027\n",
      "Epoch 10, Sample 2910000, Loss: 0.0183\n",
      "Epoch 10, Sample 2920000, Loss: 0.4222\n",
      "Epoch 10, Sample 2930000, Loss: 0.2140\n",
      "Epoch 10, Sample 2940000, Loss: 0.0865\n",
      "Epoch 10, Sample 2950000, Loss: 0.0036\n",
      "Epoch 10, Sample 2960000, Loss: 0.1615\n",
      "Epoch 10, Sample 2970000, Loss: 0.1230\n",
      "Epoch 10, Sample 2980000, Loss: 0.3170\n",
      "Epoch 10, Sample 2990000, Loss: 0.0756\n",
      "Epoch 10, Sample 3000000, Loss: 0.3913\n",
      "Epoch 10, Sample 3010000, Loss: 0.0727\n",
      "Epoch 10, Sample 3020000, Loss: 0.2148\n",
      "Epoch 10, Sample 3030000, Loss: 0.0014\n",
      "Epoch 10, Sample 3040000, Loss: 0.0851\n",
      "Epoch 10, Sample 3050000, Loss: 0.0580\n",
      "Epoch 10, Sample 3060000, Loss: 0.0012\n",
      "Epoch 10, Sample 3070000, Loss: 0.1244\n",
      "Epoch 10/10, Average Loss: 0.1516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAAIhCAYAAACrJeDiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABkX0lEQVR4nO3dB3TUVd7G8Se9QAohEFroPSC9SQSRaseCoq5gwbaK2AXdVXjVVbArKjZWVBRFsVMELIgivUNC7xACIRBIL++5F5MlYkkCmf9k5vs5557M/KfkN+aCPLnNp6CgoEAAAAAAgFLxLd3TAQAAAAAGYQoAAAAAyoAwBQAAAABlQJgCAAAAgDIgTAEAAABAGRCmAAAAAKAMCFMAAAAAUAaEKQAAAAAoA8IUAAAAAJQBYQoAcFqMHDlSzZo1+8t27bXXntL3ePnll+37lPdrysqV3wsA4Dx/pwsAAHiGf/7znxo8eHDR/VdffVXr1q3T+PHji65Vrlz5lL7HoEGDdNZZZ5X7awAAKAnCFADgtKhbt65thaKiohQYGKi2bduetu9Ro0YN28r7NQAAlATT/AAALjVt2jS1bNlSU6dOVffu3dW5c2dt2rRJeXl5euONN3TBBRfojDPOsCHMjHT9+uuvfzqNzkwbfPjhh+3rzj77bLVu3dq+ZtWqVaf0GuOHH37QpZdeamvp37+/vv76a/Xt29e+36natm2b7rzzTvv5zec0NS1durTYc8z3u+iii+z379q1q+677z4lJSUVPb5mzRoNHTpUHTp0ULt27XTddddpxYoVp1wbAKDkCFMAAJczwWnixIl64oknNGrUKDVq1EjPPPOMnRp45ZVX6q233tJjjz2m1NRUjRgxQhkZGX/6XrNmzdLcuXP1r3/9S88995wOHDig4cOH2+9R1teYAGemLdasWdOGp2uuuUaPPvqo9u7de8qf3QRHE9J27dplv7/53D4+PjYYLVq0yD7HBKsHHnhA/fr105tvvmn/G5ma7r33Xvv40aNHNWzYMFWpUsXW9/zzz9v/RjfeeKPS0tJOuUYAQMkwzQ8A4Ihbb73VjgwV2r9/v+6+++5im1QEBQXZkJOYmPin0wVzc3P19ttvF63HOnbsmB588EGtX79erVq1KtNrTEBp0qSJXe9lgo5RtWpV3XPPPaf8uc17mumP7777btH3N/8dzIjcuHHj9Mknn9gwFRwcrJtvvtk+14iMjNTq1atVUFBgA9mhQ4c0ZMgQtW/f3j7esGFDffTRR/azhIWFnXKdAIC/x8gUAMARLVq0KHb/2WeftaMzKSkpWrJkiT799FN9+eWX9rHs7Ow/fZ/GjRsX29giJibGfv2r0ay/eo35XsuXL7ejQoVByhgwYID8/U/9d5Bm9KlXr17Fvr953/PPP99O3TNhqFOnTrYWE7DMfxfz3yM+Pl533HGHrckEPbMmzQTSRx55RLNnz1Z0dLTuv/9+1ocBgAsRpgAAjggNDS1234y6XH755erWrZudwvbhhx/K1/f4/6bMaMyfCQkJKXa/8DX5+flleo2ZWmim+5mRqBP5+fnZ0aFTdfjwYRt8fs9cM5/TTOEza6DMmq7Y2Fj997//tdMMe/Tooffee88+t1KlSpo8ebJ69uypGTNm2JBl/ruZYPVXwRMAcHoxzQ8A4LjCNUBmo4hvvvnGTlkzAefHH3+065tcyYSogIAAu47qRIVB61RFRESc9N5GcnKy/WrWQRlmO3fTzAiVWS9lpgU+/vjjatOmjd2Uwvw3evrpp23wM5tnfPHFFzaAmh0VzX9LAED5Y2QKAOC4LVu22KBi1gCZKXiFI0Xz5s3721Gm082MQJl1SGaDihN99913dq3VqTJT+L7//nsbIAuZQGRCpNlZ0KyRGjt2rC677DI7UmVG0cy0QLOmy9izZ49mzpxpd/gzAczUa0ayRo8erfDwcPs4AMA1GJkCADiuQYMGdg3RhAkT7Poh08yIlNmM4e/WP5UHs2252QjDfDVTD01AefHFF+1jJ66j+jPvvPPOSddM0DG7+JkpeSYkmuBoNpgwo2Dvv/++du7caXcxNExQMtP7Ro4cabdHz8nJsY+ZaYbmMTOVzwTM22+/3b6HmfZnpvuZnfzMWi8AgGsQpgAAjjO7z5lt0c1udmYrdBMOzAYVJmTcdNNNdgOGc845x2X1dOzY0e7oZwKU2SK9du3a+ve//213GzS1/Z0nn3zypGtm+p0JU2bziA8++MBuyW62PDfhzEzbM9P4zPc1zFoos2W62T6+cNMJc56UeU7hui0Trkx95swsEzbN+5qaTdgCALiGT8FfreoFAMALmSl+Zle8uLi4omsbN260u+uZ0Ne7d29H6wMAuAdGpgAA+J358+dr+vTpuu++++wUxKSkJL322mt20wezRTkAAAYjUwAA/E5mZqadQmfWbZnDhM3UOrOz3r333vuH25oDALwTYQoAAAAAyoCt0QEAAACgDAhTAAAAAFAGhCkAAAAAKAN285PswYfmVHtfX98SHcYIAAAAwDOZLSVMPjAHyJt88FcIU5INUqtXr3a6DAAAAABuonXr1goMDPzL5xCmzFzH3xKn+Q/m5+fndDnKy8uz4c5d6oFno7/B1ehzcCX6G1yNPuc5P8O/G5UyCFNmf/jfpvaZDu9Ond7d6oFno7/B1ehzcCX6G1yNPlfxlWT5DxtQAAAAAEAZEKYAAAAAoAwIUwAAAABQBoQpAAAAACgDwhQAAAAAlAFhCgAAAADKgDAFAAAAAGVAmAIAAACAMiBMAQAAAEAZEKYAAAAAoAwIUwAAAABQBoQpAAAAACgDwhQAAAAAlAFhCgAAAADKgDAFAAAAAGVAmAIAAACAMvAvy4tQfgoKCrR8R6py8gqcLgUAAADAX2Bkys38uCFZl7/+q15cmOp0KQAAAAD+AmHKzdSrWkl+vj76dXeWlmw/5HQ5AAAAAP4EYcrNNIiupMvb17a3x81KtNP+AAAAALgfwpQbuvOcxgr0k5ZuT9Wc9fudLgcAAADAHyBMuaEaEcE6v0kle3vczATl5TM6BQAAALgbwpSbuqRZJUWEBGjj/qP6dNkup8sBAAAA8DuEKTdVKdBX/zy7ob39/OwNyszJc7okAAAAACcgTLmxa7vUVa2IYO09nKl3F2xzuhwAAAAAJyBMubGgAD/d3bepvf3K95t1OCPH6ZIAAAAA/IYw5eYubV9HTWMq2yA14cfNTpcDAAAA4DeEKTdnDvB9oH9ze3vi/K3adzjT6ZIAAAAAEKYqht4tqqtT/SrKys3XC3M2OF0OAAAAAMJUxeDj46OR5x4fnfp4yU5t2n/U6ZIAAAAAr0eYqiA61ItS35YxMuf3Pj0rwelyAAAAAK9HmKpAHujfTL4+0qy1SVq6/ZDT5QAAAABejTBVgTSJCdPlHerY22NnJKigoMDpkgAAAACvRZiqYO7q01RB/r5atC1F3yfud7ocAAAAwGsRpiqYWpEhuq57fXt73MxE5ZlFVAAAAABcjjBVAf2zZ2OFB/srYV+aPl++2+lyAAAAAK9EmKqAIkID9M9eje3t52ZvUGZOntMlAQAAAF6HMFVBXXdmfdUID9bu1Ay9/+t2p8sBAAAAvA5hqoIKDvDT3X2b2Nvjv9+kI5k5TpcEAAAAeBXCVAV2Wfs6aly9slLTc/T6j5udLgcAAADwKoSpCszfz1f3929mb789f6v2H8l0uiQAAADAazgaprKysvTQQw+pY8eOio+P18SJE//0uT/88IMuvvhitWvXThdeeKHmzp1b9Jg5vPaNN97QOeeco/bt22vo0KHatGmTvEG/ljFqXzdSmTn5emHuRqfLAQAAALyGo2Fq3LhxWrNmjSZNmqRHH31U48eP18yZM096XkJCgu644w5ddtll+vzzzzV48GCNGDHCXjemTJlig9i///1vffrpp6pTp45uuukmZWRkyNP5+Pho5Lkt7O2PFu/UluSjTpcEAAAAeAXHwlR6erqmTp2qhx9+WHFxcerbt6+GDRumyZMnn/Tcr7/+Wl27dtWQIUNUr149XXPNNerSpYtmzJhhH//ss890ww03qFevXmrQoIFGjx6t1NRULVu2TN6gc4Mo9W5e3R7g+8y3iU6XAwAAAHgFf6e+sRlVys3NtdP2CnXo0EETJkxQfn6+fH3/l/MuueQS5eScvFtdWlqa/frAAw/Y0agTR2vM1L/Cx73BAwOa67vE/Zq+ep9W7ExV29hIp0sCAAAAPJpjYSo5OVlVqlRRYGBg0bXo6Gi7jsqMKkVFRRVdb9SoUbHXbty4UQsWLLDT/Qyz5upEZsTLBDUTzkojL889Dr8trKM09TSuFqpL29XWp8t268np6zX5xk42VALl0d+AU0GfgyvR3+Bq9LmKrzQ/O8fClFnPdGKQMgrvZ2dn/+nrUlJSNHz4cLvRRO/evU96fOXKlRo7dqxuvPFGVatWrVQ1rV69Wu6ktPX0rZmnL32lhVtT9M6sRWpXI6jcaoPncbf+D89Hn4Mr0d/gavQ57+BYmAoKCjopNBXeDw4O/sPXHDhwQNdff72dwvfSSy8VmwpoLF++3G480aNHD7tBRWm1bt1afn5+coc0bP4AlqWeIYcT9Pb8bfp0Y66G9ussX19Gp1B+/Q0oC/ocXIn+Blejz3nOz9Ctw1RMTIwOHTpkp+P5+/sXTf0zQSo8PPyk5yclJdkNKIx333232DRAY+HChbr11lvVvXt3PfvssycFrZIwHd6dOn1Z6rmjVxN9vHiX1u9L0zdrkjSwXe1yqw+exd36PzwffQ6uRH+Dq9HnvINju/m1aNHChqgVK1YUXVu6dKlN8b8PQmbnP7PTn7n+/vvv2yB2og0bNui2227TWWedpRdeeEEBAQHyVlUqBerWs4+vMTM7+2XlMl8XAAAA8KgwFRISooEDB9ptzFetWqU5c+bYs6IKR5/MKFVmZqa9/frrr2vHjh12LVThY6YV7tb3yCOPqGbNmho1apQd7Sp8vPD13uaG7g1UPSxIuw5l6IOFO5wuBwAAAPBIjh7aa8KPOWNq6NChGjNmjN1Yol+/fvax+Ph4TZ8+3d6eNWuWDUaDBg2y1wvbE088YUOTWSu1adMmnX322cUeL3y9twkJ9NNdfZra2y9/t0lpmSdvKw8AAADg1Di2ZqpwdMqMNhWOOJ0oMfF/h8/OnDnzL9/nxOfiuCs61tFbP23RlgPH9Oa8LbqnXzOnSwIAAAA8iqMjUyg//n6+emDA8QD11vyt2p/mnVMeAQAAgPJCmPJg/eNqqG1spNKz8/Ty3E1OlwMAAAB4FMKUB/Px8dHIc5vb2x8u2qFtB445XRIAAADgMQhTHq5rw6o6u1k15eYX2K3SAQAAAJwehCkv8ED/5vLxkb5etVerdx12uhwAAADAIxCmvEDLWuG6pG1te3vszASnywEAAAA8AmHKS9zdt6kC/Xw1f9MB/bQx2elyAAAAgAqPMOUlYqNC9Y+u9eztp2YkKD+/wOmSAAAAgAqNMOVF7jinsSoH+WvtniP6evVep8sBAAAAKjTClBeJqhSoW3o0tLefmZWo7Nx8p0sCAAAAKizClJe58awGiq4cpB0p6ZqyeIfT5QAAAAAVFmHKy4QG+mtEnyb29ktzN+poVq7TJQEAAAAVEmHKCw3uFKv6VUN14Gi23vppi9PlAAAAABUSYcoLBfj56v7+ze3tN+dt0YGjWU6XBAAAAFQ4hCkvdV7rGjqjToSOZedp/HebnC4HAAAAqHAIU17Kx8dHIwccH52avHC7dhxMd7okAAAAoEIhTHmxMxtHq0fTasrJK9CzsxOdLgcAAACoUAhTXu6B/s3s1y9W7NGa3YedLgcAAACoMAhTXq5V7Qhd3LaWvT1uFqNTAAAAQEkRpqB7+zZTgJ+P5m1I1s+bDjhdDgAAAFAhEKagulVDdU2Xevb22JkJKigocLokAAAAwO0RpmDdcU5jVQr006pdhzV99T6nywEAAADcHmEKVnTlIN3Uo6G9/fSsBOXk5TtdEgAAAODWCFMoMuyshoquHKhtB9M1ZfFOp8sBAAAA3BphCkUqB/nrzt5N7O0X52zUsaxcp0sCAAAA3BZhCsUM7lRXdaNCdeBolibO3+p0OQAAAIDbIkyhmEB/X93320G+r8/bopRj2U6XBAAAALglwhROckHrmoqrFa6jWbka/90mp8sBAAAA3BJhCifx9fXRyHOb29vv/7pdO1PSnS4JAAAAcDuEKfyhs5pUU3zjaGXn5ev52RucLgcAAABwO4Qp/KkHBxwfnfpsxW6t23PE6XIAAAAAt0KYwp9qXSdCF5xRUwUF0rhZCU6XAwAAALgVwhT+0n39msnf10c/JCZrweaDTpcDAAAAuA3CFP5S/ehKuqpzXXv7qZkJKjDDVAAAAAAIU/h7w3s3Vmign1buTNWstfucLgcAAABwC4Qp/K3qYcEadlZDe3vczETl5uU7XRIAAADgOMIUSuSmsxooqlKgthw4po+X7HK6HAAAAMBxhCmUSFhwgIaf09jefmHOBmVk5zldEgAAAOAowhRK7OoudVWnSoj2p2Vp4s9bnS4HAAAAcBRhCiUW5O9nt0o3JvywWYeOZTtdEgAAAOAYwhRK5aI2tdSiZrjSsnL16g+bnC4HAAAAcAxhCqXi6+ujBwccH52a9Mt27U7NcLokAAAAwBGEKZRaz6bV1K1hVWXn5ev52RucLgcAAABwBGEKpebj46OR5za3tz9dtksJ+444XRIAAADgcoQplEmb2Eid17qGCgqkp2cmOl0OAAAA4F1hKisrSw899JA6duyo+Ph4TZw48U+f+8MPP+jiiy9Wu3btdOGFF2ru3Ll/+LzXXntNI0eOLMeqUcjs7Ofn66O5Cfu1aGuK0+UAAAAA3hOmxo0bpzVr1mjSpEl69NFHNX78eM2cOfOk5yUkJOiOO+7QZZddps8//1yDBw/WiBEj7PUTff3113r55Zdd+Am8W8NqlXVlp1h7+6kZ61VghqkAAAAAL+Hv1DdOT0/X1KlT9eabbyouLs62jRs3avLkyRowYMBJIalr164aMmSIvV+vXj199913mjFjhpo3b67c3Fw99thj+uyzzxQbe/wf93CNu3o30WfLdmvZjlR9uy5J/eNqOF0SAAAA4NkjU2ZUyYQgM22vUIcOHbRy5Url5+cXe+4ll1yi++6776T3SEtLKwpmiYmJ+vjjj4u9H8pf9fBg3RjfwN5+elaicvOK/+wAAAAAT+XYyFRycrKqVKmiwMDAomvR0dF2HVVqaqqioqKKrjdq1KjYa80I1oIFC+x0PyM8PFxTpkw55Zry8vLkDgrrcJd6/s6w+HqavHC7Nu0/qqlLduqKjnWcLgke3N9Q8dHn4Er0N7gafa7iK83PzrEwlZGRUSxIGYX3s7Oz//R1KSkpGj58uNq3b6/evXuf1ppWr14td+Ju9fyVi5sE652VOXp65jrV80lWkJ+P0yXBg/sbPAN9Dq5Ef4Or0ee8g2NhKigo6KTQVHg/ODj4D19z4MABXX/99Xajg5deekm+vqd3lmLr1q3l5+cnd0jD5g+gu9RTEi1a5Wv29nnanZqpFccidEuPhk6XBA/ub6jY6HNwJfobXI0+5zk/Q7cOUzExMTp06JBdN+Xv71809c8EKTNt7/eSkpKKNqB49913i00DPF1Mh3enTu9u9fyVUD8/3dO3me6dulITftyia7rUV0RogNNlwUP7GzwDfQ6uRH+Dq9HnvINjG1C0aNHChqgVK1YUXVu6dKlN8b8fcTIbTAwbNsxef//9920Qg/sZ2K62mtcI05HMXL364yanywEAAAA8M0yFhIRo4MCBGj16tFatWqU5c+bYQ3sLR5/MKFVmZqa9/frrr2vHjh0aO3Zs0WOmFe7mB/dgDvB9YEAze/u/P2/TntQMp0sCAAAAPPPQ3lGjRtnzpYYOHaoxY8bYjSX69etnH4uPj9f06dPt7VmzZtlgNWjQIHu9sD3xxBNOlo8/0KtZdXVuEKXs3Hy9MGeD0+UAAAAA5caxNVOFo1NmtKlwxOlE5tyoQjNnzizxez711FOnrT6Uno+Pj0ae21yXvvqLPlm6Szed1VBNYsKcLgsAAADwrJEpeKb2dauof1yM8gukcbP+F4oBAAAAT0KYQrm4v39z+fpIs9claen2FKfLAQAAAE47whTKRePqlXVlp1h7+6kZCfZsMAAAAMCTEKZQbkb0bqogf18t3nZIc9fvd7ocAAAA4LQiTKHc1IgI1g3xDeztcbMSlGcWUQEAAAAegjCFcnVrz0aKCAnQhqSjmrZsl9PlAAAAAKcNYQrlygSp23s1srefn71BmTl5TpcEAAAAnBaEKZS7Id3qq1ZEsPYcztR7C7Y7XQ4AAABwWhCmUO6CA/x0V9+m9vb47zfpcEaO0yUBAAAAp4wwBZe4rH0dNY2pbIPU6z9udrocAAAA4JQRpuASfr4+eqB/c3t74s9bte9wptMlAQAAAKeEMAWX6d2iujrWq6LMnHy9OHeD0+UAAAAAp4QwBZfx8fHRyHOPj059vGSXNu0/6nRJAAAAQJkRpuBSHetHqU+LGHuA7zOzEp0uBwAAACgzwhRc7oEBzeTrI81cu0/LdhxyuhwAAACgTAhTcLmmMWG6vEMde/upGQkqKChwuiQAAACg1AhTcMRdfZoq0N9Xi7am6IfEZKfLAQAAAEqNMAVH1IoM0fVn1re3x85MsGuoAAAAgIqEMAXH3HZ2I4UH+ythX5q+WLHb6XIAAACAUiFMwTGRoYG67ezG9vaz325QVm6e0yUBAAAAJUaYgqOu715fNcKDtTs1Q+//usPpcgAAAIASI0zBUcEBfrq7bxN7e/x3G3UkM8fpkgAAAIASIUzBcZe1r6NG1SrpUHqO3vhxi9PlAAAAACVCmILj/P189cCA5vb22/O3av+RTKdLAgAAAP4WYQpuoV/LGLWvG6mMnDy9OHej0+UAAAAAf4swBbfg4+OjB38bnZqyeKe2JB91uiQAAADgLxGm4Da6NKyq3s2r2wN8zVbpAAAAgDsjTMGt3D+gmXx8pG9W79XKnalOlwMAAAD8KcIU3ErzGuG6tF0de/upGQkqKChwuiQAAADgDxGm4Hbu6ddUgf6+WrDloOZtPOB0OQAAAMAfIkzB7dSODNGQrvWKRqfy8xmdAgAAgPshTMEt3d6rscKC/LV+7xF9tWqP0+UAAAAAJyFMwS1VqRSoW89uZG8/822isnPznS4JAAAAKIYwBbd1fff6qh4WpJ0pGfpg4XanywEAAACKIUzBbYUG+uuuPk3t7Ze+26S0zBynSwIAAACKEKbg1q7oWEcNoysp5Vi23vxpq9PlAAAAAEUIU3Br/n6+ur9/M3v7rZ+2KDkty+mSAAAAAIswBbc3oFUNtYmNVHp2nl7+bqPT5QAAAAAWYQpuz8fHRyMHNLe3P1i4Q9sOHHO6JAAAAIAwhYqhW6OqOrtZNeXmF+jZ2RucLgcAAAAgTKHieKB/c/n4SF+t3KPVuw47XQ4AAAC8HGEKFUbLWuEa2La2vT1uVoLT5QAAAMDLEaZQodzTt6kC/Xz108YD+mljstPlAAAAwIsRplChxEaF6pqude3tsTMTlJ9f4HRJAAAA8FKEKVQ4d/RqrMpB/lqz+4i+Wb3X6XIAAADgpQhTqHCqVg7SLT0a2tvPfJuo7Nx8p0sCAACAF3I0TGVlZemhhx5Sx44dFR8fr4kTJ/7pc3/44QddfPHFateunS688ELNnTu32ONff/21+vTpozZt2uj2229XSkqKCz4BnHLjWQ0UXTlI2w+ma8riHU6XAwAAAC/kaJgaN26c1qxZo0mTJunRRx/V+PHjNXPmzJOel5CQoDvuuEOXXXaZPv/8cw0ePFgjRoyw141Vq1bp4Ycfts/56KOPdOTIEY0aNcqBTwRXCQ3014g+Teztl+Zu1LGsXKdLAgAAgJdxLEylp6dr6tSpNgTFxcWpb9++GjZsmCZPnnzSc82oU9euXTVkyBDVq1dP11xzjbp06aIZM2bYx99//32de+65GjhwoJo3b25D2o8//qidO3c68MngKoM7xap+1VAdOJqtt37a6nQ5AAAA8DKOhSkzqpSbm2un7RXq0KGDVq5cqfz84mtgLrnkEt13330nvUdaWpr9al5jpgoWqlmzpmrVqmWvw3MF+Pnqvv7N7O035m3WwaNZTpcEAAAAL+Lv1DdOTk5WlSpVFBgYWHQtOjrarqNKTU1VVFRU0fVGjRoVe+3GjRu1YMECO93P2L9/v6pXr17sOVWrVtW+fftKVVNeXp7cQWEd7lKPO+vforpa1w7X6t1H7HS/Ry5o4XRJFQ79Da5Gn4Mr0d/gavS5iq80PzvHwlRGRkaxIGUU3s/Ozv7T15mNJYYPH6727durd+/e9lpmZuYfvtdfvc8fWb16tdyJu9Xjri5t5KfVu6XJv25X58hjqlHZsW5dodHf4Gr0ObgS/Q2uRp/zDo79qzMoKOiksFN4Pzg4+A9fc+DAAV1//fUqKCjQSy+9JF9f3798r5CQkFLV1Lp1a/n5+ckd0rD5A+gu9bi7tpK+27NYP206qG92+uulwW3k4+PjdFkVBv0NrkafgyvR3+Bq9DnP+Rm6dZiKiYnRoUOH7Lopf3//oql/JkiFh4ef9PykpCS7AYXx7rvvFpsGaN7LBK0TmfvVqlUrVU2mw7tTp3e3etzZg+e20Pzx8zV9zT5V+SpQj13cSr6+BKrSoL/B1ehzcCX6G1yNPucdHNuAokWLFjZErVixouja0qVLbYovHHE6cec/s9OfuW527jPh6UTmbCnz2kJ79+61zVyHd2hVO0JPXtJaZkBq8sIdunfqSuXmcZgvAAAAPDBMmSl4Zivz0aNH23Oi5syZYw/tLRx9MqNUZi2U8frrr2vHjh0aO3Zs0WOmFe7md9VVV+mLL76wW62bXQIfeOABnX322YqNjXXq48EBgzvX1QtXtpWfr48+W75bt3+wTFm5LP4EAACABx7aaw7WNWdMDR06VGPGjLEbS/Tr188+Fh8fr+nTp9vbs2bNssFq0KBB9nphe+KJJ+zjZnv1//u//9Mrr7xig1VERISefPJJJz8aHHJx29qa8I8OCvTz1ay1SRo2aYkysglUAAAAOP0c3fbMjE6Z0abCEacTJSYmFt2eOXPm377XpZdeahvQt2WMJl7XSTe9u0Q/bTygIRMX6u3rOik8OMDp0gAAAOBBHB2ZAspLfJNovT+ss8KC/bV42yFd8+ZCHTpWuq3yAQAAgL9CmILH6lAvSh/e1FVRlQK1evdhXfnGAu0/cnwdHgAAAHCqCFPw+F3+Pr6lq2LCg7Qh6agGvb5Auw6lO10WAAAAPABhCh6vcfUwTb3lTMVGhWj7wXQNmrBAW5KPOl0WAAAAKjjCFLxC3aqhNlA1qlZJew9n6orXF2j93iNOlwUAAIAKjDAFr1EjIlgf39JNLWuG68DRbF35+gIt33HI6bIAAABQQRGm4FWqVg7Shzd3Vfu6kTqSmat/vLVQCzYfdLosAAAAVECEKXidiJAAvXdjF3VvXFXHsvN03X8X6fuE/U6XBQAAgAqGMAWvVCnIX28P7aQ+LaorKzffHvD7zaq9TpcFAACACoQwBa8VHOCn1/7RQRe2qaXc/AIN/3CZPl6y0+myAAAAUEEQpuDVAvx89cKVbTW4U6zyC6QHPlmld37e6nRZAAAAqAAIU/B6fr4+evLS1roxvoG9P/qrdXrl+01OlwUAAAA3R5gCJPn4+Ohf57fQiN5N7P2nZyVq7MwEFRQUOF0aAAAA3BRhCjghUN3dt6kePq+Fvf/aD5v16JdrlW/m/wEAAAC/Q5gCfuemHg31n0tay8dHenfBdt3/ySrl5uU7XRYAAADcDGEK+ANXd6mr569oa9dTfbpsl4Z/uFzZuQQqAAAA/A9hCvgTA9vV1qvXtFegn69mrNlnz6LKyM5zuiwAAAC4CcIU8Bf6x9XQ29d1VEiAn37ckKyh/12ktMwcp8sCAACAGyBMAX/jrCbV9O6NnRUW5K9FW1P0j7cWKjU92+myAAAA4DDCFFACnepH6YObuqpKaIBW7jqsK1//VfvTMp0uCwAAAA4iTAEl1LpOhD66pZuqhwUpMSnNBqrdqRlOlwUAAACHEKaAUmgaE6apt3ZT7cgQbT1wTFdMWGC/AgAAwPsQpoBSqle1kj65rZsaVqtkR6YGTVighH1HnC4LAAAALkaYAsqgZkSIPr6lm1rUDNeBo1ka/MavWrkz1emyAAAA4EKEKaCMoisHacpNXdWubqRS03N09Zu/6tctB50uCwAAAC5CmAJOQURogN6/sYvObFRVx7LzNHTiIv2QuN/psgAAAOAChCngFFUK8tfE6zqpd/PqysrN103vLtGM1XudLgsAAADljDAFnAbBAX6acG0HXXBGTeXkFej2D5bpk6W7nC4LAAAA5YgwBZwmAX6+enFwO13RsY7yC6T7pq7Uewu2OV0WAAAAyglhCjiN/Hx99NSlZ+j67vXt/X9/sVav/bDZ6bIAAABQDghTwGnm6+ujRy5oqeHnNLb3x85M0NOzElRQUOB0aQAAADiNCFNAOfDx8dG9/Zpp5LnN7f1Xvt+sMV+tU76Z/wcAAACPQJgCytGtPRvpsYGt7O13ftmmBz9dpTwCFQAAgEcgTAHl7Nqu9fTcFW3k6yNNXbpLd364XNm5+U6XBQAAgFNEmAJc4NL2dfTqNe0V4Oejb1bv1a3vL1VmTp7TZQEAAOAUEKYAFxnQqqbeGtpJwQG++i5hv67/72Idzcp1uiwAAACUEWEKcKGeTavp3Ru6qHKQvxZsOah/vLVQh9NznC4LAAAAZUCYAlysc4MofXBTF0WGBmjFzlRd+cYCJadlOV0WAAAASokwBTjgjDqR+ujmbqoWFqSEfWm68vUF2pOa4XRZAAAAKAXCFOCQZjXCNPWWbqodGaItB45p0IQF2nbgmNNlAQAAoIQIU4CD6kdX0tRbu6lhdCXtTs3QoNcXKHFfmtNlAQAAoAQIU4DDakWG6KNbuql5jTC7dsqsoVq1K9XpsgAAAPA3CFOAGzBrp6bc3FVtYiOVmp6jq99cqEVbU5wuCwAAAH+BMAW4icjQQE0e1kVdG0bZ86eGTFyoeRuSnS4LAAAAf4IwBbgRc/7UO9d31tnNqikzJ1/DJi3RzDX7nC4LAAAA7hamsrKy9NBDD6ljx46Kj4/XxIkT//Y1S5YsUe/evYtdKygo0Ntvv61zzjnHvteoUaN07Bi7oqFiCg7w0xvXdtR5rWsoOy9ft3+wTJ8t3+V0WQAAAHCnMDVu3DitWbNGkyZN0qOPPqrx48dr5syZf/r8xMREjRgxwoanE3300Uf2tffcc48+/PBDJSUl6d5773XBJwDKR6C/r14a3E6Xd6ijvPwC3fPxSr3/63anywIAAIA7hKn09HRNnTpVDz/8sOLi4tS3b18NGzZMkydP/sPnT5kyRYMHD1bVqlVPeuz999/X9ddfrwsuuEBNmjTRU089pR9++EFbtmxxwScByoe/n6/GXXaGhnarJ/P7g399vkav/7jZ6bIAAABwqmFq8+bNSks7fh7OTz/9pDFjxthwVFIJCQnKzc1Vu3btiq516NBBK1euVH5+/knPnzdvnsaOHavrrrvupMd27typNm3aFN2vXr26oqKitGLFijJ8MsB9+Pr6aPRFcbq9VyN7/8kZCXru28STRmcBAADgev5leZGZVvd///d/+u9//6vKlSvrtttuU9euXTV79mzt2bPHTsX7O8nJyapSpYoCAwOLrkVHR9t1VKmpqTYMnejVV1+1X6dNm3bSe5nRKjO178RRr8OHD+vQoUOl+lx5eXlyB4V1uEs9cN49fZooNMBPT3+7QS99t0lpmTl6+Lzm8vHxOeX3pr/B1ehzcCX6G1yNPlfxleZnV6Yw9dZbb9lRos6dO+uxxx5TixYt7LXFixfr7rvvLlGYysjIKBakjML72dnZparnvPPO0+uvv25HturUqWOn+Rk5OTmlep/Vq1fLnbhbPXBW1whpWLswvbU8Tf/9Zbt27t2vmzuEy+80BCqD/gZXo8/BlehvcDX6nHcoU5gyo0AmuBjff/+9rrzySnu7Ro0aJd5FLygo6KTQVHg/ODi4VPX885//tFP9zj//fPn7+9u1Vc2bN7ejZqXRunVr+fn5yR3SsPkD6C71wH20bSs1bbhbI6et1pytGQoOi9Azl5+hAL+yL3+kv8HV6HNwJfobXI0+5zk/w3ILUw0bNtRXX31lp+KZaX19+vSxo0Bma3MTYkoiJibGTsMz66ZMACqc+meCVHh4eKnqCQ0N1YsvvmjXcJlpTyZEdevWTbVr1y7V+5gO706d3t3qgXu4olNdVQ4O0Igpy/X1qn32PKrxV7e3W6qfCvobXI0+B1eiv8HV6HPeoUy/zn7wwQftuU7/+te/dPXVV6tRo0Z68skn7ZopsztfSZipgSZEnbhJxNKlS22K9/X1LfUW65999pnCwsJskFq1apUNVidubgF4kvNa19QbQzoqyN9Xc9bv142TFutYVq7TZQEAAHiVMoUpM+qzYMECLVy4UI888kjRVDsz5a9Vq1Yleo+QkBANHDhQo0ePtuFnzpw5dmRryJAhRaNUmZmZJXovs3ufOWfKvI85t+r+++/XVVddpcjIyLJ8PKBC6NWsuibd0FmVAv3086aDuvbthTqcUbp1ggAAACi7Mi+0mD9/vp2iZ3zyySd66KGH9Morr5Rq84hRo0bZM6aGDh1qt1YfPny4+vXrZx+Lj4/X9OnTS/Q+1157rc455xzddNNNtvXq1cuOngGermvDqpp8U1dFhARo2Y5UXfXGrzpwNMvpsgAAALyCT0EZDqwxocns3vfOO+/YrczN2U+DBg3SkiVL7A5/jz76qCraIjMz3bBt27ZuMbfV3eqB+0vYd0T/eGuRDVKNqlXS+8O6qGZESIleS3+Dq9Hn4Er0N7gafc67foZlGpn6+OOP9fLLL9uDcr/44gt16tTJjiyZLclLOpoE4PRpXiNcH9/SVbUigrU5+ZgGTVigHQfTnS4LAADAo5UpTJkDcc2OfmZQ64cffrDT6gyz+QMHlAHOaFitsj6+tZvqVw3VrkMZunzCL9qYlOZ0WQAAAB6rTGHKbH9udvMzmz6kpKSob9++9uyp5557zg6HAXBGnSqh+viWbmoWE6b9aVm68o1ftWb3YafLAgAA8EhlClNmBz6zPmrSpEm655577HlOZg3V7t27K9x6KcDTVA8P1pSbu6pNnQilHMu2m1Is2ZbidFkAAAAex7+sI1NmrdSJzHbkgYGBp6suAKegSqVAuwnFjZOWaNHWFF379iK9OaSj4ptEO10aAACAxyjz1ujr1q3Tvffeq0suuUQXXXSRDVOLFi06vdUBKLOw4ABNur6zejatpoycPN3wzmJ9u3af02UBAAB4d5iaPXu2rrjiCrsBxaWXXmqbj4+PbrjhBnv4LgD3EBLopzeGdNC5rWooOy9ft01epi9W7Ha6LAAAAO+d5vfiiy/qvvvus+dLncicO2W2TO/Tp8/pqg/AKQry99PLV7XTA5+u0rRlu3XXRyuUnp2nqzrXdbo0AAAA7xuZ2rlzZ9F26Ccy17Zu3Xo66gJwGvn7+eqZy9vo2q71ZI7pHjVttd76aYvTZQEAAHhfmGrUqJHmzZt30vUff/zR7uwHwP34+vro/y6O0609G9n7j3+zXi/M2WCn6wIAAMBF0/yGDx9u28qVK9WmTRt7bcWKFZo1a5bGjRtXlrcE4AJmbePIc5srLNhfT89K1AtzNiotM0fn1iBQAQAAuGRkykzne/PNN5WVlaUPP/xQ06ZNs7/d/uCDD3TeeeeV5S0BuNDtvRrr0Qtb2ttvz9+mCUuPKDs33+myAAAAPH9kyujWrZttJzLhyqynio2NPR21AShH13dvoEqB/ho5bZXmbM3QwFd/0dOD2uiMOpFOlwYAAODZ50z9EXPOVL9+/U7nWwIoR1d0itVr17RXeJCvEpOOauArP+upGQnKzMlzujQAAADvClMAKp4+Larrxf7RuvCMmsovkCb8uFnnvfSTlm5Pcbo0AAAAt0aYAmBHpl64so3euLaDqocFaUvyMV0+YYH+76t1Ss/Odbo8AAAAt0SYAlCkX1wNzb67pwZ1qGPPo5r481YNeOEnLdh80OnSAAAAKu4GFIsXL/7b5yQmJp5qPQAcFhEaYDeiuKBNLY36dJV2pKTrqjd/1T+61tXIc1uoclCZ960BAADwKCX+V9G1115b4nNsAFR8PZtW06y7e2jszAS9/+sO275PSNZ/Lm1tHwMAAPB2JQ5TCQkJ5VsJALcTFhygxwe21nmta2rkp6vtKNXQiYt0Rcc6evj8looICXC6RAAAAMewZgrA3zqzUbRm3nWWbujeQGbw+eMlu9T3uR81e12S06UBAAA4hjAFoERCA/31yIUtNfWWbmoYXUn707J007tLNGLKcqUcy3a6PAAAAJcjTAEolY71ozR9xFm6tWcj+fpIX6zYY0epvlm11+nSAAAAXIowBaDUggP8NPLc5vrsn93VLCZMB49l6/YPlum295dqf1qm0+UBAAC4BGEKQJm1iY3Ul8O7687eTeTv66MZa/ap3/Pz9NnyXSowB1UBAAB4MMIUgFMS5O+ne/o21Zd3xCuuVrhS03N090crdeOkJdp7OMPp8gAAAMoNYQrAadGyVrg+v7277u/fTIF+vvouYb/6PTdPHy3ewSgVAADwSIQpAKdNgJ+vbu/VWN/cGa+2sZFKy8rVg5+u1rVvL9LOlHSnywMAADitCFMATrsmMWH69LYz9a/zWyjI31fzNx1Q/xfm6d0F25SfzygVAADwDIQpAOXCz9dHw85qqJl39VDnBlFKz87TI1+s1eA3f9W2A8ecLg8AAOCUEaYAlKsG0ZU05aau+r+L4xQa6KdFW1M04MV5euunLcpjlAoAAFRghCkA5c7X10dDutXXrLt6KL5xtDJz8vX4N+t12Wu/aGNSmtPlAQAAlAlhCoDLxEaF6r0bO2vsZa0VFuSvFTtTdf5L8/XK95uUk5fvdHkAAAClQpgC4FI+Pj66slNdfXtPD/VqVk3Zefl6elaiLnn1Z63bc8Tp8gAAAEqMMAXAETUjQjTxuk56/so2iggJ0JrdR3TR+Pl6bvYGZecySgUAANwfYQqAo6NUl7Sro9n39NCAuBrKzS/QS3M36sKX52vVrlSnywMAAPhLhCkAjqseFqzX/tFer1zdXlUrBSoxKU0DX/lZT81IUGZOntPlAQAA/CHCFAC3GaU6/4yamn1PT13UppbMrukTftys8176SUu3pzhdHgAAwEkIUwDcSlSlQL10VTu9cW0HVQ8L0pbkY7p8wgKN+Wqt0rNznS4PAACgCGEKgFvqF1dDs+/uqUEd6qigQPrvz9s04IWf9MvmA06XBgAAYBGmALitiNAAPT2ojSbd0Fm1IoK1IyVdV7+5UA9/tlppmTlOlwcAALwcYQqA2+vZtJpm3d1D/+ha196fvHCH+j8/Tz8k7ne6NAAA4MUIUwAqhLDgAD0+sLU+uKmL6kaFas/hTF3338W6b+pKHU5nlAoAALgeYQpAhXJmo2jNvOss3dC9gXx8pE+W7lLf53/U7HVJTpcGAAC8DGEKQIUTGuivRy5sqam3dFPD6Eran5alm95dojs/XK6UY9lOlwcAALyEo2EqKytLDz30kDp27Kj4+HhNnDjxb1+zZMkS9e7du9i1goICvfzyy+rRo4c6deqku+66SykpnEsDeLqO9aM0fcRZurVnI/n6SF+u3KO+z/2or1ftsX8vAAAAeGyYGjdunNasWaNJkybp0Ucf1fjx4zVz5sw/fX5iYqJGjBhx0j+SPvroI33yySd65plnNHnyZO3fv18PP/ywCz4BAKcFB/hp5LnN9dk/u6tZTJgOHsvWHR8s123vL9P+tEynywMAAB7MsTCVnp6uqVOn2tATFxenvn37atiwYTYM/ZEpU6Zo8ODBqlq16kmP/fjjjzrvvPPUuXNnNW3a1L7Pr7/+6oJPAcBdtImN1FfD43Vn7yby9/XRzLX71Pe5eZq2bBejVAAAwLPCVEJCgnJzc9WuXbuiax06dNDKlSuVn59/0vPnzZunsWPH6rrrrjvpscjISP3www9KSkpSZmamvvnmG7Vo0aLcPwMA9xLo76t7+jbVl3fEK65WuA5n5Oiej1fqhncWa+/hDKfLAwAAHsbfqW+cnJysKlWqKDAwsOhadHS0XUeVmpqqqKioYs9/9dVX7ddp06ad9F633367brvtNrtmys/PT9WqVbNT/0orLy9P7qCwDnepB57NE/tbs5hK+vTWrnpr/la9NHeTvk9MtqNUD53bTFd0rCMfsw0gHOOJfQ7ui/4GV6PPVXyl+dk5FqYyMjKKBSmj8H52dul249q9e7eCg4M1YcIEhYeH27VYZmOLkmxocaLVq1fLnbhbPfBsntjfukVIdfpU1SuLD2tjSo4e+nytPvxlo27rGK6YSo799QcP7nNwX/Q3uBp9zjs49q+JoKCgk0JT4X0TjErKrIV48MEH9cADD6hXr1722gsvvGBvmymDbdq0KfF7tW7d2o5suUMaNn8A3aUeeDZP729tJZ0XX6B3ftmmZ2dv1Or92bpvziHd36+p/tGlrnzNNoBwKU/vc3Av9De4Gn3Oc36Gbh2mYmJidOjQIbtuyt/fv2jqnwlSZnSppMwW6Hv37lWzZs2KrtWsWdNOITQjVqUJU6bDu1Ond7d64Nk8ub+Zj3Vzz8bqG1dTD366Sou2pmjM1+s1Y02Sxl5+hhpEV3K6RK/kyX0O7of+Blejz3kHxzagMBtEmBC1YsWKomtLly61Kd7Xt+RlRURE2OmBmzdvLhawzLqrOnXqnPa6AVRcJjRNuamr/u/iOIUG+mnRthQNeGGe3py3RXn57PgHAAAqSJgKCQnRwIEDNXr0aK1atUpz5syxa5yGDBlSNEpldub7OyaQXXrppXanv8WLF2vDhg26//777YiUCWYAcCIzrW9It/qadVcPxTeOVlZuvp6Yvl6XvfaLNialOV0eAACoQBw9tHfUqFH2jKmhQ4dqzJgxGj58uPr162cfi4+P1/Tp00v0PmazCfO6e++9V9dee62dJmh2/2PHLgB/JjYqVO/d2FljL2utsCB/rdiZqvNfmq/x321UTt7JxzMAAAD8nk8Bp1naRWZmumHbtm3dYm6ru9UDz0Z/kz2D6qFpq+0W6oY5o2rc5WcorlaE06V5JPocXIn+Blejz3nXz9DRkSkAcAc1I0I08bpOev7KNooICdDaPUd08fif9dy3icrK5ZwQAADwxwhTAGCG6X18dEm7Opp9Tw8NiKuh3PwCvfTdJl348nw7BRAAAOD3CFMAcILqYcF67R/t9crV7VW1UqA2JB3Vpa/+rCenr1dmDqNUAADgfwhTAPAHo1Tnn1FTs+/pqYvb1pLZNf31eVt03os/acm2FKfLAwAAboIwBQB/IqpSoF4c3E5vDumo6mFB2nLgmAa9vkCjv1yr9Oxcp8sDAAAOI0wBwN/o2zJGs+/uqUEd6sjsf/rOL9vU/4V5+mXTAadLAwAADiJMAUAJRIQG6OlBbTTphs6qFRGsnSkZuvqthbrt/aVav/eI0+UBAAAHEKYAoBR6Nq2mWXf30DVd6tr7M9bs07kv/kSoAgDACxGmAKCUwoID9MQlrTXrrh52owofn/+FqlvfW6p1ewhVAAB4A8IUAJRRsxphdgv1mSP+F6pmrt2n814iVAEA4A0IUwBwighVAAB4J8IUAJwmhCoAALwLYQoAyilUmTVVFxCqAADwWIQpACgnTWPCNP5PQtUt7y3R2j2HnS4RAACcAsIUADgQqmatTdL5L80nVAEAUIERpgDARQhVAAB4FsIUALgYoQoAAM9AmAIAh0PVt3f10IVtahGqAACoYAhTAOCwJjFhevmqdn8Yqm5+l1AFAIC7IkwBgBuHqm/XEaoAAHBXhCkAqGChas1uQhUAAO6AMAUAFSBUXXRCqLrgZUIVAADugDAFABUgVL10VTvNvptQBQCAOyFMAUAF0bj6n4eqmwhVAAC4HGEKADwgVM0mVAEA4HKEKQCooAhVAAA4izAFAB4Uqi5uS6gCAMBVCFMA4EGh6sXBhCoAAFyFMAUAXhSqhk0iVAEAcLoQpgDA40NVz6JQNWc9oQoAgNOFMAUAHq5x9cpFoWpg21ryJVQBAHBaEKYAwItC1QuD2+lbQhUAAKcFYQoAvMxfh6rFWr2LUAUAQEkQpgDAS/1xqNqvC8cTqgAAKAnCFAB4OUIVAABlQ5gCABQLVbPv6alL2tUmVAEA8DcIUwCAYhpVq6znr2xLqAIA4G8QpgAApQ5VN76zWKt2pTpdIgAAjiJMAQBKHarmJuzXReN/JlQBALwaYQoAUCKEKgAAiiNMAQDKFKrm3NNTlxKqAABejDAFACiThtUq6zlCFQDAixGmAADlFqpueGexVu4kVAEAPBNhCgBQbqHqu4T9uvgVQhUAwDMRpgAA5Req2p8cqlZxThUAwEMQpgAA5ReqrmirufeeXSxUXfLaAo3+MUVfrdqrrNw8p8sEAKBihqmsrCw99NBD6tixo+Lj4zVx4sS/fc2SJUvUu3fvYteaNWv2h+3zzz8vx+oBACXRILrSSaFq9f5s3fXRSnX9z1w9/vU6bdp/1OkyAQAoNX85aNy4cVqzZo0mTZqkPXv26MEHH1StWrU0YMCAP3x+YmKiRowYoaCgoGLX58+fX+z+O++8oxkzZpwUugAAzoequ85prJe+WaL5u3O170iW3pq/1bbO9aM0uHOszmtdU8EBfk6XCwCA+4ap9PR0TZ06VW+++abi4uJs27hxoyZPnvyHYWrKlCkaO3asYmNjdfRo8d9gVqtWrej2zp079d5772nChAkKCwtzyWcBAJRc7SohuqpVmJ646gz9tClFUxbvsNP/Fm1LsW30l2t1afs6Nlg1rxHudLkAALhfmEpISFBubq7atWtXdK1Dhw42BOXn58vXt/gMxHnz5tkwZYLU+PHj//R9X3rpJXXr1k1nnnlmqWvKy3OPufuFdbhLPfBs9De4WmFf81GBejWLtm3v4Ux9snSXpi7dpd2pmXrnl222tYuN0JUdY3X+GTUUGujoZApUUPwdB1ejz1V8pfnZOfZ/puTkZFWpUkWBgYFF16Kjo+06qtTUVEVFRRV7/quvvmq/Tps27U/f00wV/Prrr+0oVlmsXr1a7sTd6oFno7/B6T53VpR0Zp8IrUoK0ewt6VqyJ0vLdx62bcxXa3VW3WD1bRiqhlUCHKsZFRd/x8HV6HPewbEwlZGRUSxIGYX3s7Ozy/Sen3zyiVq1aqU2bdqU6fWtW7eWn5+fW6Rh8wfQXeqBZ6O/wd36XAdJ15tfuqVl6dNlu/XRkl3akZKub7dk2NaqVriu7FRHF55RS2HBjFbhr/F3HFyNPuc5P8OScOz/QmYTid+HpsL7wcHBZXrPWbNmafDgwWWuyXR4d+r07lYPPBv9De7W52pEhur2c5rotrMb69ctB/XBoh2atXaf1uw5ojVfrNOTMxJtoDJrq9rGRsrHx8el9aNi4e84uBp9zjs4FqZiYmJ06NAhu27K39+/aOqfCVLh4aVfcLx3715t2rSJHfwAwMP4+vrozMbRth08mqVpy3brw8U7tCX5mD5astO25jXCdFXnuhrYrrYiQpgGCADw8HOmWrRoYUPUihUriq4tXbrUDon+fvOJkli5cqVq1qxpt1YHAHimqpWDdFOPhpp7T099fEs3XdKutgL9fZWwL02PfrlWXf4zR/d8vEKLt6WooKDA6XIBAB7OsTAVEhKigQMHavTo0Vq1apXmzJljD+0dMmRI0ShVZmZmid/PbKveqFGjcqwYAOAuzJS+zg2i9PyVbbX4oT4afWFLNYsJU2ZOvh25GjRhgfo+P09v/bRFh46VbR0uAABuG6aMUaNG2fOlhg4dqjFjxmj48OHq16+ffSw+Pl7Tp08v8XsdOHBAERER5VgtAMAdRYQG6LruDTTzrrM07Z9n6oqOdRQS4KdN+4/q8W/Wq8t/5urOD5frl80HGK0CAJxWPgX8n8Xu2GGmG7Zt29YtFgq6Wz3wbPQ3eGKfO5KZoy9X7NGHi3Zo7Z4jRdfrVw3V4M51dXmHOoquHFQu3xvuhb/j4Gr0Oe/6GbKnLADA44QHB+gfXevZtnrXYbthxRfLd2vbwXQ9NSNBz8xKVL+4GA3uVFfxjaPtJhcAAJQWYQoA4NFa14lQ6zqt9fB5LfT1KjNatVMrdqZq+up9tsVGhejKjrEa1DFWMeFlO5oDAOCdCFMAAK9QKchfV3aqa9v6vUc0ZdEOTVu+WztTMvTMtxv0/JyNOqd5dV3VOVY9m1aXH6NVAIC/QZgCAHidFjXDNebiVhp5bgtNX71XUxbv0OJthzR7XZJtNSOCdUXHWF3RKVa1I0OcLhcA4KYIUwAArxUS6KfLOtSxbWNSmqYs3qlPl+3S3sOZenHuRr303Uad3bSa3bTCjFoF+Dm6CS4AwM0QpgAAkNQkJkz/vqCl7u/fTLPW7tOURTu1YMtBfZ+YbFu1sCC77brZtCI2KtTpcgEAboAwBQDACYID/HRx29q2bT1wzE4B/HTpLiWnZemV7zfbdlaTaBuq+raMUaA/o1UA4K0IUwAA/IkG0ZU06twWurdvM81Zn2TPrZq/6YB+2ni8Va0UaM+surJTrBpWq+x0uQAAFyNMAQDwN8zo03mta9q2MyVdHy3eqY+X7NT+tCy9Pm+LbV0aROnqLnXVP66GHd0CAHg+whQAAKVg1kvd17+Z7urTRN8l7LebVvyQuF8Lt6bYFhkaoEvb1bFbrJt1WAAAz0WYAgCgDPz9fNUvroZte1Iz7EjVx4t3as/hTE38eattHepV0VWd6+r81jXtzoEAAM9CmAIA4BTVigzRXX2aavg5TTRvY7I+XLhDcxP2a+n2Q7aN+WqtLmlX225a0bJWuNPlAgBOE8IUAACniZ+vj3o1q27b/iOZmrp0l90NcGdKht5dsN22NnUi7GjVhW1qqVIQ/xsGgIqMv8UBACgH1cODdXuvxrqtZyP9svmg3Qnw23X7tHLXYa3ctVqPfb1OF7WtZYNV69oR8vHxcbpkAEApEaYAAChHvr4+im8SbduBo1matmyXPly0055hZb6a1rJmuK7qUlcXt62l8OAAp0sGAJQQJw0CAOAi0ZWDdHOPRvru3p6acnNXDWxby267vm7vEf378zXq8sRc3T91pV1nVVBQ4HS5AIC/wcgUAAAuZqb0dW1Y1bbR6dmatmy3nQa4cf9Ru87KtKYxle0UQLNxRWRooNMlAwD+AGEKAAAHmaB0Q3wDXd+9vpbtOGSn/X29ao82JB3VmK/W6ckZCTqvVQ0brDo3iGJtFQC4EcIUAABuwISkDvWibPv3BS315Yrd+mDRTq3fe0Sfr9hjW72qoRrQqobObVXT7gpIsAIAZxGmAABwMxEhAbq2W339o2s9rdp12G6v/uWKPdp+MF2v/7jFtpoRweofV8OGq071o+y27AAA1yJMAQDgpszIU5vYSNv+dX5L/ZCYrBlr9ur7hP3aezhT7/yyzbaqlQLVLy7GhqszG0XbTS0AAOWPMAUAQAVgDvg9/4yatmXm5Gn+xgOauXafZq9L0sFj2UXbrIcF+6tPi+PBqmfTagoJ9HO6dADwWIQpAAAqmOAAP/VpGWNbTl6+Fm5J0cy1ezVrbZKS07L02fLdtoUE+OnsZtXsVMBzmldXGGdYAcBpRZgCAKACC/DzLToU+P8uamV3BJy5Zp9mrNmn3akZ9qtpgX6+6t64qt28woSwqEpstw4Ap4owBQCAh/D19VHH+lG2PXx+C63dc8SusTJhakvyMX2fmGyb32c+6tIgyo5Y9WtZQzUigp0uHQAqJMIUAAAeunlFq9oRtt3fv7k27U/TjNX77DorE7J+2XzQtke+WKv2dSNtsBoQV1N1q4Y6XToAVBiEKQAAvEDj6mEa3tu0JtpxMF2z1h4PVku3H9KyHam2/Wd6glrWDP/tLKsaaly9MmdZAcBfIEwBAOBlzOjTTT0a2pZ0JFPfrj2+rmrh1hSt23vEtudmb1DDapVsqDIjVq1qhxOsAOB3CFMAAHixmPBge0CwaSnHsjVnfZLdwMJsvW7WWb3y/WbbakeGHJ8K2KqG2tetwiHBAECYAgAAhcwOf1d0jLUtLTNH3yXst9MBv09ItjsDvj1/q23VwoLUr2WMDVZdG1a1OwoCgDciTAEAgJOYM6kublvbNnNI8I8bkjVrzT7NXn/8LKvJC3fYFhESYA8JNsHqrCbR9gwsAPAWhCkAAPCXTEDqH1fDtuzcfC3YctBOBZy9bp8OHM3Wp8t22VYp0E9nN6+uAXE11Kt5dVUO4p8ZADwbf8sBAIASC/T3Vc+m1Wx7fGArLdmWYjevMNMB9x7O1Der9tpmntejSbQGmEOCW1RXZCiHBAPwPIQpAABQJmYTii4Nq9r26IUttWrXYRusZq7Zq20H0zVn/X7bzPO6Nax6/JDguBhVD+OQYACegTAFAABOmdk2vU1spG0PDmimDUlHNWPNXjsdMGFfmuZvOmDbv79Yow51q9hgZaYNxkZxSDCAioswBQAATnuwalYjzLa7+jTV1gPH7DRAM2q1cmeqlmw/ZNvj36xX69oRRVuuN6pW2enSAaBUCFMAAKBcNYiupFt7NrJtT2pG0SHBi7elaPXuw7Y9PStRTapXLgpWLWtySDAA90eYAgAALlMrMkTXdW9g24GjWZq97vghwb9sPqCN+49q43eb9PJ3mxQbFWJ3BTQbWLSLjZQvhwQDcEOEKQAA4IjoykG6qnNd2w5nmEOCjwcrc6bVzpQMvfnTVttiwoPs+ioTrjo3iJI/hwQDcBOEKQAA4Dhz+O8l7erYlp6dqx8TkzVz7T7NXb9fSUey9O6C7bZVCQ1Q35bHDwnu3jhaQf4cEgzAOYQpAADgVkID/XVu65q2ZeXm6ZdNB+3OgGZK4KH0HH28ZJdt5lDgc8whwa1q6Oxm1ezrAMCV+FsHAAC4LTPy1Kt5ddty8/K1aFuKnQpodgc0I1ZfrtxjW9Bvhwmf27qGzmkeY0e6AKC8EaYAAECFYNZKndko2rbRF8Zp+c7U37Zc32vXWH27Lsk2f18fndk4Wue2qqFzmkU7XTYAD0aYAgAAFY7Z3a9DvSq2jTq3udbtPaJZa45vuW52BZy3Idk2swlgs6oBOvfQZvVoVt2ea+XHzoAAThPCFAAAqNDMeVRxtSJsu6dfM23af9SOWJnpgOYMq/UHcrR+zkY9N2ejwoP91a1RVcU3jlZ8k2qqXzWU86wAVMwwlZWVpTFjxujbb79VcHCwbrjhBtv+ypIlS/Tggw9q7ty5xa7PnDlTzz//vJKSktS+fXs99thjql27djl/AgAA4G4aV6+sxtUb6/ZejbXjwFG9N3e5tmeFaMGWgzqSmatZa5NsM2pHhqh746p2Z0DTzHbtAFAhwtS4ceO0Zs0aTZo0SXv27LEhqVatWhowYMAfPj8xMVEjRoxQUFDxv+iWLVume++9V//+97/VuXNn+7733HOPPvroIxd9EgAA4I5qVwnRgMahatu2rQrkozV7jmj+xmTN33RAS7cf0u7UjKLdAY0WNcMV/1u46tKgqkIC2XodgBuGqfT0dE2dOlVvvvmm4uLibNu4caMmT578h2FqypQpGjt2rGJjY3X06NFij02cOFEXXXSRBg8ebO8//PDDGjp0qFJSUhQVFeWyzwQAANx7A4u2sZG23XFOE3ue1eJth34LVwe1fu+RomYOCw7081X7epF2SqAJV2fUiWS9FQD3CFMJCQnKzc1Vu3btiq516NBBEyZMUH5+vnx9i59uPm/ePBumTJAaP358sccWLVqkp556qui+CVzfffedCz4FAACoqMy5VGY7ddOMA0ez9Mvmg8fD1cYD2nM4U79uSbHtmW83KCzYX2f+tt7KhKsG0ZVYbwV4OcfCVHJysqpUqaLAwMCia9HR0XYdVWpq6kkjSq+++qr9Om3atGLXjxw5osOHDysvL0833nijDWlnnHGGRo8erZiYmFLVZN7DHRTW4S71wLPR3+Bq9Dm4a3+rEuKv81vF2FZQUKBtB9OPh6tNB+16q7TfrbeqFRms7o2q2oBlGuutYPB3XMVXmp+dY2EqIyOjWJAyCu9nZ2eXarqg8fjjj+vuu++2a6pefPFF3XLLLTZ4/X6E66+sXr1a7sTd6oFno7/B1ehzqAj9LS5Iiovz0bCWVbXlUI5WJWXblnAwW3tSMzV16W7bjPoR/jojJlBnxASpRXSAgv1L/m8QeB7+jvMOjoUps4nE70NT4X2zs19J+fkdXxg6aNAgDRw40N5+5pln1L17d61YscLu7FdSrVu3Lno/p9Ow+QPoLvXAs9Hf4Gr0OVTU/tbB/Hvjt9tmvdWS7Yf086aDdvRq3d40bTuca9uXG9IV4Oej9nUj1b2RmRJYVa1qhds1W/B8/B3nOT9Dtw5TZgreoUOH7Lopf3//oql/JkiFh4eX+H3MVMGAgAA1bNiw2LXIyEjt27evVDWZDu9Ond7d6oFno7/B1ehzqMj9LSzET72a17DtxPVWP288YHcKNLsELtx6yDZzvpVZb9WtYVWd1YT1Vt6Cv+O8g2NhqkWLFjZEmdGjjh072mtLly61Kb40U/PMe5idAM1aqfPOO89eM7v4maDGOVMAAMAVzHqpi9rUsq1wvZUJVSZc/bL5gD3f6tt1SbYZtSKCbaiKbxKtMxtFq1oY662AisixMBUSEmKn5ZmNIv7zn/9o//79dovzJ598smiUKiwsrERT/q6//nqNGjXKBrSmTZvq6aeftrfNRhQAAACuZEaczMiTadd2rae8/AKt3n1YP286YHcJNOdbmZ0Cpy7dZZvRvEaY3SXQhKvODaLsToMA3J+jf1JNADJhypwJVblyZQ0fPlz9+vWzj8XHx9tgdemll/7t+5hzqcyufiZEHTx40B7ca3b/Y/gcAAA4zZxNVXi+1e29GisjO0+Lt6XYkSsTrtbtPaKEfWm2vTV/62/rraoUhavWtSNYbwW4KZ8CMxbt5cwiMzPd0JyO7g5zW92tHng2+htcjT4HV6oI/e1g0flW/1tvdaLC9VYmWJmAxXor91YR+hxO38+QMWQAAAAHVa0cpAvb1LLN/I57e+F6q98a660A90WYAgAAcBNmxKl+dCXb/vHbeqs1uw8XTQn8q/VW3ZtEqwvrrQCX4k8bAACAG6+3ahMbaduJ663sZhabDmjtnj9fb2XC1RmstwLKFWEKAACggggJ9FOPptVsO3G9lQlXP20sPN8qxbZnZ28ott7KTA1syHor4LQiTAEAAHjgeisTsg5n5LDeCihHhCkAAAAPX29lwtWSbay3Ak43/sQAAAB4oLKst2pXt4rOYr0VUGKEKQAAAC9db7Vgy/HzrQrXWy3ammKbXW8V5K/29arYDS061KuiNrERCgsOcPpjAG6FMAUAAOCl660uOKOWbWa91Y6U4+utTLgqXG/144Zk2wyzb0WzmDAbsDrUrWK/1q8ayoYW8GqEKQAAAC9nAlG9qpVsu6bL8fVW6/cesedambZsxyHtOpRRNC3wg4U77OuiKgWqfd1IOz3Qjl7VibQjYIC3IEwBAADgpPVWrWpH2Db0zPr22v4jmTZULduRagPW6t2HlXIsW3PW77et8HUta4bbgFU4RbBOlRBGr+CxCFMAAAD4W9XDgzWgVU3bjKzcPLuJxbLfRq6WbU/VviOZNmSZNmnBdvs8s/368WmBkXb0Kq5WhIIDGL2CZyBMAQAAoNSC/P3syJNphfakZhRNCzQhy4St5LQszVy7zzYj0M9XcbXN6NXxqYHma42IYAc/CVB2hCkAAACcFrUiQ2wzhwgbmTl5WrXrcFG4Ml8PHM3W8h2ptr09f+vx10UEF9s5sGWtcAWwLTsqAMIUAAAAyoWZzte5QZRthtk1cGdKhpbuSLHTAs0oVsK+I/Yw4T2r9urrVXvt84L8fe1mFu3qRRaNfpnpgoC7IUwBAADAJcxGFHWrhtp2Sbs69tqxrFyt3JlatLmF+ZqanqNF21JsK1Q3KvS3aYHHN7cw27RzqDCcRpgCAACAYyoF+evMxtG2FY5ebTlwzI5aLd9xfGv2jfuP2nOwTPts+W77vNBAP7WN/W3kql6k2sVWUZVKgQ5/GngbwhQAAADcavSqUbXKtl3RMdZeO5KZoxW/bcluRq7M7bSsXHu4sGmFGlarVHSgsBnFalytsnx92ZYd5YcwBQAAALcWHhygHk2r2WaYQ4U37T/6v50DdxzSluRjRW3q0l32eWHB/nb0qnDXwLZ1I+17AacLYQoAAAAVijkcuFmNMNuu7lLXXjt0LFvLdx6fFmg2t1i5K1Vpmbn6aeMB2wxzdnDT6mG/7Rx4PGQ1iK7EocIoM8IUAAAAKjyzXuqc5jG2Gbl5+UrYl1a07spsbmHWXCUmpdn24aIdx18XGqB2v23J3q5upN1F0KzjAkqCngIAAACPY3b6a1U7wrZru9W318wBwieeeWXOwDqUnqPvEvbbVjjq1bxGWLFDhWOjQhi9wh8iTAEAAMArmLOq+sfVsM3Izs3Xur1HbLhauuOQlm8/ZM+8WrvniG3v/brdPi+6clDRtEAzRbB17Qh7hhZAmAIAAIBXCvT3tRtUmHaDGthrew9n2DVXy36bHrh2z2EdOJqlb9cl2WYE+PmoZa2I/wWsulVUKzLE4U8DJxCmAAAAgN/UjAjR+WeYVtPez8zJ05rdh4vClVl7ZaYLmoOGTfvvz9t+e13w8R0DYyMUmpGtJlm5Cg9l9MrTEaYAAACAP2Gm83WsH2Vb4aHCuw5lFK29MtMD1+9N097Dmfpm9V7bjH/9MMfuFBhXK0JxtcJ/axGK4mBhj0KYAgAAAErIbEQRGxVq28Vta9tr6dm5Wrnz+OjVsu0pWr7toFIy84vOvfpq5Z6i15sRLBOsWp4QsmpHssFFRUWYAgAAAE5BaKC/ujWqalteXp5WrFihOo1bKCHpmJ0iuM5uaHFY2w6m2xEs0+asP757oBEZGqCWNf83etWqdrgaRFe2OwvCvRGmAAAAgNPM7ADYMyJUPZtWK7qWlpljpwSaYFW4Y+DGpDSlpufol80HbSsUEuCn5jXDigKW+do0JoxdBN0MYQoAAABwgbDgAHVuEGVboazcPG1MOlosYK3fe0Tp2XlaviPVtkL+vj5qXL2yWp4QsMzt8OAAhz4RCFMAAACAQ4L8/YoOFy6Ul1+gbQdPnCJ4fJqgOWA4YV+abdOW7S56ft2o0GKbXJiv1cODHfpE3oUwBQAAALgRs1aqUbXKthVucmF2ETRrrQqDlflqgtbu1AztSEm3bcaafcUOKP59wDKhi40uTi/CFAAAAODmTAgyBwOb1rdlTNH1Q8eytW7v/wKWaVuSj9qzsH5ITLatUFiQv1r8LmCZaYMBfr4OfaqKjzAFAAAAVFBVKgWqe+No2wqZrdrNRhfrTghYifvSlJaVq0VbU2wrFOjvq2YxhRtdHN+yvUXNMLtDIf4e/5UAAAAAD2KCUId6VWwrlJOXr037zUYX/xvFWr/niA1Yq3cftq2Q2ZG9YbXKJ00TjAzlwOHfI0wBAAAAHs5M5WtRM9y2yzvUsdfy8wu081B6sYBlmpkiaIKXaV+s+N+Bw+Zw4Za/C1g1I4K9eh0WYQoAAADwQr6+PqpXtZJt57WuWXR9/5HiG12YZja4MJtdmDZ7XVLRc6MqBRYdOGyCltmVsEHVSva9vQFhCgAAAEARs626ab2aVy+6diQzp9g27eb2xv1HlXIsW/M3HbCtUGignx0BO3GaYJOYynYbeE9DmAIAAADwl8zBwF0bVrWtUGZOnjYkpRVfh/XbgcNLtx+y7cQDh5ucsNFF3G8bXZiDjCsywhQAAACAUgsO8NMZdSJtO/HAYbM1+++nCR7OyLFBy7RPlv7vPepXNQcOR9gpgj2bVit2eHFFQJgCAAAAcNoOHG4SE2bbwHb/O3DYrLUqDFaFW7abQ4i3HUy37ZvVe/XMt4n6ZeQ5qhkRooqCMAUAAACg3Pj4+KhOlVDb+sfVKLp+8GjWbwcOH2+Vg/wUXTlIFQlhCgAAAIDLVa0cpLOaVLOtovJ1ugAAAAAAqIgIUwAAAABQ0cJUVlaWHnroIXXs2FHx8fGaOHHi375myZIl6t2790nXzXs0a9asWDt27Fg5VQ4AAADA2zm6ZmrcuHFas2aNJk2apD179ujBBx9UrVq1NGDAgD98fmJiokaMGKGgoOIL05KSkpSWlqY5c+YoODi46HpoaGi5fwYAAAAA3smxMJWenq6pU6fqzTffVFxcnG0bN27U5MmT/zBMTZkyRWPHjlVsbKyOHj1a7LHNmzerWrVq9jEAAAAA8OhpfgkJCcrNzVW7du2KrnXo0EErV65Ufn7+Sc+fN2+eDVPXXXfdSY9t2rRJDRo0KPeaAQAAAMDxkank5GRVqVJFgYGBRdeio6PtOqrU1FRFRUUVe/6rr75qv06bNu2k9zIjUxkZGbr22mu1detWtWjRwq7FKm3AysvLkzsorMNd6oFno7/B1ehzcCX6G1yNPlfxleZn51iYMuHnxCBlFN7Pzs4u1Xtt2bJFhw8f1j333KPKlSvbqYNmBOubb76x90tq9erVcifuVg88G/0NrkafgyvR3+Bq9Dnv4FiYMptI/D40Fd4/cROJknj77beVk5OjSpUq2fvPPPOMevbsqe+//14XXnhhid+ndevW8vPzk9NMGjZ/AN2lHng2+htcjT4HV6K/wdXoc57zM3TrMBUTE6NDhw7ZdVP+/v5FU/9MkAoPDy/Ve5kRrRNHuUxQq1Onjt3lrzRMh3enTu9u9cCz0d/gavQ5uBL9Da5Gn/MOjm1AYdY1mRC1YsWKomtLly61Kd7Xt+RlFRQUqE+fPsXWUpmdArdv366GDRue9roBAAAAwNEwFRISooEDB2r06NFatWqVPSPKHNo7ZMiQolGqzMzMv30fHx8fnX322Xr55Ze1cOFCu736Aw88oBo1atipfgAAAADgUWHKGDVqlD1faujQoRozZoyGDx+ufv362cfi4+M1ffr0Er3P/fffr/79++vee+/VoEGD7NTBN954g6FVAAAAAOXGsTVThaNT5uwo034vMTHxD19z6aWX2nYis0Zq5MiRtgEAAACAx49MAQAAAEBFRZgCAAAAgDIgTAEAAABAGRCmAAAAAKCibUDhLsxZVYWnHbuDwjrcpR54NvobXI0+B1eiv8HV6HMVX+HPrjAj/BWfgpI8y8NlZ2dr9erVTpcBAAAAwE20bt1agYGBf/kcwpSk/Px8ezaVr6+vPQQYAAAAgHcqKCiw+cDf39/mg79CmAIAAACAMmADCgAAAAAoA8IUAAAAAJQBYQoAAAAAyoAwBQAAAABlQJgCAAAAgDIgTAEAAABAGRCmAAAAAKAMCFNuJCsrSw899JA6duyo+Ph4TZw40emS4OGSkpJ05513qnPnzjrrrLP05JNP2n4IlLebb75ZI0eOdLoMeLjs7GyNGTNGnTp10plnnqnnnnvOHsYJlJe9e/fqlltuUfv27XXOOefonXfecboklDP/8v4GKLlx48ZpzZo1mjRpkvbs2aMHH3xQtWrV0oABA5wuDR7I/IPCBKnw8HBNnjxZhw8ftmHenPRt+h5QXr755hv9+OOPuuSSS5wuBR7u8ccf18KFC/X222/r2LFjuvvuu+3/VwcPHux0afBQd911l+1j06ZN06ZNm3Tfffepdu3a6tu3r9OloZwwMuUm0tPTNXXqVD388MOKi4uzf+iGDRtm/5ELlIctW7ZoxYoVdjSqSZMmdkTUhKuvv/7a6dLgwVJTU+0vjlq3bu10KfCCvvbpp5/qscce0xlnnKFu3brphhtu0MqVK50uDR7K/FLS/H/1tttuU/369dWnTx8762PBggVOl4ZyRJhyEwkJCcrNzVW7du2KrnXo0MH+pZ+fn+9obfBM1apV01tvvaXo6Ohi148ePepYTfB8Y8eO1cUXX6zGjRs7XQo83NKlS1W5cmU7jfnE6aXmF0hAeQgODlZISIgdlcrJybG/tFy2bJlatGjhdGkoR4QpN5GcnKwqVaooMDCw6Jr5R65Zv2J+uwacbmZ6n/mNWSET2t9//3117drV0brgucxvZ5csWaJ//vOfTpcCL7Bz5047verzzz+30+V79+6tV155hV9QotwEBQXpkUce0UcffaQ2bdro3HPPVY8ePTRo0CCnS0M5Ys2Um8jIyCgWpIzC+2YBLVDenn76aa1bt06ffPKJ06XAA5lfDD366KP2Hxrmt7eAK6bPb9++XVOmTLGjUeaXlqb/mZEDM90PKA+bN29Wr169dP3112vjxo12mqmZYnrRRRc5XRrKCWHKjX6b8fvQVHiff3jAFUHKbHzy/PPPq2nTpk6XAw80fvx4tWrVqthoKFCe/P397bTlZ5991o5QGWZzpw8//JAwhXIbfTe/kDQb7Jh/u5m1oWbX3Ndee40w5cEIU24iJiZGhw4dsuumzP8ADPNbNPOH0UzHAsqL+a2Z+ceFCVT9+/d3uhx48A5+Bw4cKFoXWvjLolmzZmn58uUOVwdPXRdqflFZGKSMBg0a2K2rgfJgdmSuV69esV+Ct2zZUhMmTHC0LpQvwpSbMIsTTYgyu8CYXdUKF8+a32qYraqB8hotMFNgzNkrbMGP8vTee+/ZXxYVeuaZZ+xXs20wUB7MmhUzvXTr1q02RBlmQ4ATwxVwOlWvXt1OLTW/LCpcqmH6XJ06dZwuDeWIf6W7CTOHe+DAgRo9erRWrVqlOXPm2EN7hwwZ4nRp8OB53a+++qpuuukmu3OkGQktbMDpZv4Ba35jW9gqVapkm7kNlIeGDRvq7LPP1qhRo+yOuT/99JPeeOMNXXXVVU6XBg9lDukNCAjQv/71Lxviv/vuOzsqde211zpdGsqRTwFHgbvVJhQmTH377bd2O9cbb7xR1113ndNlwUOZf1SYtQR/JDEx0eX1wLuMHDnSfn3qqaecLgUeLC0tzU5lnj17tv2l5dVXX63bb79dPj4+TpcGD2UO6n3iiSfsL8ajoqJ0zTXXaOjQofQ5D0aYAgAAAIAyYJofAAAAAJQBYQoAAAAAyoAwBQAAAABlQJgCAAAAgDIgTAEAAABAGRCmAAAAAKAMCFMAAAAAUAaEKQAAAAAoA/+yvAgAAHdyzjnnaPfu3X/42LvvvqsuXbqUy/cdOXKk/frUU0+Vy/sDANwbYQoA4BEeeughnXfeeSddj4iIcKQeAIDnI0wBADxCWFiYqlWr5nQZAAAvwpopAIBXTAN85513dOGFF6pt27a6+eablZycXPT45s2bdeONN6p9+/Y666yzNH78eOXn5xc9/sUXX2jAgAFq06aNBg8erHXr1hU9dvToUd199932sbPPPltfffWVyz8fAMAZhCkAgFd4+eWXNWzYMH300UfKyMjQ8OHD7fWUlBRdffXVql69uqZOnapHH31U77//vl1rZfz00096+OGHNXToUH355Zdq1aqVbrnlFmVnZ9vHZ8+erbi4OH399dc699xz7XTDtLQ0Rz8rAMA1fAoKCgpc9L0AACi3kScz0uTvX3z2eq1atfTNN9/Yx/v06WODjrFz505734wi/frrr5o4caLmzJlT9PoPP/xQr7zyiubPn6877rhDlStXLtpkwoSo559/XjfccIOeffZZbdu2TVOmTLGPmRDVsWNHffzxx3akCgDg2VgzBQDwCHfeeaf69etX7NqJ4cpM4SsUGxuryMhIO73PNDOydOJz27VrZ8PZkSNHtHXrVju1r1BgYKAefPDBYu914rotIysrqxw+IQDA3RCmAAAeoWrVqqpXr96fPv77Uau8vDz5+voqKCjopOcWrpcyz/n9637Pz8/vpGtM+gAA78CaKQCAV0hISCi6vX37djslr1mzZmrQoIHWrl2rnJycoseXL1+uqKgoO3plAtqJrzUBy0wbXLp0qcs/AwDAvRCmAAAewYQjMzXv9y09Pd0+bjaUmDt3rg1GZu1U9+7dVb9+fbvDn1kH9cgjj9gpf2btlNms4qqrrpKPj4+uvfZau/HEZ599ZkPYk08+aUeezNRAAIB3Y5ofAMAj/Oc//7Ht90aMGGG/XnLJJXruuee0Z88e9ezZU2PGjLHXzeYSb731lp544gkNHDjQjkiZnfvMjn1Gp06d7A5/ZkMKE87Mbn4TJkxQcHCwiz8hAMDdsJsfAMDjmWl5Zle+Sy+91OlSAAAehGl+AAAAAFAGhCkAAAAAKAOm+QEAAABAGTAyBQAAAABlQJgCAAAAgDIgTAEAAABAGRCmAAAAAKAMCFMAAAAAUAaEKQAAAAAoA8IUAAAAAJQBYQoAAAAAVHr/D+bcfbYT2/u+AAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Evaluation",
   "id": "1e658d41032bbe10"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T22:14:41.223340Z",
     "start_time": "2025-07-08T22:14:36.858672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_fast(model, test_df, train_df, k=10, max_users=100):\n",
    "    \"\"\"FAST evaluation - much faster than the original\"\"\"\n",
    "\n",
    "    print(f\"Fast evaluation with max {max_users} users...\")\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    test_grouped = test_df.groupby('playlist_idx')['track_idx'].apply(list).to_dict()\n",
    "    train_grouped = train_df.groupby('playlist_idx')['track_idx'].apply(set).to_dict()\n",
    "\n",
    "    # Sample users if too many (for speed)\n",
    "    users_to_evaluate = list(test_grouped.keys())\n",
    "    if len(users_to_evaluate) > max_users:\n",
    "        users_to_evaluate = np.random.choice(users_to_evaluate, max_users, replace=False)\n",
    "        print(f\"Sampling {max_users} users out of {len(test_grouped)} for speed\")\n",
    "\n",
    "    for i, user_id in enumerate(users_to_evaluate):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Evaluating user {i+1}/{len(users_to_evaluate)}\")\n",
    "\n",
    "        true_items = test_grouped[user_id]\n",
    "        if len(true_items) == 0:\n",
    "            continue\n",
    "\n",
    "        # Get user's training items (for filtering)\n",
    "        user_train_items = train_grouped.get(user_id, set())\n",
    "\n",
    "        # FAST: Only compute predictions for candidate items (not all items)\n",
    "        # Get candidate items: random sample + some popular items\n",
    "        candidate_items = set()\n",
    "\n",
    "        # Add random items\n",
    "        random_items = np.random.choice(model.n_items, min(500, model.n_items), replace=False)\n",
    "        candidate_items.update(random_items)\n",
    "\n",
    "        # Add true test items (to ensure recall can be > 0)\n",
    "        candidate_items.update(true_items)\n",
    "\n",
    "        # Remove training items\n",
    "        candidate_items = candidate_items - user_train_items\n",
    "        candidate_items = list(candidate_items)\n",
    "\n",
    "        # Compute predictions only for candidates\n",
    "        item_scores = []\n",
    "        for item_id in candidate_items:\n",
    "            score = model.predict(user_id, item_id)\n",
    "            item_scores.append((item_id, score))\n",
    "\n",
    "        # Sort and get top-k\n",
    "        item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_k_items = [item for item, score in item_scores[:k]]\n",
    "\n",
    "        # Calculate metrics\n",
    "        relevant_items = set(true_items)\n",
    "        recommended_items = set(top_k_items)\n",
    "\n",
    "        if len(recommended_items) > 0:\n",
    "            precision = len(relevant_items & recommended_items) / len(recommended_items)\n",
    "            precisions.append(precision)\n",
    "\n",
    "        if len(relevant_items) > 0:\n",
    "            recall = len(relevant_items & recommended_items) / len(relevant_items)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "\n",
    "    print(f\"Evaluated {len(precisions)} users\")\n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "# Evaluate (MUCH FASTER)\n",
    "print(\"\\nEvaluating model...\")\n",
    "\n",
    "# Try the fast evaluation first\n",
    "precision_5, recall_5 = evaluate_fast(model, test_df, train_df, k=5, max_users=50)\n",
    "precision_10, recall_10 = evaluate_fast(model, test_df, train_df, k=10, max_users=50)\n",
    "\n",
    "print(f\"\\nFast Evaluation Results:\")\n",
    "print(f\"Precision@5: {precision_5:.4f}\")\n",
    "print(f\"Recall@5: {recall_5:.4f}\")\n",
    "print(f\"Precision@10: {precision_10:.4f}\")\n",
    "print(f\"Recall@10: {recall_10:.4f}\")"
   ],
   "id": "97ac3482b4cbbe18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model...\n",
      "Fast evaluation with max 50 users...\n",
      "Sampling 50 users out of 49725 for speed\n",
      "Evaluating user 1/50\n",
      "Evaluating user 21/50\n",
      "Evaluating user 41/50\n",
      "Evaluated 50 users\n",
      "Fast evaluation with max 50 users...\n",
      "Sampling 50 users out of 49725 for speed\n",
      "Evaluating user 1/50\n",
      "Evaluating user 21/50\n",
      "Evaluating user 41/50\n",
      "Evaluated 50 users\n",
      "\n",
      "Fast Evaluation Results:\n",
      "Precision@5: 0.2720\n",
      "Recall@5: 0.1626\n",
      "Precision@10: 0.1940\n",
      "Recall@10: 0.2593\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7. Recommendation Example",
   "id": "ccf096e36b622ac7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T23:12:38.120133Z",
     "start_time": "2025-07-07T23:12:37.475954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_recommendations_simple(model, user_id, train_df, df_processed, k=10):\n",
    "    \"\"\"Get recommendations for a user\"\"\"\n",
    "\n",
    "    # Get user's training items\n",
    "    user_train_items = set(train_df[train_df['playlist_idx'] == user_id]['track_idx'].values)\n",
    "\n",
    "    # Get predictions for all items\n",
    "    predictions = model.predict_all_for_user(user_id)\n",
    "\n",
    "    # Sort and filter\n",
    "    item_scores = [(i, score) for i, score in enumerate(predictions) if i not in user_train_items]\n",
    "    item_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    recommendations = []\n",
    "    for item_idx, score in item_scores[:k]:\n",
    "        track_info = df_processed[df_processed['track_idx'] == item_idx].iloc[0]\n",
    "        recommendations.append({\n",
    "            'track_name': track_info['track_name'],\n",
    "            'artist_name': track_info['artist_name'],\n",
    "            'score': score\n",
    "        })\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Example recommendations\n",
    "sample_user = np.random.choice(train_df['playlist_idx'].unique())\n",
    "print(f\"\\nRecommendations for user {sample_user}:\")\n",
    "\n",
    "# Show user's known tracks\n",
    "user_tracks = train_df[train_df['playlist_idx'] == sample_user]['track_idx'].values[:5]\n",
    "print(\"\\nUser's known tracks:\")\n",
    "for track_idx in user_tracks:\n",
    "    track_info = df_processed[df_processed['track_idx'] == track_idx].iloc[0]\n",
    "    print(f\"- {track_info['track_name']} by {track_info['artist_name']}\")\n",
    "\n",
    "# Show recommendations\n",
    "recommendations = get_recommendations_simple(model, sample_user, train_df, df_processed)\n",
    "print(f\"\\nTop 10 recommendations:\")\n",
    "for i, rec in enumerate(recommendations):\n",
    "    print(f\"{i+1}. {rec['track_name']} by {rec['artist_name']} ({rec['score']:.3f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BASELINE COMPLETE!\")\n",
    "print(\"This pure NumPy implementation avoids all PyTorch compatibility issues.\")\n",
    "print(\"You can now compare your GNN against these baseline results:\")\n",
    "print(f\"Precision@10: {precision_10:.4f}\")\n",
    "print(f\"Recall@10: {recall_10:.4f}\")\n",
    "print(\"=\"*50)"
   ],
   "id": "9c5c03f4c1e1f124",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recommendations for user 23994:\n",
      "\n",
      "User's known tracks:\n",
      "- Everybody Move by Tech N9ne\n",
      "- Levi by Old Crow Medicine Show\n",
      "- My Favorite Things by Julie Andrews\n",
      "- Constantly by Immature\n",
      "- Endless Summer Nights by Richard Marx\n",
      "\n",
      "Top 10 recommendations:\n",
      "1. #SameTeam by Swoope (0.952)\n",
      "2. No Surprise by Daughtry (0.950)\n",
      "3. Luv U Giv by Tommy Trash (0.949)\n",
      "4. Hurt No More by Rich Homie Quan (0.949)\n",
      "5. I Can't Turn You Loose by Otis Redding (0.947)\n",
      "6. Love Is All Around by Wet Wet Wet (0.946)\n",
      "7. Light - Taska Black Remix by San Holo (0.946)\n",
      "8. Walk On The Wild Side by DJ Disse (0.945)\n",
      "9. Riot by Rascal Flatts (0.945)\n",
      "10. Be Real by Phoebe Ryan (0.945)\n",
      "\n",
      "==================================================\n",
      "BASELINE COMPLETE!\n",
      "This pure NumPy implementation avoids all PyTorch compatibility issues.\n",
      "You can now compare your GNN against these baseline results:\n",
      "Precision@10: 0.2120\n",
      "Recall@10: 0.2955\n",
      "==================================================\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8. Save Model and Result",
   "id": "9db4ba8c74c65fcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-08T22:14:49.529056Z",
     "start_time": "2025-07-08T22:14:48.261466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = './models/simple_mf_baseline.pkl'\n",
    "print(f\"\\nSaving model to {model_filename}...\")\n",
    "\n",
    "model_data = {\n",
    "    'model': model,\n",
    "    'n_users': n_users,\n",
    "    'n_items': n_items,\n",
    "    'playlist_encoder': playlist_encoder,\n",
    "    'track_encoder': track_encoder,\n",
    "    'training_params': {\n",
    "        'n_factors': 32,\n",
    "        'learning_rate': 0.01,\n",
    "        'reg': 0.01,\n",
    "        'epochs': 10\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(\" Model saved successfully!\")\n",
    "\n",
    "# Save evaluation results\n",
    "results = {\n",
    "    'model_type': 'Simple Matrix Factorization',\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_interactions': len(df),\n",
    "        'filtered_interactions': len(df_processed),\n",
    "        'n_users': n_users,\n",
    "        'n_items': n_items,\n",
    "        'train_interactions': len(train_df),\n",
    "        'test_interactions': len(test_df),\n",
    "        'sparsity': 1 - (len(df_processed) / (n_users * n_items))\n",
    "    },\n",
    "    'model_params': {\n",
    "        'n_factors': 32,\n",
    "        'learning_rate': 0.01,\n",
    "        'regularization': 0.01,\n",
    "        'epochs': 10\n",
    "    },\n",
    "    'evaluation_metrics': {\n",
    "        'precision_at_5': float(precision_5),\n",
    "        'recall_at_5': float(recall_5),\n",
    "        'precision_at_10': float(precision_10),\n",
    "        'recall_at_10': float(recall_10),\n",
    "        'ranking_accuracy': float(acc_5) if 'acc_5' in locals() else float(precision_5)\n",
    "    },\n",
    "    'training_info': {\n",
    "        'final_loss': float(losses[-1]) if losses else None,\n",
    "        'training_time_epochs': 10,\n",
    "        'convergence': 'completed'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results as JSON\n",
    "results_filename = f'./results/mf_baseline_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "with open(results_filename, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\" Results saved to {results_filename}\")\n",
    "\n",
    "# Save a summary report\n",
    "summary_filename = f'./results/mf_baseline_summary_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"MATRIX FACTORIZATION BASELINE SUMMARY\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "    f.write(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "    f.write(\"DATASET INFORMATION:\\n\")\n",
    "    f.write(f\"  Original interactions: {len(df):,}\\n\")\n",
    "    f.write(f\"  Filtered interactions: {len(df_processed):,}\\n\")\n",
    "    f.write(f\"  Number of users (playlists): {n_users:,}\\n\")\n",
    "    f.write(f\"  Number of items (tracks): {n_items:,}\\n\")\n",
    "    f.write(f\"  Matrix sparsity: {(1 - len(df_processed)/(n_users * n_items))*100:.2f}%\\n\")\n",
    "    f.write(f\"  Train interactions: {len(train_df):,}\\n\")\n",
    "    f.write(f\"  Test interactions: {len(test_df):,}\\n\\n\")\n",
    "\n",
    "    f.write(\"MODEL CONFIGURATION:\\n\")\n",
    "    f.write(f\"  Algorithm: Simple Matrix Factorization (SGD)\\n\")\n",
    "    f.write(f\"  Embedding dimensions: 32\\n\")\n",
    "    f.write(f\"  Learning rate: 0.01\\n\")\n",
    "    f.write(f\"  Regularization: 0.01\\n\")\n",
    "    f.write(f\"  Training epochs: 10\\n\")\n",
    "    f.write(f\"  Total parameters: {(n_users + n_items) * 32 + n_users + n_items + 1:,}\\n\\n\")\n",
    "\n",
    "    f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "    f.write(f\"  Precision@5:  {precision_5:.4f}\\n\")\n",
    "    f.write(f\"  Recall@5:     {recall_5:.4f}\\n\")\n",
    "    f.write(f\"  Precision@10: {precision_10:.4f}\\n\")\n",
    "    f.write(f\"  Recall@10:    {recall_10:.4f}\\n\")\n",
    "    if 'acc_5' in locals():\n",
    "        f.write(f\"  Ranking Acc:  {acc_5:.4f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"TRAINING INFORMATION:\\n\")\n",
    "    f.write(f\"  Final training loss: {losses[-1]:.6f}\\n\")\n",
    "    f.write(f\"  Training completed: Yes\\n\")\n",
    "    f.write(f\"  Model saved to: {model_filename}\\n\\n\")\n",
    "\n",
    "    f.write(\"BASELINE EXPECTATIONS:\\n\")\n",
    "    f.write(\"  Your GNN should aim to beat these metrics by 10-30%\\n\")\n",
    "    f.write(\"  Target Precision@10: > {:.4f}\\n\".format(precision_10 * 1.15))\n",
    "    f.write(\"  Target Recall@10: > {:.4f}\\n\".format(recall_10 * 1.15))\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(f\" Summary report saved to {summary_filename}\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MATRIX FACTORIZATION BASELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Dataset: {n_users:,} users, {n_items:,} items, {len(df_processed):,} interactions\")\n",
    "print(f\" Model: 32 factors, {(n_users + n_items) * 32 + n_users + n_items + 1:,} parameters\")\n",
    "print(f\"  Training: 10 epochs, final loss: {losses[-1]:.6f}\")\n",
    "print(f\" Performance:\")\n",
    "print(f\"   Precision@5:  {precision_5:.4f}\")\n",
    "print(f\"   Recall@5:     {recall_5:.4f}\")\n",
    "print(f\"   Precision@10: {precision_10:.4f}\")\n",
    "print(f\"   Recall@10:    {recall_10:.4f}\")\n",
    "print(f\" Saved: {model_filename}\")\n",
    "print(f\" Results: {results_filename}\")\n",
    "print(f\" Summary: {summary_filename}\")\n",
    "print(\"=\"*60)\n"
   ],
   "id": "1c779869930bab8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving model to ./models/simple_mf_baseline.pkl...\n",
      " Model saved successfully!\n",
      " Results saved to ./results/mf_baseline_results_20250708_231449.json\n",
      " Summary report saved to ./results/mf_baseline_summary_20250708_231449.txt\n",
      "\n",
      "============================================================\n",
      "MATRIX FACTORIZATION BASELINE COMPLETE!\n",
      "============================================================\n",
      " Dataset: 49,863 users, 95,365 items, 1,905,189 interactions\n",
      " Model: 32 factors, 4,792,525 parameters\n",
      "  Training: 10 epochs, final loss: 0.151616\n",
      " Performance:\n",
      "   Precision@5:  0.2720\n",
      "   Recall@5:     0.1626\n",
      "   Precision@10: 0.1940\n",
      "   Recall@10:    0.2593\n",
      " Saved: ./models/simple_mf_baseline.pkl\n",
      " Results: ./results/mf_baseline_results_20250708_231449.json\n",
      " Summary: ./results/mf_baseline_summary_20250708_231449.txt\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9. Utility Functions for Loading Saved Model",
   "id": "417e90e58ec34028"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_baseline_model(model_filename='models/simple_mf_baseline.pkl'):\n",
    "    \"\"\"Load a saved baseline model\"\"\"\n",
    "    print(f\"Loading model from {model_filename}...\")\n",
    "\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "\n",
    "    print(\" Model loaded successfully!\")\n",
    "    print(f\"   Users: {model_data['n_users']:,}\")\n",
    "    print(f\"   Items: {model_data['n_items']:,}\")\n",
    "    print(f\"   Saved: {model_data['timestamp']}\")\n",
    "\n",
    "    return model_data\n",
    "\n",
    "def compare_with_baseline(gnn_results, baseline_file='models/simple_mf_baseline.pkl'):\n",
    "    \"\"\"Compare GNN results with baseline\"\"\"\n",
    "\n",
    "    # Load baseline results\n",
    "    try:\n",
    "        baseline_data = load_baseline_model(baseline_file)\n",
    "        baseline_results = baseline_data.get('evaluation_results', {})\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"BASELINE vs GNN COMPARISON\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        metrics = ['precision_at_10', 'recall_at_10']\n",
    "\n",
    "        for metric in metrics:\n",
    "            baseline_val = baseline_results.get(metric, 0)\n",
    "            gnn_val = gnn_results.get(metric, 0)\n",
    "            improvement = ((gnn_val - baseline_val) / baseline_val * 100) if baseline_val > 0 else 0\n",
    "\n",
    "            print(f\"{metric.replace('_', ' ').title()}:\")\n",
    "            print(f\"  Baseline (MF): {baseline_val:.4f}\")\n",
    "            print(f\"  GNN:           {gnn_val:.4f}\")\n",
    "            print(f\"  Improvement:   {improvement:+.1f}%\")\n",
    "            print()\n",
    "\n",
    "        print(\"=\"*50)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\" Baseline file {baseline_file} not found\")\n",
    "        print(\"   Run the baseline notebook first!\")\n",
    "\n",
    "# Example usage for future GNN comparison:\n",
    "print(\"\\n NEXT STEPS:\")\n",
    "print(\"1. Use this baseline to compare your GNN performance\")\n",
    "print(\"2. Load the model later with: load_baseline_model()\")\n",
    "print(\"3. Compare results with: compare_with_baseline(your_gnn_results)\")\n",
    "print(\"\\nExample GNN comparison:\")\n",
    "print(\"```python\")\n",
    "print(\"# After training your GNN:\")\n",
    "print(\"gnn_results = {\")\n",
    "print(\"    'precision_at_10': 0.0850,  # Your GNN results\")\n",
    "print(\"    'recall_at_10': 0.1240\")\n",
    "print(\"}\")\n",
    "print(\"compare_with_baseline(gnn_results)\")\n",
    "print(\"```\")"
   ],
   "id": "bf080e475396f02c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
