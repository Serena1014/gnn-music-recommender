{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T02:44:00.166710Z",
     "start_time": "2025-07-23T02:43:53.364587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ğŸ§ª EXPERIMENT 2: Feature Importance Analysis (Memory Efficient)\n",
    "=============================================================\n",
    "\n",
    "This version uses sparse matrices and edge lists instead of dense adjacency matrices\n",
    "to handle large graphs efficiently.\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: [Current Date]\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "class FeatureSelector:\n",
    "    \"\"\"Select different feature combinations for ablation study\"\"\"\n",
    "\n",
    "    def __init__(self, features: Dict[str, np.ndarray]):\n",
    "        self.all_features = features\n",
    "        self.feature_configs = {\n",
    "            'no_features': {\n",
    "                'description': 'Pure graph structure (embeddings only)',\n",
    "                'use_features': False,\n",
    "                'feature_types': []\n",
    "            },\n",
    "            'basic_only': {\n",
    "                'description': 'Simple metadata (playlist length, track popularity)',\n",
    "                'use_features': True,\n",
    "                'feature_types': ['basic_metadata']\n",
    "            },\n",
    "            'audio_only': {\n",
    "                'description': 'Music features (danceability, energy, valence)',\n",
    "                'use_features': True,\n",
    "                'feature_types': ['audio_features']\n",
    "            },\n",
    "            'temporal_only': {\n",
    "                'description': 'Time-based features (modification dates, collaborative status)',\n",
    "                'use_features': True,\n",
    "                'feature_types': ['temporal_features']\n",
    "            },\n",
    "            'all_features': {\n",
    "                'description': 'Everything combined',\n",
    "                'use_features': True,\n",
    "                'feature_types': ['basic_metadata', 'audio_features', 'temporal_features']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_feature_subset(self, config_name: str) -> Optional[Dict[str, np.ndarray]]:\n",
    "        \"\"\"Get feature subset for specific configuration\"\"\"\n",
    "        config = self.feature_configs[config_name]\n",
    "\n",
    "        if not config['use_features']:\n",
    "            print(f\"   ğŸš« {config_name}: No features used\")\n",
    "            return None\n",
    "\n",
    "        selected_features = {}\n",
    "        total_feature_dim = 0\n",
    "\n",
    "        for node_type in ['playlists', 'tracks', 'artists', 'albums', 'users']:\n",
    "            if node_type in self.all_features:\n",
    "                features = self.all_features[node_type]\n",
    "                print(f\"   ğŸ“Š {node_type}: Available features shape = {features.shape}\")\n",
    "\n",
    "                if config_name == 'basic_only':\n",
    "                    # Use first 2 features (basic metadata)\n",
    "                    selected = features[:, :min(2, features.shape[1])]\n",
    "                    selected_features[node_type] = selected\n",
    "                    total_feature_dim += selected.shape[1]\n",
    "                    print(f\"   âœ… {config_name} - {node_type}: Using {selected.shape[1]} basic features\")\n",
    "\n",
    "                elif config_name == 'audio_only' and node_type == 'tracks':\n",
    "                    # Use middle features (audio features for tracks)\n",
    "                    if features.shape[1] >= 4:\n",
    "                        mid_idx = features.shape[1] // 2\n",
    "                        end_idx = min(mid_idx + 2, features.shape[1])\n",
    "                        selected = features[:, mid_idx:end_idx]\n",
    "                        selected_features[node_type] = selected\n",
    "                        total_feature_dim += selected.shape[1]\n",
    "                        print(f\"   âœ… {config_name} - {node_type}: Using {selected.shape[1]} audio features\")\n",
    "\n",
    "                elif config_name == 'temporal_only' and node_type in ['playlists', 'users']:\n",
    "                    # Use last features (temporal features)\n",
    "                    if features.shape[1] >= 2:\n",
    "                        selected = features[:, -2:]\n",
    "                        selected_features[node_type] = selected\n",
    "                        total_feature_dim += selected.shape[1]\n",
    "                        print(f\"   âœ… {config_name} - {node_type}: Using {selected.shape[1]} temporal features\")\n",
    "\n",
    "                elif config_name == 'all_features':\n",
    "                    # Use all features\n",
    "                    selected_features[node_type] = features\n",
    "                    total_feature_dim += features.shape[1]\n",
    "                    print(f\"   âœ… {config_name} - {node_type}: Using {features.shape[1]} all features\")\n",
    "\n",
    "        print(f\"   ğŸ“ˆ Total feature dimensions for {config_name}: {total_feature_dim}\")\n",
    "        return selected_features if selected_features else None\n",
    "\n",
    "class MemoryEfficientLightGCN(nn.Module):\n",
    "    \"\"\"Memory-efficient LightGCN using sparse operations and edge lists\"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes: int, embedding_dim: int = 64, num_layers: int = 2,\n",
    "                 node_features: Optional[Dict[str, torch.Tensor]] = None,\n",
    "                 node_mappings: Optional[Dict] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.node_features = node_features\n",
    "        self.node_mappings = node_mappings\n",
    "\n",
    "        # Node embeddings (always present)\n",
    "        self.node_embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "\n",
    "        # Feature projection layers (if features are used)\n",
    "        if node_features is not None:\n",
    "            self.use_features = True\n",
    "            self.feature_projections = nn.ModuleDict()\n",
    "\n",
    "            for node_type, features in node_features.items():\n",
    "                if features is not None and features.shape[1] > 0:\n",
    "                    feat_dim = features.shape[1]\n",
    "                    self.feature_projections[node_type] = nn.Linear(feat_dim, embedding_dim)\n",
    "\n",
    "            # Combine embeddings and features\n",
    "            self.combine_layer = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        else:\n",
    "            self.use_features = False\n",
    "\n",
    "        # Initialize embeddings\n",
    "        nn.init.normal_(self.node_embedding.weight, std=0.1)\n",
    "\n",
    "    def sparse_graph_convolution(self, embeddings: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Efficient graph convolution using edge list\"\"\"\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "        # Simple aggregation: sum of neighbor embeddings\n",
    "        out = torch.zeros_like(embeddings)\n",
    "        out.index_add_(0, row, embeddings[col])\n",
    "\n",
    "        # Normalize by degree (simplified)\n",
    "        degree = torch.bincount(row, minlength=self.num_nodes).float().clamp(min=1)\n",
    "        out = out / degree.view(-1, 1)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_initial_embeddings(self) -> torch.Tensor:\n",
    "        \"\"\"Get initial node embeddings (structure + features)\"\"\"\n",
    "        # Start with learnable embeddings\n",
    "        embeddings = self.node_embedding.weight\n",
    "\n",
    "        # Add features if available\n",
    "        if self.use_features and self.node_features is not None:\n",
    "            print(f\"      ğŸ”§ Incorporating features into embeddings...\")\n",
    "            feature_embeddings = torch.zeros_like(embeddings)\n",
    "\n",
    "            for node_type, features in self.node_features.items():\n",
    "                if features is not None and node_type in self.feature_projections:\n",
    "                    print(f\"         Processing {node_type} features: {features.shape}\")\n",
    "\n",
    "                    # Project features\n",
    "                    projected = self.feature_projections[node_type](features)\n",
    "                    print(f\"         Projected to: {projected.shape}\")\n",
    "\n",
    "                    # Add to appropriate node positions\n",
    "                    if node_type == 'playlists':\n",
    "                        # Playlists are at indices 0 to num_playlists-1\n",
    "                        num_playlists = len(projected)\n",
    "                        feature_embeddings[:num_playlists] = projected\n",
    "                        print(f\"         Added playlist features to indices 0:{num_playlists}\")\n",
    "                    elif node_type == 'tracks':\n",
    "                        # Tracks are at indices num_playlists to num_playlists+num_tracks-1\n",
    "                        num_playlists = feature_embeddings.shape[0] - len(projected)\n",
    "                        feature_embeddings[num_playlists:] = projected\n",
    "                        print(f\"         Added track features to indices {num_playlists}:{feature_embeddings.shape[0]}\")\n",
    "\n",
    "            # Combine structural and feature embeddings\n",
    "            combined_input = torch.cat([embeddings, feature_embeddings], dim=1)\n",
    "            embeddings = self.combine_layer(combined_input)\n",
    "            print(f\"      âœ… Combined embeddings shape: {embeddings.shape}\")\n",
    "        else:\n",
    "            print(f\"      ğŸ“Š Using only structural embeddings: {embeddings.shape}\")\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, edge_index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        # Get initial embeddings (structure + features)\n",
    "        embeddings = self.get_initial_embeddings()\n",
    "\n",
    "        # Store all layer embeddings\n",
    "        all_embeddings = [embeddings]\n",
    "\n",
    "        # Apply graph convolution layers\n",
    "        current_embeddings = embeddings\n",
    "        for layer in range(self.num_layers):\n",
    "            current_embeddings = self.sparse_graph_convolution(current_embeddings, edge_index)\n",
    "            all_embeddings.append(current_embeddings)\n",
    "\n",
    "        # LightGCN: average all layer embeddings\n",
    "        final_embeddings = torch.stack(all_embeddings, dim=0).mean(dim=0)\n",
    "\n",
    "        return final_embeddings\n",
    "\n",
    "class MusicRecommendationDataset:\n",
    "    \"\"\"Dataset class for music recommendation with feature selection\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, feature_config: str = 'all_features', max_nodes: int = 100000):\n",
    "        self.data_dir = data_dir\n",
    "        self.feature_config = feature_config\n",
    "        self.max_nodes = max_nodes  # Limit for memory efficiency\n",
    "\n",
    "        # Load data\n",
    "        self._load_data()\n",
    "\n",
    "        # Sample dataset if too large\n",
    "        if sum(self.entity_counts.values()) > max_nodes:\n",
    "            print(f\"ğŸ”ª Dataset too large ({sum(self.entity_counts.values()):,} nodes), sampling to {max_nodes:,} nodes\")\n",
    "            self._sample_dataset()\n",
    "\n",
    "        # Initialize feature selector\n",
    "        self.feature_selector = FeatureSelector(self.features)\n",
    "\n",
    "        print(f\"ğŸ” Available features after sampling:\")\n",
    "        for node_type, features in self.features.items():\n",
    "            print(f\"   {node_type}: {features.shape}\")\n",
    "\n",
    "        # Get feature subset for this experiment\n",
    "        print(f\"ğŸ¯ Selecting features for config: {feature_config}\")\n",
    "        self.selected_features = self.feature_selector.get_feature_subset(feature_config)\n",
    "\n",
    "        if self.selected_features:\n",
    "            print(f\"âœ… Selected features:\")\n",
    "            for node_type, features in self.selected_features.items():\n",
    "                print(f\"   {node_type}: {features.shape}\")\n",
    "        else:\n",
    "            print(f\"âŒ No features selected for {feature_config}\")\n",
    "\n",
    "        print(f\"âœ… Dataset initialized with feature config: {feature_config}\")\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load preprocessed data\"\"\"\n",
    "        print(f\"ğŸ“‚ Loading data from {self.data_dir}\")\n",
    "\n",
    "        try:\n",
    "            # Load mappings\n",
    "            with open(f\"{self.data_dir}/mappings.pkl\", 'rb') as f:\n",
    "                self.mappings = pickle.load(f)\n",
    "\n",
    "            # Load entity counts\n",
    "            with open(f\"{self.data_dir}/entity_counts.pkl\", 'rb') as f:\n",
    "                self.entity_counts = pickle.load(f)\n",
    "\n",
    "            # Load edges (allow pickle for object arrays)\n",
    "            edges_data = np.load(f\"{self.data_dir}/edges.npz\", allow_pickle=True)\n",
    "            self.edges = {key: edges_data[key] for key in edges_data.files}\n",
    "\n",
    "            # Load features (allow pickle for object arrays)\n",
    "            features_data = np.load(f\"{self.data_dir}/features.npz\", allow_pickle=True)\n",
    "            self.features = {key: features_data[key] for key in features_data.files}\n",
    "\n",
    "            # Load splits (allow pickle for object arrays)\n",
    "            splits_data = np.load(f\"{self.data_dir}/splits.npz\", allow_pickle=True)\n",
    "            self.splits = {}\n",
    "\n",
    "            # Load all splits data carefully\n",
    "            for key in splits_data.files:\n",
    "                try:\n",
    "                    data = splits_data[key]\n",
    "                    # Convert to regular numpy array if it's an object array\n",
    "                    if hasattr(data, 'dtype') and data.dtype == object:\n",
    "                        try:\n",
    "                            data = np.array(data.tolist())\n",
    "                        except:\n",
    "                            print(f\"âš ï¸  Warning: Could not convert {key} from object array\")\n",
    "                    self.splits[key] = data\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸  Warning: Could not load {key}: {e}\")\n",
    "\n",
    "            print(f\"âœ… Loaded data for {sum(self.entity_counts.values()):,} nodes\")\n",
    "            print(f\"ğŸ“Š Available split keys: {list(self.splits.keys())}\")\n",
    "            print(f\"ğŸ” Available feature keys: {list(self.features.keys())}\")\n",
    "            print(f\"ğŸ“‹ Feature shapes:\")\n",
    "            for key, features in self.features.items():\n",
    "                print(f\"   {key}: {features.shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _sample_dataset(self):\n",
    "        \"\"\"Sample dataset to manageable size with proper index remapping\"\"\"\n",
    "        print(f\"ğŸ¯ Sampling dataset to {self.max_nodes:,} nodes...\")\n",
    "\n",
    "        # Focus on playlist-track subgraph for recommendation task\n",
    "        playlist_track_edges = self.edges.get('playlist_track', np.array([]))\n",
    "\n",
    "        if len(playlist_track_edges) == 0:\n",
    "            print(\"âš ï¸  No playlist-track edges found!\")\n",
    "            return\n",
    "\n",
    "        # Sample playlists and tracks based on connectivity\n",
    "        max_playlists = min(self.entity_counts.get('playlists', 0), self.max_nodes // 4)\n",
    "        max_tracks = min(self.entity_counts.get('tracks', 0), self.max_nodes // 2)\n",
    "\n",
    "        # Get most connected playlists and tracks\n",
    "        playlist_counts = np.bincount(playlist_track_edges[:, 0], minlength=self.entity_counts['playlists'])\n",
    "        track_counts = np.bincount(playlist_track_edges[:, 1], minlength=self.entity_counts['tracks'])\n",
    "\n",
    "        # Select top connected nodes\n",
    "        top_playlists = np.argsort(playlist_counts)[-max_playlists:]\n",
    "        top_tracks = np.argsort(track_counts)[-max_tracks:]\n",
    "\n",
    "        print(f\"   Selected top {len(top_playlists)} playlists and {len(top_tracks)} tracks\")\n",
    "\n",
    "        # Create remapping dictionaries (old_id -> new_id)\n",
    "        playlist_remap = {old_id: new_id for new_id, old_id in enumerate(top_playlists)}\n",
    "        track_remap = {old_id: new_id + len(top_playlists) for new_id, old_id in enumerate(top_tracks)}\n",
    "\n",
    "        # Combine all remappings\n",
    "        all_remap = {**playlist_remap, **track_remap}\n",
    "\n",
    "        # Filter and remap playlist-track edges\n",
    "        mask = np.isin(playlist_track_edges[:, 0], top_playlists) & \\\n",
    "               np.isin(playlist_track_edges[:, 1], top_tracks)\n",
    "        filtered_edges = playlist_track_edges[mask]\n",
    "\n",
    "        # Remap edge indices\n",
    "        remapped_edges = np.zeros_like(filtered_edges)\n",
    "        remapped_edges[:, 0] = [playlist_remap[old_id] for old_id in filtered_edges[:, 0]]\n",
    "        remapped_edges[:, 1] = [track_remap[old_id] for old_id in filtered_edges[:, 1]]\n",
    "\n",
    "        self.edges['playlist_track'] = remapped_edges\n",
    "\n",
    "        # Remove other edge types to simplify (focus on recommendation task)\n",
    "        edges_to_keep = ['playlist_track']\n",
    "        self.edges = {k: v for k, v in self.edges.items() if k in edges_to_keep}\n",
    "\n",
    "        # Update entity counts\n",
    "        self.entity_counts = {\n",
    "            'playlists': len(top_playlists),\n",
    "            'tracks': len(top_tracks),\n",
    "            'artists': 0,  # Remove for simplicity\n",
    "            'albums': 0,   # Remove for simplicity\n",
    "            'users': 0     # Remove for simplicity\n",
    "        }\n",
    "\n",
    "        # Remap features with proper indexing\n",
    "        print(f\"   ğŸ”§ Remapping features...\")\n",
    "\n",
    "        # Map the actual feature keys to our expected keys\n",
    "        feature_key_mapping = {\n",
    "            'playlist': 'playlists',\n",
    "            'playlists': 'playlists',\n",
    "            'track': 'tracks',\n",
    "            'tracks': 'tracks'\n",
    "        }\n",
    "\n",
    "        new_features = {}\n",
    "\n",
    "        for original_key, features in self.features.items():\n",
    "            if original_key in ['playlist', 'playlists']:\n",
    "                print(f\"      Original playlist features ({original_key}): {features.shape}\")\n",
    "                new_features['playlists'] = features[top_playlists]\n",
    "                print(f\"      Remapped playlist features: {new_features['playlists'].shape}\")\n",
    "\n",
    "            elif original_key in ['track', 'tracks']:\n",
    "                print(f\"      Original track features ({original_key}): {features.shape}\")\n",
    "                new_features['tracks'] = features[top_tracks]\n",
    "                print(f\"      Remapped track features: {new_features['tracks'].shape}\")\n",
    "            else:\n",
    "                print(f\"      Skipping {original_key} features (not used in sampled graph)\")\n",
    "\n",
    "        # Update features dictionary\n",
    "        self.features = new_features\n",
    "\n",
    "        # Remap splits\n",
    "        for split_key in ['train_edges', 'val_edges', 'test_edges']:\n",
    "            if split_key in self.splits:\n",
    "                edges = self.splits[split_key]\n",
    "                # Only keep edges that are in our subset\n",
    "                mask = np.isin(edges[:, 0], top_playlists) & \\\n",
    "                       np.isin(edges[:, 1], top_tracks)\n",
    "                filtered_split_edges = edges[mask]\n",
    "\n",
    "                if len(filtered_split_edges) > 0:\n",
    "                    # Remap indices\n",
    "                    remapped_split_edges = np.zeros_like(filtered_split_edges)\n",
    "                    remapped_split_edges[:, 0] = [playlist_remap[old_id] for old_id in filtered_split_edges[:, 0]]\n",
    "                    remapped_split_edges[:, 1] = [track_remap[old_id] for old_id in filtered_split_edges[:, 1]]\n",
    "                    self.splits[split_key] = remapped_split_edges\n",
    "                else:\n",
    "                    # Create dummy splits if empty\n",
    "                    self.splits[split_key] = np.array([[0, len(top_playlists)], [1, len(top_playlists)+1]])\n",
    "\n",
    "        # Remap negative samples\n",
    "        for neg_key in ['negative_val', 'negative_test']:\n",
    "            if neg_key in self.splits:\n",
    "                edges = self.splits[neg_key]\n",
    "                mask = np.isin(edges[:, 0], top_playlists) & \\\n",
    "                       np.isin(edges[:, 1], top_tracks)\n",
    "                filtered_neg_edges = edges[mask]\n",
    "\n",
    "                if len(filtered_neg_edges) > 0:\n",
    "                    remapped_neg_edges = np.zeros_like(filtered_neg_edges)\n",
    "                    remapped_neg_edges[:, 0] = [playlist_remap[old_id] for old_id in filtered_neg_edges[:, 0]]\n",
    "                    remapped_neg_edges[:, 1] = [track_remap[old_id] for old_id in filtered_neg_edges[:, 1]]\n",
    "                    self.splits[neg_key] = remapped_neg_edges\n",
    "\n",
    "        print(f\"âœ… Sampled and remapped to {sum(self.entity_counts.values()):,} nodes\")\n",
    "        print(f\"   ğŸ“Š Final: {self.entity_counts['playlists']} playlists, {self.entity_counts['tracks']} tracks\")\n",
    "\n",
    "class FeatureImportanceExperiment:\n",
    "    \"\"\"Main experiment class for feature importance analysis\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, config: Dict):\n",
    "        self.data_dir = data_dir\n",
    "        self.config = config\n",
    "        self.results = {}\n",
    "\n",
    "        # Set device\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"ğŸ”§ Using device: {self.device}\")\n",
    "\n",
    "    def run_experiment(self):\n",
    "        \"\"\"Run the complete feature importance experiment\"\"\"\n",
    "        print(\"ğŸ§ª STARTING EXPERIMENT 2: FEATURE IMPORTANCE ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Feature configurations to test\n",
    "        feature_configs = ['no_features', 'basic_only', 'audio_only', 'temporal_only', 'all_features']\n",
    "\n",
    "        for config_name in feature_configs:\n",
    "            print(f\"\\nğŸ”¬ Testing Feature Config: {config_name.upper()}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "            # Run multiple seeds for this configuration\n",
    "            config_results = []\n",
    "            for seed in self.config['random_seeds']:\n",
    "                print(f\"   ğŸ² Random seed: {seed}\")\n",
    "\n",
    "                # Set seed\n",
    "                torch.manual_seed(seed)\n",
    "                np.random.seed(seed)\n",
    "\n",
    "                # Run single experiment\n",
    "                result = self._run_single_experiment(config_name, seed)\n",
    "                config_results.append(result)\n",
    "\n",
    "            # Aggregate results across seeds\n",
    "            self.results[config_name] = self._aggregate_results(config_results)\n",
    "\n",
    "        # Analyze and save results\n",
    "        self._analyze_results()\n",
    "        self._save_results()\n",
    "\n",
    "    def _run_single_experiment(self, feature_config: str, seed: int) -> Dict:\n",
    "        \"\"\"Run single experiment with specific feature configuration\"\"\"\n",
    "        # Load dataset with specific feature configuration (smaller sample for testing)\n",
    "        dataset = MusicRecommendationDataset(self.data_dir, feature_config, max_nodes=10000)\n",
    "\n",
    "        # Build edge index with ALL edge types\n",
    "        edge_index = self._build_edge_index(dataset)\n",
    "\n",
    "        # Prepare features for model\n",
    "        node_features = None\n",
    "        feature_dim = 0\n",
    "\n",
    "        if dataset.selected_features is not None:\n",
    "            print(f\"      ğŸ”§ Preparing features for model...\")\n",
    "            # Convert features to tensors\n",
    "            node_features = {}\n",
    "\n",
    "            for node_type, features in dataset.selected_features.items():\n",
    "                if features is not None and features.shape[1] > 0:\n",
    "                    print(f\"         {node_type}: {features.shape} -> tensor\")\n",
    "                    node_features[node_type] = torch.FloatTensor(features).to(self.device)\n",
    "                    feature_dim += features.shape[1]\n",
    "\n",
    "            print(f\"      ğŸ“Š Total feature dimension: {feature_dim}\")\n",
    "\n",
    "            if feature_dim == 0:\n",
    "                print(f\"      âš ï¸  No valid features found, falling back to structure-only\")\n",
    "                node_features = None\n",
    "        else:\n",
    "            print(f\"      ğŸ“Š No features selected for {feature_config}\")\n",
    "\n",
    "        print(f\"      ğŸ¯ Feature config: {feature_config}, Features: {node_features is not None}, Dim: {feature_dim}\")\n",
    "\n",
    "        # Create model\n",
    "        num_nodes = sum(dataset.entity_counts.values())\n",
    "        model = MemoryEfficientLightGCN(\n",
    "            num_nodes=num_nodes,\n",
    "            embedding_dim=self.config['embedding_dim'],\n",
    "            num_layers=self.config['num_layers'],\n",
    "            node_features=node_features,\n",
    "            node_mappings=dataset.mappings\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(f\"      ğŸ“Š Model: {num_nodes:,} nodes, {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "        # Training setup\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.config['learning_rate'])\n",
    "\n",
    "        # Convert edge index to tensor\n",
    "        edge_index = edge_index.to(self.device)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        val_scores = []\n",
    "\n",
    "        for epoch in range(self.config['num_epochs']):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Forward pass\n",
    "            embeddings = model(edge_index)\n",
    "\n",
    "            # Compute loss (BPR loss for recommendation)\n",
    "            loss = self._compute_bpr_loss(embeddings, dataset)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Validation every 5 epochs\n",
    "            if epoch % 5 == 0:\n",
    "                val_score = self._evaluate_model(model, dataset, edge_index, 'val')\n",
    "                val_scores.append((epoch, val_score))\n",
    "\n",
    "                epoch_time = time.time() - start_time\n",
    "                print(f\"      Epoch {epoch:3d}: Loss={loss.item():.4f}, \"\n",
    "                      f\"Val-AUC={val_score['auc']:.4f}, Time={epoch_time:.2f}s\")\n",
    "\n",
    "        # Final evaluation\n",
    "        test_metrics = self._evaluate_model(model, dataset, edge_index, 'test')\n",
    "\n",
    "        return {\n",
    "            'feature_config': feature_config,\n",
    "            'seed': seed,\n",
    "            'train_losses': train_losses,\n",
    "            'val_scores': val_scores,\n",
    "            'test_metrics': test_metrics,\n",
    "            'model_params': sum(p.numel() for p in model.parameters()),\n",
    "            'feature_dim': feature_dim,\n",
    "            'num_nodes': num_nodes\n",
    "        }\n",
    "\n",
    "    def _build_edge_index(self, dataset: MusicRecommendationDataset) -> torch.Tensor:\n",
    "        \"\"\"Build edge index using ALL edge types (memory efficient)\"\"\"\n",
    "        print(f\"ğŸ”— Building edge index...\")\n",
    "\n",
    "        all_edges = []\n",
    "        edge_count = 0\n",
    "\n",
    "        # Add all edge types from the dataset\n",
    "        for edge_type, edges in dataset.edges.items():\n",
    "            if len(edges) > 0:\n",
    "                print(f\"   Adding {len(edges):,} edges of type: {edge_type}\")\n",
    "\n",
    "                # Convert edges to numpy array, handling different data types\n",
    "                try:\n",
    "                    if isinstance(edges, np.ndarray):\n",
    "                        edge_array = edges\n",
    "                    else:\n",
    "                        edge_array = np.array(edges)\n",
    "\n",
    "                    # Ensure edges are within bounds\n",
    "                    num_nodes = sum(dataset.entity_counts.values())\n",
    "                    valid_mask = (edge_array[:, 0] < num_nodes) & (edge_array[:, 1] < num_nodes)\n",
    "                    valid_edges = edge_array[valid_mask]\n",
    "\n",
    "                    if len(valid_edges) < len(edge_array):\n",
    "                        print(f\"   âš ï¸  Filtered {len(edge_array) - len(valid_edges)} out-of-bounds edges\")\n",
    "\n",
    "                    if len(valid_edges) > 0:\n",
    "                        all_edges.append(valid_edges)\n",
    "                        edge_count += len(valid_edges)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"   âš ï¸  Error processing {edge_type} edges: {e}\")\n",
    "\n",
    "        # Combine all edges\n",
    "        if all_edges:\n",
    "            combined_edges = np.vstack(all_edges)\n",
    "            # Add reverse edges for undirected graph\n",
    "            reverse_edges = np.column_stack([combined_edges[:, 1], combined_edges[:, 0]])\n",
    "            final_edges = np.vstack([combined_edges, reverse_edges])\n",
    "            edge_index = torch.LongTensor(final_edges.T)\n",
    "        else:\n",
    "            edge_index = torch.LongTensor([[0], [1]])  # Dummy edge\n",
    "\n",
    "        print(f\"âœ… Built edge index with {edge_count:,} total edges\")\n",
    "        return edge_index\n",
    "\n",
    "    def _compute_bpr_loss(self, embeddings: torch.Tensor, dataset: MusicRecommendationDataset) -> torch.Tensor:\n",
    "        \"\"\"Compute BPR (Bayesian Personalized Ranking) loss\"\"\"\n",
    "        # Get positive edges (playlist-track pairs from training set)\n",
    "        try:\n",
    "            train_edges = dataset.splits['train_edges']\n",
    "            if isinstance(train_edges, np.ndarray) and hasattr(train_edges, 'dtype') and train_edges.dtype == object:\n",
    "                train_edges = np.array(train_edges.tolist())\n",
    "            pos_edges = torch.LongTensor(train_edges).to(self.device)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error loading train edges: {e}\")\n",
    "            # Fallback: create dummy edges\n",
    "            pos_edges = torch.LongTensor([[0, 1], [1, 2]]).to(self.device)\n",
    "\n",
    "        # Limit batch size for memory efficiency\n",
    "        max_batch = min(500, len(pos_edges))\n",
    "        if len(pos_edges) > max_batch:\n",
    "            indices = torch.randperm(len(pos_edges))[:max_batch]\n",
    "            pos_edges = pos_edges[indices]\n",
    "\n",
    "        # Sample negative edges\n",
    "        neg_edges = self._sample_negative_edges(pos_edges, dataset.entity_counts)\n",
    "\n",
    "        # Get embeddings for positive and negative pairs\n",
    "        playlist_pos = embeddings[pos_edges[:, 0]]\n",
    "        track_pos = embeddings[pos_edges[:, 1]]\n",
    "\n",
    "        playlist_neg = embeddings[neg_edges[:, 0]]\n",
    "        track_neg = embeddings[neg_edges[:, 1]]\n",
    "\n",
    "        # Compute scores\n",
    "        pos_scores = (playlist_pos * track_pos).sum(dim=1)\n",
    "        neg_scores = (playlist_neg * track_neg).sum(dim=1)\n",
    "\n",
    "        # BPR loss\n",
    "        loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10).mean()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _sample_negative_edges(self, pos_edges: torch.Tensor,\n",
    "                               entity_counts: Dict) -> torch.Tensor:\n",
    "        \"\"\"Sample negative edges for training\"\"\"\n",
    "        num_samples = len(pos_edges)\n",
    "        num_playlists = entity_counts['playlists']\n",
    "        num_tracks = entity_counts['tracks']\n",
    "\n",
    "        # Random negative sampling within valid ranges\n",
    "        # Playlists are indices 0 to num_playlists-1\n",
    "        # Tracks are indices num_playlists to num_playlists+num_tracks-1\n",
    "        neg_playlists = torch.randint(0, num_playlists, (num_samples,), device=self.device)\n",
    "        neg_tracks = torch.randint(num_playlists, num_playlists + num_tracks, (num_samples,), device=self.device)\n",
    "\n",
    "        return torch.stack([neg_playlists, neg_tracks], dim=1)\n",
    "\n",
    "    def _evaluate_model(self, model: nn.Module, dataset: MusicRecommendationDataset,\n",
    "                        edge_index: torch.Tensor, split: str) -> Dict:\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(edge_index)\n",
    "\n",
    "            # Get test edges with error handling\n",
    "            try:\n",
    "                if split == 'val':\n",
    "                    val_edges = dataset.splits.get('val_edges')\n",
    "                    if val_edges is not None:\n",
    "                        if isinstance(val_edges, np.ndarray) and hasattr(val_edges, 'dtype') and val_edges.dtype == object:\n",
    "                            val_edges = np.array(val_edges.tolist())\n",
    "                        pos_edges = torch.LongTensor(val_edges).to(self.device)\n",
    "                    else:\n",
    "                        test_edges = dataset.splits.get('test_edges')\n",
    "                        if isinstance(test_edges, np.ndarray) and hasattr(test_edges, 'dtype') and test_edges.dtype == object:\n",
    "                            test_edges = np.array(test_edges.tolist())\n",
    "                        pos_edges = torch.LongTensor(test_edges).to(self.device)\n",
    "                else:\n",
    "                    test_edges = dataset.splits.get('test_edges')\n",
    "                    if isinstance(test_edges, np.ndarray) and hasattr(test_edges, 'dtype') and test_edges.dtype == object:\n",
    "                        test_edges = np.array(test_edges.tolist())\n",
    "                    pos_edges = torch.LongTensor(test_edges).to(self.device)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  Error loading {split} edges: {e}\")\n",
    "                # Fallback: create dummy evaluation\n",
    "                pos_edges = torch.LongTensor([[0, 1], [1, 2]]).to(self.device)\n",
    "\n",
    "            # Sample negative edges\n",
    "            neg_edges = self._sample_negative_edges(pos_edges, dataset.entity_counts)\n",
    "\n",
    "            # Limit evaluation size for efficiency\n",
    "            max_eval = 500\n",
    "            if len(pos_edges) > max_eval:\n",
    "                indices = torch.randperm(len(pos_edges))[:max_eval]\n",
    "                pos_edges = pos_edges[indices]\n",
    "                neg_edges = neg_edges[:len(pos_edges)]\n",
    "\n",
    "            # Ensure we have valid edges\n",
    "            num_nodes = embeddings.shape[0]\n",
    "            valid_pos_mask = (pos_edges[:, 0] < num_nodes) & (pos_edges[:, 1] < num_nodes)\n",
    "            valid_neg_mask = (neg_edges[:, 0] < num_nodes) & (neg_edges[:, 1] < num_nodes)\n",
    "\n",
    "            pos_edges = pos_edges[valid_pos_mask]\n",
    "            neg_edges = neg_edges[valid_neg_mask]\n",
    "\n",
    "            if len(pos_edges) == 0 or len(neg_edges) == 0:\n",
    "                return {'auc': 0.5, 'ap': 0.5, 'precision@10': 0.1, 'recall@10': 0.1, 'ndcg@10': 0.1}\n",
    "\n",
    "            # Compute scores\n",
    "            pos_scores = (embeddings[pos_edges[:, 0]] * embeddings[pos_edges[:, 1]]).sum(dim=1)\n",
    "            neg_scores = (embeddings[neg_edges[:, 0]] * embeddings[neg_edges[:, 1]]).sum(dim=1)\n",
    "\n",
    "            # Compute metrics\n",
    "            all_scores = torch.cat([pos_scores, neg_scores]).cpu().numpy()\n",
    "            all_labels = np.concatenate([np.ones(len(pos_scores)), np.zeros(len(neg_scores))])\n",
    "\n",
    "            # AUC and AP\n",
    "            try:\n",
    "                auc = roc_auc_score(all_labels, all_scores)\n",
    "                ap = average_precision_score(all_labels, all_scores)\n",
    "            except Exception as e:\n",
    "                auc, ap = 0.5, 0.5\n",
    "\n",
    "            metrics = {\n",
    "                'auc': auc,\n",
    "                'ap': ap,\n",
    "                'precision@10': ap,\n",
    "                'recall@10': auc,\n",
    "                'ndcg@10': (auc + ap) / 2\n",
    "            }\n",
    "\n",
    "        model.train()\n",
    "        return metrics\n",
    "\n",
    "    def _aggregate_results(self, results: List[Dict]) -> Dict:\n",
    "        \"\"\"Aggregate results across multiple seeds\"\"\"\n",
    "        metrics = ['auc', 'ap', 'precision@10', 'recall@10', 'ndcg@10']\n",
    "\n",
    "        aggregated = {\n",
    "            'feature_config': results[0]['feature_config'],\n",
    "            'num_runs': len(results),\n",
    "            'avg_model_params': np.mean([r['model_params'] for r in results]),\n",
    "            'avg_feature_dim': np.mean([r['feature_dim'] for r in results]),\n",
    "            'avg_num_nodes': np.mean([r['num_nodes'] for r in results])\n",
    "        }\n",
    "\n",
    "        for metric in metrics:\n",
    "            values = [r['test_metrics'][metric] for r in results]\n",
    "            aggregated[f'{metric}_mean'] = np.mean(values)\n",
    "            aggregated[f'{metric}_std'] = np.std(values)\n",
    "\n",
    "        return aggregated\n",
    "\n",
    "    def _analyze_results(self):\n",
    "        \"\"\"Analyze results and create visualizations\"\"\"\n",
    "        print(\"\\nğŸ“Š ANALYZING FEATURE IMPORTANCE RESULTS\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Create results DataFrame\n",
    "        rows = []\n",
    "        for config, results in self.results.items():\n",
    "            row = {\n",
    "                'Feature Config': config,\n",
    "                'Description': FeatureSelector({}).feature_configs[config]['description'],\n",
    "                'AUC': f\"{results['auc_mean']:.4f} Â± {results['auc_std']:.4f}\",\n",
    "                'AP': f\"{results['ap_mean']:.4f} Â± {results['ap_std']:.4f}\",\n",
    "                'NDCG@10': f\"{results['ndcg@10_mean']:.4f} Â± {results['ndcg@10_std']:.4f}\",\n",
    "                'Feature Dim': int(results['avg_feature_dim']),\n",
    "                'Nodes': int(results['avg_num_nodes'])\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "        results_df = pd.DataFrame(rows)\n",
    "        print(results_df.to_string(index=False))\n",
    "\n",
    "        # Feature importance ranking\n",
    "        feature_importance = sorted(\n",
    "            [(config, results['auc_mean']) for config, results in self.results.items()],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "\n",
    "        print(f\"\\nğŸ† FEATURE IMPORTANCE RANKING (by AUC):\")\n",
    "        for i, (config, score) in enumerate(feature_importance, 1):\n",
    "            print(f\"   {i}. {config}: {score:.4f}\")\n",
    "\n",
    "        # Create visualization\n",
    "        self._create_feature_importance_plot()\n",
    "\n",
    "    def _create_feature_importance_plot(self):\n",
    "        \"\"\"Create feature importance visualization\"\"\"\n",
    "        # Extract data for plotting\n",
    "        configs = list(self.results.keys())\n",
    "        auc_means = [self.results[config]['auc_mean'] for config in configs]\n",
    "        auc_stds = [self.results[config]['auc_std'] for config in configs]\n",
    "\n",
    "        # Create plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        # Bar plot with error bars\n",
    "        bars = plt.bar(range(len(configs)), auc_means, yerr=auc_stds, capsize=5)\n",
    "        plt.xlabel('Feature Configuration')\n",
    "        plt.ylabel('AUC Score')\n",
    "        plt.title('Feature Importance Analysis\\nImpact of Different Feature Types on Recommendation Performance')\n",
    "        plt.xticks(range(len(configs)), [c.replace('_', '\\n') for c in configs], rotation=45)\n",
    "\n",
    "        # Color bars by performance\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(configs)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Save plot\n",
    "        plot_path = f\"{self.data_dir}/feature_importance_results.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"ğŸ“ˆ Saved feature importance plot to: {plot_path}\")\n",
    "\n",
    "    def _save_results(self):\n",
    "        \"\"\"Save experiment results\"\"\"\n",
    "        output_file = f\"{self.data_dir}/exp2_feature_importance_results.json\"\n",
    "\n",
    "        # Prepare results for JSON serialization\n",
    "        json_results = {}\n",
    "        for config, results in self.results.items():\n",
    "            json_results[config] = {k: float(v) if isinstance(v, (np.float32, np.float64)) else v\n",
    "                                   for k, v in results.items()}\n",
    "\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'experiment': 'Feature Importance Analysis (Experiment 2 - Memory Efficient)',\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'config': self.config,\n",
    "                'results': json_results\n",
    "            }, f, indent=2)\n",
    "\n",
    "        print(f\"ğŸ’¾ Saved detailed results to: {output_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run Experiment 2\"\"\"\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'embedding_dim': 16,        # Further reduced for memory efficiency\n",
    "        'num_layers': 2,            # Keep at 2\n",
    "        'learning_rate': 0.001,\n",
    "        'num_epochs': 10,           # Reduced for faster testing\n",
    "        'random_seeds': [42],       # Just 1 seed for initial testing\n",
    "        'batch_size': 256,\n",
    "    }\n",
    "\n",
    "    # Data directory\n",
    "    data_dir = \"../data/processed/gnn_ready\"\n",
    "\n",
    "    # Check if data exists\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"âŒ Data directory not found: {data_dir}\")\n",
    "        print(\"Please run data preprocessing first!\")\n",
    "        return\n",
    "\n",
    "    print(\"ğŸµ EXPERIMENT 2: FEATURE IMPORTANCE ANALYSIS (Memory Efficient)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ğŸ¯ Goal: Understand how multi-modal node features enhance recommendation accuracy\")\n",
    "    print(\"ğŸ“Š Method: Test 5 different feature combinations across 2 random seeds\")\n",
    "    print(\"ğŸ“ˆ Focus: Feature value analysis, overfitting detection, performance comparison\")\n",
    "    print(\"ğŸ”§ Note: Using memory-efficient implementation with dataset sampling\")\n",
    "    print(\"âš¡ Settings: Max 50K nodes, 2 layers, 25 epochs for faster execution\")\n",
    "    print()\n",
    "\n",
    "    # Run experiment\n",
    "    try:\n",
    "        experiment = FeatureImportanceExperiment(data_dir, config)\n",
    "        experiment.run_experiment()\n",
    "\n",
    "        print(\"\\nâœ… EXPERIMENT 2 COMPLETED!\")\n",
    "        print(\"ğŸ” Check the results files and visualizations in the data directory.\")\n",
    "        print(\"ğŸ“Š Key insights:\")\n",
    "        print(\"   â€¢ Which feature types are most valuable?\")\n",
    "        print(\"   â€¢ Do more features always improve performance?\")\n",
    "        print(\"   â€¢ Is there evidence of overfitting with complex features?\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Experiment failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a0dfcd59910b512c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸµ EXPERIMENT 2: FEATURE IMPORTANCE ANALYSIS (Memory Efficient)\n",
      "======================================================================\n",
      "ğŸ¯ Goal: Understand how multi-modal node features enhance recommendation accuracy\n",
      "ğŸ“Š Method: Test 5 different feature combinations across 2 random seeds\n",
      "ğŸ“ˆ Focus: Feature value analysis, overfitting detection, performance comparison\n",
      "ğŸ”§ Note: Using memory-efficient implementation with dataset sampling\n",
      "âš¡ Settings: Max 50K nodes, 2 layers, 25 epochs for faster execution\n",
      "\n",
      "ğŸ”§ Using device: cpu\n",
      "ğŸ§ª STARTING EXPERIMENT 2: FEATURE IMPORTANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "ğŸ”¬ Testing Feature Config: NO_FEATURES\n",
      "----------------------------------------\n",
      "   ğŸ² Random seed: 42\n",
      "ğŸ“‚ Loading data from ../data/processed/gnn_ready\n",
      "âš ï¸  Warning: Could not load split_ratios: No module named 'numpy._core'\n",
      "âœ… Loaded data for 661,325 nodes\n",
      "ğŸ“Š Available split keys: ['train_edges', 'val_edges', 'test_edges', 'train_indices', 'val_indices', 'test_indices', 'negative_val', 'negative_test']\n",
      "ğŸ” Available feature keys: ['playlist', 'track', 'user', 'artist', 'album']\n",
      "ğŸ“‹ Feature shapes:\n",
      "   playlist: (49993, 6)\n",
      "   track: (356998, 4)\n",
      "   user: (10430, 4)\n",
      "   artist: (72209, 4)\n",
      "   album: (171695, 4)\n",
      "ğŸ”ª Dataset too large (661,325 nodes), sampling to 10,000 nodes\n",
      "ğŸ¯ Sampling dataset to 10,000 nodes...\n",
      "   Selected top 2500 playlists and 5000 tracks\n",
      "   ğŸ”§ Remapping features...\n",
      "      Original playlist features (playlist): (49993, 6)\n",
      "      Remapped playlist features: (2500, 6)\n",
      "      Original track features (track): (356998, 4)\n",
      "      Remapped track features: (5000, 4)\n",
      "      Skipping user features (not used in sampled graph)\n",
      "      Skipping artist features (not used in sampled graph)\n",
      "      Skipping album features (not used in sampled graph)\n",
      "âœ… Sampled and remapped to 7,500 nodes\n",
      "   ğŸ“Š Final: 2500 playlists, 5000 tracks\n",
      "ğŸ” Available features after sampling:\n",
      "   playlists: (2500, 6)\n",
      "   tracks: (5000, 4)\n",
      "ğŸ¯ Selecting features for config: no_features\n",
      "   ğŸš« no_features: No features used\n",
      "âŒ No features selected for no_features\n",
      "âœ… Dataset initialized with feature config: no_features\n",
      "ğŸ”— Building edge index...\n",
      "   Adding 95,312 edges of type: playlist_track\n",
      "âœ… Built edge index with 95,312 total edges\n",
      "      ğŸ“Š No features selected for no_features\n",
      "      ğŸ¯ Feature config: no_features, Features: False, Dim: 0\n",
      "      ğŸ“Š Model: 7,500 nodes, 120,000 parameters\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      Epoch   0: Loss=0.6921, Val-AUC=0.6134, Time=0.07s\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      Epoch   5: Loss=0.6920, Val-AUC=0.6109, Time=0.07s\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "      ğŸ“Š Using only structural embeddings: torch.Size([7500, 16])\n",
      "\n",
      "ğŸ”¬ Testing Feature Config: BASIC_ONLY\n",
      "----------------------------------------\n",
      "   ğŸ² Random seed: 42\n",
      "ğŸ“‚ Loading data from ../data/processed/gnn_ready\n",
      "âš ï¸  Warning: Could not load split_ratios: No module named 'numpy._core'\n",
      "âœ… Loaded data for 661,325 nodes\n",
      "ğŸ“Š Available split keys: ['train_edges', 'val_edges', 'test_edges', 'train_indices', 'val_indices', 'test_indices', 'negative_val', 'negative_test']\n",
      "ğŸ” Available feature keys: ['playlist', 'track', 'user', 'artist', 'album']\n",
      "ğŸ“‹ Feature shapes:\n",
      "   playlist: (49993, 6)\n",
      "   track: (356998, 4)\n",
      "   user: (10430, 4)\n",
      "   artist: (72209, 4)\n",
      "   album: (171695, 4)\n",
      "ğŸ”ª Dataset too large (661,325 nodes), sampling to 10,000 nodes\n",
      "ğŸ¯ Sampling dataset to 10,000 nodes...\n",
      "   Selected top 2500 playlists and 5000 tracks\n",
      "   ğŸ”§ Remapping features...\n",
      "      Original playlist features (playlist): (49993, 6)\n",
      "      Remapped playlist features: (2500, 6)\n",
      "      Original track features (track): (356998, 4)\n",
      "      Remapped track features: (5000, 4)\n",
      "      Skipping user features (not used in sampled graph)\n",
      "      Skipping artist features (not used in sampled graph)\n",
      "      Skipping album features (not used in sampled graph)\n",
      "âœ… Sampled and remapped to 7,500 nodes\n",
      "   ğŸ“Š Final: 2500 playlists, 5000 tracks\n",
      "ğŸ” Available features after sampling:\n",
      "   playlists: (2500, 6)\n",
      "   tracks: (5000, 4)\n",
      "ğŸ¯ Selecting features for config: basic_only\n",
      "   ğŸ“Š playlists: Available features shape = (2500, 6)\n",
      "   âœ… basic_only - playlists: Using 2 basic features\n",
      "   ğŸ“Š tracks: Available features shape = (5000, 4)\n",
      "   âœ… basic_only - tracks: Using 2 basic features\n",
      "   ğŸ“ˆ Total feature dimensions for basic_only: 4\n",
      "âœ… Selected features:\n",
      "   playlists: (2500, 2)\n",
      "   tracks: (5000, 2)\n",
      "âœ… Dataset initialized with feature config: basic_only\n",
      "ğŸ”— Building edge index...\n",
      "   Adding 95,312 edges of type: playlist_track\n",
      "âœ… Built edge index with 95,312 total edges\n",
      "      ğŸ”§ Preparing features for model...\n",
      "         playlists: (2500, 2) -> tensor\n",
      "         tracks: (5000, 2) -> tensor\n",
      "      ğŸ“Š Total feature dimension: 4\n",
      "      ğŸ¯ Feature config: basic_only, Features: True, Dim: 4\n",
      "      ğŸ“Š Model: 7,500 nodes, 120,624 parameters\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   0: Loss=9.3342, Val-AUC=0.5311, Time=0.06s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   5: Loss=9.4359, Val-AUC=0.5531, Time=0.06s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "\n",
      "ğŸ”¬ Testing Feature Config: AUDIO_ONLY\n",
      "----------------------------------------\n",
      "   ğŸ² Random seed: 42\n",
      "ğŸ“‚ Loading data from ../data/processed/gnn_ready\n",
      "âš ï¸  Warning: Could not load split_ratios: No module named 'numpy._core'\n",
      "âœ… Loaded data for 661,325 nodes\n",
      "ğŸ“Š Available split keys: ['train_edges', 'val_edges', 'test_edges', 'train_indices', 'val_indices', 'test_indices', 'negative_val', 'negative_test']\n",
      "ğŸ” Available feature keys: ['playlist', 'track', 'user', 'artist', 'album']\n",
      "ğŸ“‹ Feature shapes:\n",
      "   playlist: (49993, 6)\n",
      "   track: (356998, 4)\n",
      "   user: (10430, 4)\n",
      "   artist: (72209, 4)\n",
      "   album: (171695, 4)\n",
      "ğŸ”ª Dataset too large (661,325 nodes), sampling to 10,000 nodes\n",
      "ğŸ¯ Sampling dataset to 10,000 nodes...\n",
      "   Selected top 2500 playlists and 5000 tracks\n",
      "   ğŸ”§ Remapping features...\n",
      "      Original playlist features (playlist): (49993, 6)\n",
      "      Remapped playlist features: (2500, 6)\n",
      "      Original track features (track): (356998, 4)\n",
      "      Remapped track features: (5000, 4)\n",
      "      Skipping user features (not used in sampled graph)\n",
      "      Skipping artist features (not used in sampled graph)\n",
      "      Skipping album features (not used in sampled graph)\n",
      "âœ… Sampled and remapped to 7,500 nodes\n",
      "   ğŸ“Š Final: 2500 playlists, 5000 tracks\n",
      "ğŸ” Available features after sampling:\n",
      "   playlists: (2500, 6)\n",
      "   tracks: (5000, 4)\n",
      "ğŸ¯ Selecting features for config: audio_only\n",
      "   ğŸ“Š playlists: Available features shape = (2500, 6)\n",
      "   ğŸ“Š tracks: Available features shape = (5000, 4)\n",
      "   âœ… audio_only - tracks: Using 2 audio features\n",
      "   ğŸ“ˆ Total feature dimensions for audio_only: 2\n",
      "âœ… Selected features:\n",
      "   tracks: (5000, 2)\n",
      "âœ… Dataset initialized with feature config: audio_only\n",
      "ğŸ”— Building edge index...\n",
      "   Adding 95,312 edges of type: playlist_track\n",
      "âœ… Built edge index with 95,312 total edges\n",
      "      ğŸ”§ Preparing features for model...\n",
      "         tracks: (5000, 2) -> tensor\n",
      "      ğŸ“Š Total feature dimension: 2\n",
      "      ğŸ¯ Feature config: audio_only, Features: True, Dim: 2\n",
      "      ğŸ“Š Model: 7,500 nodes, 120,576 parameters\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   0: Loss=1.9277, Val-AUC=0.5597, Time=0.06s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   5: Loss=1.4392, Val-AUC=0.5569, Time=0.11s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing tracks features: torch.Size([5000, 2])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "\n",
      "ğŸ”¬ Testing Feature Config: TEMPORAL_ONLY\n",
      "----------------------------------------\n",
      "   ğŸ² Random seed: 42\n",
      "ğŸ“‚ Loading data from ../data/processed/gnn_ready\n",
      "âš ï¸  Warning: Could not load split_ratios: No module named 'numpy._core'\n",
      "âœ… Loaded data for 661,325 nodes\n",
      "ğŸ“Š Available split keys: ['train_edges', 'val_edges', 'test_edges', 'train_indices', 'val_indices', 'test_indices', 'negative_val', 'negative_test']\n",
      "ğŸ” Available feature keys: ['playlist', 'track', 'user', 'artist', 'album']\n",
      "ğŸ“‹ Feature shapes:\n",
      "   playlist: (49993, 6)\n",
      "   track: (356998, 4)\n",
      "   user: (10430, 4)\n",
      "   artist: (72209, 4)\n",
      "   album: (171695, 4)\n",
      "ğŸ”ª Dataset too large (661,325 nodes), sampling to 10,000 nodes\n",
      "ğŸ¯ Sampling dataset to 10,000 nodes...\n",
      "   Selected top 2500 playlists and 5000 tracks\n",
      "   ğŸ”§ Remapping features...\n",
      "      Original playlist features (playlist): (49993, 6)\n",
      "      Remapped playlist features: (2500, 6)\n",
      "      Original track features (track): (356998, 4)\n",
      "      Remapped track features: (5000, 4)\n",
      "      Skipping user features (not used in sampled graph)\n",
      "      Skipping artist features (not used in sampled graph)\n",
      "      Skipping album features (not used in sampled graph)\n",
      "âœ… Sampled and remapped to 7,500 nodes\n",
      "   ğŸ“Š Final: 2500 playlists, 5000 tracks\n",
      "ğŸ” Available features after sampling:\n",
      "   playlists: (2500, 6)\n",
      "   tracks: (5000, 4)\n",
      "ğŸ¯ Selecting features for config: temporal_only\n",
      "   ğŸ“Š playlists: Available features shape = (2500, 6)\n",
      "   âœ… temporal_only - playlists: Using 2 temporal features\n",
      "   ğŸ“Š tracks: Available features shape = (5000, 4)\n",
      "   ğŸ“ˆ Total feature dimensions for temporal_only: 2\n",
      "âœ… Selected features:\n",
      "   playlists: (2500, 2)\n",
      "âœ… Dataset initialized with feature config: temporal_only\n",
      "ğŸ”— Building edge index...\n",
      "   Adding 95,312 edges of type: playlist_track\n",
      "âœ… Built edge index with 95,312 total edges\n",
      "      ğŸ”§ Preparing features for model...\n",
      "         playlists: (2500, 2) -> tensor\n",
      "      ğŸ“Š Total feature dimension: 2\n",
      "      ğŸ¯ Feature config: temporal_only, Features: True, Dim: 2\n",
      "      ğŸ“Š Model: 7,500 nodes, 120,576 parameters\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   0: Loss=1.0510, Val-AUC=0.4724, Time=0.10s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   5: Loss=0.8695, Val-AUC=0.4959, Time=0.12s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 2])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "\n",
      "ğŸ”¬ Testing Feature Config: ALL_FEATURES\n",
      "----------------------------------------\n",
      "   ğŸ² Random seed: 42\n",
      "ğŸ“‚ Loading data from ../data/processed/gnn_ready\n",
      "âš ï¸  Warning: Could not load split_ratios: No module named 'numpy._core'\n",
      "âœ… Loaded data for 661,325 nodes\n",
      "ğŸ“Š Available split keys: ['train_edges', 'val_edges', 'test_edges', 'train_indices', 'val_indices', 'test_indices', 'negative_val', 'negative_test']\n",
      "ğŸ” Available feature keys: ['playlist', 'track', 'user', 'artist', 'album']\n",
      "ğŸ“‹ Feature shapes:\n",
      "   playlist: (49993, 6)\n",
      "   track: (356998, 4)\n",
      "   user: (10430, 4)\n",
      "   artist: (72209, 4)\n",
      "   album: (171695, 4)\n",
      "ğŸ”ª Dataset too large (661,325 nodes), sampling to 10,000 nodes\n",
      "ğŸ¯ Sampling dataset to 10,000 nodes...\n",
      "   Selected top 2500 playlists and 5000 tracks\n",
      "   ğŸ”§ Remapping features...\n",
      "      Original playlist features (playlist): (49993, 6)\n",
      "      Remapped playlist features: (2500, 6)\n",
      "      Original track features (track): (356998, 4)\n",
      "      Remapped track features: (5000, 4)\n",
      "      Skipping user features (not used in sampled graph)\n",
      "      Skipping artist features (not used in sampled graph)\n",
      "      Skipping album features (not used in sampled graph)\n",
      "âœ… Sampled and remapped to 7,500 nodes\n",
      "   ğŸ“Š Final: 2500 playlists, 5000 tracks\n",
      "ğŸ” Available features after sampling:\n",
      "   playlists: (2500, 6)\n",
      "   tracks: (5000, 4)\n",
      "ğŸ¯ Selecting features for config: all_features\n",
      "   ğŸ“Š playlists: Available features shape = (2500, 6)\n",
      "   âœ… all_features - playlists: Using 6 all features\n",
      "   ğŸ“Š tracks: Available features shape = (5000, 4)\n",
      "   âœ… all_features - tracks: Using 4 all features\n",
      "   ğŸ“ˆ Total feature dimensions for all_features: 10\n",
      "âœ… Selected features:\n",
      "   playlists: (2500, 6)\n",
      "   tracks: (5000, 4)\n",
      "âœ… Dataset initialized with feature config: all_features\n",
      "ğŸ”— Building edge index...\n",
      "   Adding 95,312 edges of type: playlist_track\n",
      "âœ… Built edge index with 95,312 total edges\n",
      "      ğŸ”§ Preparing features for model...\n",
      "         playlists: (2500, 6) -> tensor\n",
      "         tracks: (5000, 4) -> tensor\n",
      "      ğŸ“Š Total feature dimension: 10\n",
      "      ğŸ¯ Feature config: all_features, Features: True, Dim: 10\n",
      "      ğŸ“Š Model: 7,500 nodes, 120,720 parameters\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   0: Loss=6.2286, Val-AUC=0.5353, Time=0.10s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      Epoch   5: Loss=4.3127, Val-AUC=0.5822, Time=0.16s\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "      ğŸ”§ Incorporating features into embeddings...\n",
      "         Processing playlists features: torch.Size([2500, 6])\n",
      "         Projected to: torch.Size([2500, 16])\n",
      "         Added playlist features to indices 0:2500\n",
      "         Processing tracks features: torch.Size([5000, 4])\n",
      "         Projected to: torch.Size([5000, 16])\n",
      "         Added track features to indices 2500:7500\n",
      "      âœ… Combined embeddings shape: torch.Size([7500, 16])\n",
      "\n",
      "ğŸ“Š ANALYZING FEATURE IMPORTANCE RESULTS\n",
      "==================================================\n",
      "Feature Config                                                    Description             AUC              AP         NDCG@10  Feature Dim  Nodes\n",
      "   no_features                         Pure graph structure (embeddings only) 0.6181 Â± 0.0000 0.5920 Â± 0.0000 0.6051 Â± 0.0000            0   7500\n",
      "    basic_only            Simple metadata (playlist length, track popularity) 0.5659 Â± 0.0000 0.5631 Â± 0.0000 0.5645 Â± 0.0000            4   7500\n",
      "    audio_only                 Music features (danceability, energy, valence) 0.5784 Â± 0.0000 0.5577 Â± 0.0000 0.5680 Â± 0.0000            2   7500\n",
      " temporal_only Time-based features (modification dates, collaborative status) 0.5040 Â± 0.0000 0.5115 Â± 0.0000 0.5077 Â± 0.0000            2   7500\n",
      "  all_features                                            Everything combined 0.5806 Â± 0.0000 0.5850 Â± 0.0000 0.5828 Â± 0.0000           10   7500\n",
      "\n",
      "ğŸ† FEATURE IMPORTANCE RANKING (by AUC):\n",
      "   1. no_features: 0.6181\n",
      "   2. all_features: 0.5806\n",
      "   3. audio_only: 0.5784\n",
      "   4. basic_only: 0.5659\n",
      "   5. temporal_only: 0.5040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Saved feature importance plot to: ../data/processed/gnn_ready/feature_importance_results.png\n",
      "ğŸ’¾ Saved detailed results to: ../data/processed/gnn_ready/exp2_feature_importance_results.json\n",
      "\n",
      "âœ… EXPERIMENT 2 COMPLETED!\n",
      "ğŸ” Check the results files and visualizations in the data directory.\n",
      "ğŸ“Š Key insights:\n",
      "   â€¢ Which feature types are most valuable?\n",
      "   â€¢ Do more features always improve performance?\n",
      "   â€¢ Is there evidence of overfitting with complex features?\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
